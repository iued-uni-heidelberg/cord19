{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNS6yaUAN/LmHdOvR5+AijN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/cord19/blob/main/Cord19_v04_download2text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apx9q6HAbXkM"
      },
      "source": [
        "# Downloading and reading CORD19 corpus\n",
        "This notebook downloads and reads the free cord19 corpus into one file. The notebook is hosted at IÜD, Heidelberg University github repository https://github.com/iued-uni-heidelberg/cord19\n",
        "\n",
        "CORD19 (covid-19) open-source corpus is available from https://www.semanticscholar.org/cord19/download. \n",
        "\n",
        "Documentation is available at https://github.com/allenai/cord19\n",
        "\n",
        "The original files are in json format. The output file is in plain text format; documents are separated (by default) by \\<doc id=\"doc1000001\"> ... \\</doc> tags\n",
        "\n",
        "The purpose of the plain text file is for further processing, e.g., generating linguistic annotation using the TreeTagger or the Standford parser for part-of-speech annotation or dependency / constituency parsing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWGPbsx3fjpA"
      },
      "source": [
        "## Downloading CORD19 corpus\n",
        "\n",
        "The corpus is downloaded and extracted from https://www.semanticscholar.org/cord19/download\n",
        "\n",
        "Please check the link above: if you need the latest release of the corpus or if you would like to choose another release. Currently the 2022-06-02 release is downloaded.\n",
        "\n",
        "File size is ~11GB (v2021-08-30)\n",
        "File size is ~18GB (v2022-06-02)\n",
        "expected download time ~9 min\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ8alxYIfvhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e069c0a5-d8f1-46a6-8ced-ea6c8c149da7"
      },
      "source": [
        "# !wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2021-08-30.tar.gz\n",
        "!wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2022-06-02.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-06 06:33:31--  https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2022-06-02.tar.gz\n",
            "Resolving ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)... 52.92.194.90\n",
            "Connecting to ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)|52.92.194.90|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18657952487 (17G) [binary/octet-stream]\n",
            "Saving to: ‘cord-19_2022-06-02.tar.gz’\n",
            "\n",
            "cord-19_2022-06-02. 100%[===================>]  17.38G  42.3MB/s    in 7m 27s  \n",
            "\n",
            "2022-10-06 06:40:59 (39.8 MB/s) - ‘cord-19_2022-06-02.tar.gz’ saved [18657952487/18657952487]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FajAVEv_zgXY"
      },
      "source": [
        "Extracting cord-19 corpus, approximate time ~ 4 min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X69zODWxxYSB",
        "outputId": "515b9ae6-e1bc-40b2-9222-14b26094af6d"
      },
      "source": [
        "# !tar -xvzf cord-19_2021-08-30.tar.gz\n",
        "!tar -xvzf cord-19_2022-06-02.tar.gz 2022-06-02/document_parses.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-02/document_parses.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8g4-kQyzuZN"
      },
      "source": [
        "Removing initial archive to free some disk space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOp7a1OJzHzY"
      },
      "source": [
        "# !rm cord-19_2021-08-30.tar.gz\n",
        "!rm cord-19_2022-06-02.tar.gz\n",
        "!mv 2022-06-02/document_parses.tar.gz ./document_parses.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrJAdw7D4rOh"
      },
      "source": [
        "Removing more files to save space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27p3sZa04K4X"
      },
      "source": [
        "# removing more files to save space\n",
        "# !rm --recursive 2021-08-30\n",
        "# !rm --recursive document_parses/pdf_json\n",
        "!rm --recursive 2022-06-02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rT3UnT9zzev"
      },
      "source": [
        "Extracting document parsers, which contain individual articles in separate json files. This is expected to take ~ 9+ min."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !tar -tvf document_parses.tar.gz >document_parses.txt"
      ],
      "metadata": {
        "id": "B_HdqkFt1_-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxHIVjtrzRCT"
      },
      "source": [
        "# !tar -xvzf 2021-08-30/document_parses.tar.gz\n",
        "!tar -xvzf document_parses.tar.gz document_parses/pmc_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output:\n",
        "\n",
        "PMC1054884.xml.json\n",
        "\n",
        "PMC1065028.xml.json\n",
        "\n",
        "PMC1065064.xml.json\n",
        "\n",
        "PMC1065120.xml.json\n",
        "\n",
        "PMC1065257.xml.json\n",
        "\n",
        "PMC1072802.xml.json\n",
        "\n",
        "PMC1072806.xml.json\n",
        "\n",
        "PMC1072807.xml.json\n",
        "\n",
        "PMC1074505.xml.json\n",
        "\n",
        "PMC1074749.xml.json\n",
        "...\n",
        "\n",
        "~ 28G of json files\n"
      ],
      "metadata": {
        "id": "e_byMXi95YBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/document_parses/pmc_json/PMC9034168.xml.json PMC9034168.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7128104.xml.json PMC7128104.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8769777.xml.json PMC8769777.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7926205.xml.json PMC7926205.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8799642.xml.json PMC8799642.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7124374.xml.json PMC7124374.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8812323.xml.json PMC8812323.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7446676.xml.json PMC7446676.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7436596.xml.json PMC7436596.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8808276.xml.json PMC8808276.xml.json"
      ],
      "metadata": {
        "id": "L3fUdPdlJZrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc /content/document_parses/pmc_json/PMC9034168.xml.json PMC9034168.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7128104.xml.json PMC7128104.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC8769777.xml.json PMC8769777.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7926205.xml.json PMC7926205.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC8799642.xml.json PMC8799642.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7124374.xml.json PMC7124374.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC8812323.xml.json PMC8812323.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7446676.xml.json PMC7446676.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7436596.xml.json PMC7436596.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC8808276.xml.json PMC8808276.xml.json"
      ],
      "metadata": {
        "id": "JJMZorcJ5gKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ls /content/document_parses/pmc_json >pms_json.txt"
      ],
      "metadata": {
        "id": "ZNK5Eoct5dTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm --recursive document_parses/pmc_json\n",
        "# !rm --recursive document_parses/pdf_json\n",
        "!rm document_parses.tar.gz"
      ],
      "metadata": {
        "id": "JsuPYd05utfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh /content/document_parses/pmc_json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFKmfiIA9Q7a",
        "outputId": "87458a6b-0df7-4419-d35f-eb04d816c50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28G\t/content/document_parses/pmc_json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/document_parses/pmc_json_sample"
      ],
      "metadata": {
        "id": "3dstcJcr-MJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Alternative: working with sample 100mw"
      ],
      "metadata": {
        "id": "EvojGBd6D-1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/b420d407463d4a728feb/?dl=1\n",
        "!mv index.html?dl=1 document_parses_sample0.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsWWcMBwEEpj",
        "outputId": "b5deb0dc-d9a9-43bc-b1e4-2985cdc4aea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-06 20:09:29--  https://heibox.uni-heidelberg.de/f/b420d407463d4a728feb/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/999fa69c-301d-4614-ad48-30cc4b5b15b6/document_parses_sample.tar.gz [following]\n",
            "--2022-10-06 20:09:30--  https://heibox.uni-heidelberg.de/seafhttp/files/999fa69c-301d-4614-ad48-30cc4b5b15b6/document_parses_sample.tar.gz\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 611303079 (583M) [application/octet-stream]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 582.98M  14.9MB/s    in 39s     \n",
            "\n",
            "2022-10-06 20:10:09 (14.9 MB/s) - ‘index.html?dl=1’ saved [611303079/611303079]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvzf document_parses_sample0.tar.gz"
      ],
      "metadata": {
        "id": "4a1Mo4mHE2YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm document_parses_sample0.tar.gz"
      ],
      "metadata": {
        "id": "v7ci8O-CFSUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh /content/document_parses/pmc_json_sample/"
      ],
      "metadata": {
        "id": "VgP2hRdOFi88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecvdez_1e_oD"
      },
      "source": [
        "## Reading json directory and merging into text file(s)\n",
        "\n",
        "Run this cell to create the class; then run the next cell to execute on the directory \"document_parses/pmc_json\"\n",
        "\n",
        "This is a class for reading a directory with json files and writing them to a single file or split into several text file, with \"split_by_docs=N\", N documents in each file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW5WsBI-dNIN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "b15b0de3-88db-42c3-962c-db31ca8d2dab"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Python script to open each file, read json input and copy to one text file for subsequent processing\n",
        "import os, re, sys\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "class clJsonDir2txt(object):\n",
        "    '''\n",
        "    @author Bogdan Babych, IÜD, Heidelberg University\n",
        "    @email bogdan [dot] babych [at] iued [dot] uni-heidelberg [dot] de\n",
        "    a script for processing covid-19 corpus:\n",
        "    @url https://www.semanticscholar.org/cord19 @url https://www.semanticscholar.org/cord19/download\n",
        "        recursively reads files from a directory, and glues them together into a single corpus file\n",
        "\n",
        "    @todo:\n",
        "        working with sections - collect titles of all sections; frequent sections; select argumentative sections (e.g., discussion, analysis...)\n",
        "        - to compare descriptive and argumentative parts of the corpus\n",
        "\n",
        "        experimenting with different annotations (pos, parsing... ); MT quality evaluation...\n",
        "    '''\n",
        "    def __init__(self, SDirName, output_file = 'corpus_out.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = True, include_authors = True, tag='doc', id=1000000, split_by_docs = 0, copy_docs = 0): # initialising by openning the directories\n",
        "        self.SOutput_file = output_file\n",
        "        self.STextFilter = textfilter\n",
        "        self.RFilter = re.compile(textfilter, re.IGNORECASE | re.MULTILINE)\n",
        "        self.BInclTitle = include_title # implemented\n",
        "        self.BInclSectionNames = include_sectionNames # implemented\n",
        "        self.BInclRefs = include_refs # not implemented yet\n",
        "        self.BInclAuth = include_authors # not implemented yet\n",
        "        self.STag = tag\n",
        "        self.ID = id\n",
        "        self.ISplitByDocs = int(split_by_docs)\n",
        "        self.ICopyDocs = int(copy_docs)\n",
        "        # global dictionary of section names (to check and make rules...)\n",
        "        self.DSectNames = defaultdict(int)\n",
        "        # print(self.ISplitByDocs)\n",
        "        self.openDir(SDirName)\n",
        "        self.printDictionary(self.DSectNames, 'corpus-section-names.txt')\n",
        "        return\n",
        "\n",
        "\n",
        "    def openDir(self, path): # implementation of recursively openning directories from a given rule directory and reading each file recursively into a string\n",
        "        i = 0\n",
        "        path_sample = path + '_sample'\n",
        "        if self.ISplitByDocs:\n",
        "            SPartFile = \"part1000000\" + self.SOutput_file\n",
        "            FOut = open(SPartFile, 'w')\n",
        "        else:\n",
        "            FOut = open(self.SOutput_file, 'w')\n",
        "\n",
        "        for root,d_names,f_names in os.walk(path):\n",
        "            for f in f_names:\n",
        "                i+=1\n",
        "                if i%10000==0: print(str(i) + '. Processing: ' + f)\n",
        "                fullpath = os.path.join(root, f)\n",
        "                # print(fullpath)\n",
        "                try:\n",
        "                    FIn = open(fullpath,'r')\n",
        "                    SIn = FIn.read()\n",
        "                    # apply text filter, if not None\n",
        "                    if self.STextFilter and (re.search(self.RFilter, SIn) == None): continue\n",
        "                    SText2Write = self.procFile(SIn,f,i)\n",
        "                    if SText2Write: FOut.write(SText2Write) # if the string is not empty then write to file\n",
        "                    FIn.close()\n",
        "                except:\n",
        "                    print(f'file {f} cannot be read or processed')\n",
        "                finally:\n",
        "                    # splitting output into chunks of \"split_by_docs\" size\n",
        "                    if self.ISplitByDocs and (i % self.ISplitByDocs == 0): # if self.ISplitByDocs == 0 then everything goes into one file; if this > 0 then\n",
        "                        SPartFile = \"part\" + str(1000000 + i) + self.SOutput_file # generate new file name\n",
        "                        FOut.flush()\n",
        "                        FOut.close()\n",
        "                        FOut = open(SPartFile, 'w')\n",
        "                    if self.ICopyDocs and (i >= self.ICopyDocs) and (i < (self.ICopyDocs + self.ISplitByDocs)):\n",
        "                        try:\n",
        "                            SOutputDirN = root + '_sample'\n",
        "                            SOutputFN = os.path.join(SOutputDirN, f)\n",
        "                            os.system(f'cp {fullpath} {SOutputFN}')\n",
        "                        except:\n",
        "                            print('.')\n",
        "\n",
        "        FOut.flush()\n",
        "        FOut.close()\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def procFile(self, SIn,SFNameIn,i): # sending each json string for extraction of text and attaching an correct tags to each output string output string\n",
        "        STagOpen = '<' + self.STag + ' id=\"' + self.STag + str(self.ID + i)  + '\">\\n'\n",
        "        STagClose = '\\n</' + self.STag + '>\\n\\n'\n",
        "        SText4Corpus = self.getJson(SIn, SFNameIn)\n",
        "        if SText4Corpus:\n",
        "            return STagOpen + SText4Corpus + STagClose\n",
        "        else:\n",
        "            print('\\tNo data read from: ' + SFNameIn)\n",
        "            return None\n",
        "\n",
        "\n",
        "    def getJson(self, SIn, SFNameIn): # for each file-level string read from a file: managing internal structure of the covid-19 json file\n",
        "        LOut = [] # collecting a list of strings\n",
        "        try:\n",
        "            DDoc = json.loads(SIn)\n",
        "        except:\n",
        "            print('\\t\\t' + SFNameIn + ' => error reading json2dictionary')\n",
        "            return None\n",
        "        # metadata:\n",
        "        try:\n",
        "            DMetaData = DDoc['metadata']\n",
        "            if DMetaData:\n",
        "                SMetaData = self.getJson_Metadata(DMetaData)\n",
        "                if SMetaData: LOut.append(SMetaData)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no metadata')\n",
        "            DMetaData = None\n",
        "        # body text\n",
        "        try:\n",
        "            LBodyText = DDoc['body_text']\n",
        "            if LBodyText:\n",
        "                SBodyText = self.getJson_BodyText(LBodyText)\n",
        "                LOut.append(SBodyText)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no body_text')\n",
        "            LBodyText = None\n",
        "        # further: to implement references\n",
        "\n",
        "        SText = '\\n\\n'.join(LOut)\n",
        "        return SText\n",
        "\n",
        "\n",
        "    def getJson_Metadata(self, DIn): # converts interesting parts of metadata into a string\n",
        "        SMetadata = ''\n",
        "        LMetadata = []\n",
        "        try: STitle = DIn[\"title\"]\n",
        "        except: STitle = None\n",
        "        if STitle and self.BInclTitle:\n",
        "            LMetadata.append(STitle)\n",
        "\n",
        "        # to implement reading of authors' names\n",
        "\n",
        "        if LMetadata: SMetadata = '\\n\\n'.join(LMetadata)\n",
        "        return SMetadata\n",
        "\n",
        "\n",
        "    def getJson_BodyText(self, LIn): # converts interesting parts of the body texts into a string\n",
        "        SBodyText = ''\n",
        "        LBodyText = []\n",
        "        SSectionName0 = '' # current section name set to empty for a new text\n",
        "        # todo: later, in post-processing stage for the whole corpus, maybe after lemmatization...\n",
        "        # ISampleNumber = 0 # samples of 100 words in text; they do not cross section boundaries \n",
        "\n",
        "        for DParagraph in LIn:\n",
        "            # sections added 2022-09-28\n",
        "            try:\n",
        "                # DParagraphs[\"section\"] ## distinction between different sections....\n",
        "                SSectionName = DParagraph[\"section\"]\n",
        "                # normalizing new section name (1)\n",
        "                SSectionName = SSectionName.replace(\"\\n\", \" \")\n",
        "\n",
        "                if self.BInclSectionNames and SSectionName: # if we opted to include section names and section name is not empty\n",
        "                    if SSectionName != SSectionName0: # if we found a new section name\n",
        "                        # processing section name\n",
        "                        SSectionName0 = SSectionName # change the current section name\n",
        "\n",
        "                        # normalizing section name (2)\n",
        "                        SSectionNameNorm = SSectionName.lower()\n",
        "                        SSectionNameNorm = re.sub('[0-9\\.]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = re.sub('[ ]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = SSectionNameNorm.strip()\n",
        "                        \n",
        "                        self.DSectNames[SSectionNameNorm] += 1\n",
        "                        SSect4text = f'<section sName=\"{SSectionNameNorm}\">\\n{SSectionName}\\n</section>'\n",
        "\n",
        "                        LBodyText.append(SSect4text)\n",
        "            except:\n",
        "                print('S!',)\n",
        "                continue\n",
        "            # first original section (we extract text after extracting section name)\n",
        "            try:\n",
        "                ## DParagraphs[section] ## -- later on >> distinction between different sections....\n",
        "                SParagraph = DParagraph[\"text\"]\n",
        "                LBodyText.append(SParagraph)\n",
        "            except:\n",
        "                print('!',)\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "        SBodyText = '\\n\\n'.join(LBodyText)\n",
        "        return SBodyText\n",
        "\n",
        "    def printDictionary(self, DFreq, SFOutDict):\n",
        "        FOutDict = open(SFOutDict, 'w')\n",
        "        for key, val in sorted(DFreq.items(), key=lambda x: x[1], reverse=True):\n",
        "            FOutDict.write(f'{key}\\t{str(val)}\\n')\n",
        "        FOutDict.flush()\n",
        "        FOutDict.close()\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# arguments:\n",
        "'''\n",
        "        sys.argv[1], # obligatory: input directory name;\n",
        "            other arguments optional:\n",
        "            output_file = 'covid19corpus.txt',\n",
        "            textfilter = None, # if this is string, only texts containing it are collected, e.g., covid\n",
        "            include_title = True, # include or exclude title\n",
        "            include_refs = False, # not implemented yet: include or exclude references\n",
        "            split_by_docs=0 # split by groups of n documents; if 0 then write to one file\n",
        "\n",
        "'''\n",
        "\n",
        "'''if __name__ == '__main__':\n",
        "    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=240000)\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"if __name__ == '__main__':\\n    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=240000)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## full corpus:"
      ],
      "metadata": {
        "id": "FJOv50sk7yDR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2EUP7kS5fkW"
      },
      "source": [
        "# remove parameter textfilter='covid', to return all documents\n",
        "# change parameter split_by_docs=40000 to split_by_docs=0 to return a single file instead of ~5 parts with <40000 in each\n",
        "OJsonDir2txt = clJsonDir2txt(\"document_parses/pmc_json\", output_file = 'cord19.txt', textfilter='covid', include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=40000, copy_docs=240000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2zYiDkn5Vbo"
      },
      "source": [
        "## numbers from previous version\n",
        "This cell will executre reading of json files into a single (or multiple) files\n",
        "\n",
        "Change the value of \"split_by_docs=0\" to \"split_by_docs=10000\" or any number ; this will create several corpus files with 10000 or any required number fo documents per file, which you wish to have.\n",
        "\n",
        "\n",
        "Approximate execution time ~10 min\n",
        "\n",
        "\n",
        "File size to download ~4.3 GB\n",
        "\n",
        "It contains ~198.000 documents,\n",
        "\n",
        "~ 671.578.587 words\n",
        "\n",
        "~ 19.381.647 paragraphs (including empty lines, i.e., ~10M real paragraphs)\n",
        "\n",
        "~ 4.619.100.883 characters\n",
        "\n",
        "## numbers for the last version\n",
        "Approximate execution time ~20 min\n",
        "\n",
        "8 BNC-size (100mw) files are generted, containg together\n",
        "\n",
        "~ 315.000 documents\n",
        "\n",
        "  827.118.629 words\n",
        "\n",
        "   22.094.653 paragraphs\n",
        "\n",
        "5.650.921.315 characters\n",
        "\n",
        "\n",
        "Download time can take up to 1 hour depending on your connection speed.\n",
        "\n",
        "To split into ~BNC size chunks (100MW), split into groups of ~40000 documents (in the following cell set \"split_by_docs=20000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VRGnoGMTH_gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "writing frequent section names (ordered by descending frequency, from highest to 1)"
      ],
      "metadata": {
        "id": "eTbi965g2zqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=500000 part1240000cord19.txt >part1240000cord19-500k.txt"
      ],
      "metadata": {
        "id": "0ELJw_EvK133"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls document_parses/pmc_json_sample | wc -l\n",
        "!du -sh document_parses/pmc_json_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkPWJaJLOG_e",
        "outputId": "eb830fb2-2f6a-4e21-8570-a5836bbc26ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40000\n",
            "3.6G\tdocument_parses/pmc_json_sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvzf document_parses_sample.tar.gz document_parses/pmc_json_sample"
      ],
      "metadata": {
        "id": "ZCW92xAFPQtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=75 /content/corpus-section-names.txt"
      ],
      "metadata": {
        "id": "kJeGtnHK5BJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=1000 /content/corpus-section-names.txt > corpus-selection-names-top1000.txt"
      ],
      "metadata": {
        "id": "6xEeB_bip1sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZN2v7snE7iQ"
      },
      "source": [
        "If you have split the text into parts, you can see the number of words in each part using this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIr8NG15EVro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb9dcf5-4a10-4836-dad6-3d9e90568209"
      },
      "source": [
        "!wc part*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   4085687  108914903  752390539 part1000000cord19.txt\n",
            "   4083605  109685695  756986287 part1040000cord19.txt\n",
            "   4117431  110113881  760911812 part1080000cord19.txt\n",
            "   4085114  109782614  757565426 part1120000cord19.txt\n",
            "   4156737  110454560  764015682 part1160000cord19.txt\n",
            "   4051419  109593018  756536541 part1200000cord19.txt\n",
            "   4110526  110115301  761073243 part1240000cord19.txt\n",
            "   3639938   98300665  678104874 part1280000cord19.txt\n",
            "  32330457  866960637 5987584404 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip part1000000cord19.txt\n",
        "!gzip part1040000cord19.txt\n",
        "!gzip part1080000cord19.txt\n",
        "!gzip part1120000cord19.txt\n",
        "!gzip part1160000cord19.txt\n",
        "!gzip part1200000cord19.txt\n",
        "!gzip part1240000cord19.txt\n",
        "!gzip part1280000cord19.txt"
      ],
      "metadata": {
        "id": "TB_sGebiyb08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternative: working with sample 100mw"
      ],
      "metadata": {
        "id": "oi-DSCshH8a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OJsonDir2txt = clJsonDir2txt(\"document_parses/pmc_json_sample\", output_file = 'cord19.txt', textfilter='covid', include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqNr0807I7tx",
        "outputId": "f4377b95-4951-46ec-9f61-f54905e53092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000. Processing: PMC7531324.xml.json\n",
            "20000. Processing: PMC8588348.xml.json\n",
            "30000. Processing: PMC8897106.xml.json\n",
            "40000. Processing: PMC8634281.xml.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=10000 cord19.txt >cord19_10k.txt"
      ],
      "metadata": {
        "id": "MIQbWMZ2KAF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmZdmvgVFF1v"
      },
      "source": [
        "To see the number of words, paragraphs in your corpus you can use this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p8xxDTvAkv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e4ab86-a1d1-49bf-851b-b7ca51f4b7d8"
      },
      "source": [
        "!wc cord19_10k.txt\n",
        "!wc cord19.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  10000  278029 1915103 cord19_10k.txt\n",
            "  4110512 110114834 761070060 cord19.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TreeTagger run on corpus"
      ],
      "metadata": {
        "id": "DiP3Lo53MFV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm -r treetagger/"
      ],
      "metadata": {
        "id": "ojknZk14OaOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# downloading and testing TreeTagger\n",
        "mkdir treetagger\n",
        "cd treetagger\n",
        "# Download the tagger package for your system (PC-Linux, Mac OS-X, ARM64, ARMHF, ARM-Android, PPC64le-Linux).\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/tree-tagger-linux-3.2.4.tar.gz\n",
        "tar -xzvf tree-tagger-linux-3.2.4.tar.gz\n",
        "# Download the tagging scripts into the same directory.\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/tagger-scripts.tar.gz\n",
        "gunzip tagger-scripts.tar.gz\n",
        "# Download the installation script install-tagger.sh.\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/install-tagger.sh\n",
        "# Download the parameter files for the languages you want to process.\n",
        "# list of all files (parameter files) https://cis.lmu.de/~schmid/tools/TreeTagger/#parfiles\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/english.par.gz\n",
        "sh install-tagger.sh\n",
        "cd ..\n",
        "sudo pip install treetaggerwrapper\n",
        "# changing options: no-unknown, sgml, lemma\n",
        "mv /content/treetagger/cmd/tree-tagger-english /content/tree-tagger-english0\n",
        "awk '{ if (NR == 9) print \"OPTIONS=\\\"-token -lemma -sgml -no-unknown\\\"\"; else print $0}' /content/tree-tagger-english0 > /content/treetagger/cmd/tree-tagger-english\n",
        "chmod a+x ./treetagger/cmd/tree-tagger-english\n",
        "\n",
        "# downloading German and Georgian \n",
        "wget https://heibox.uni-heidelberg.de/f/ec8226edebb64a359407/?dl=1\n",
        "mv index.html?dl=1 /content/treetagger/lib/german-utf8.par\n",
        "wget https://heibox.uni-heidelberg.de/f/9183090d2bdb41e09055/?dl=1\n",
        "mv index.html?dl=1 /content/treetagger/lib/georgian.par\n",
        "\n",
        "wget https://heibox.uni-heidelberg.de/f/9cafab0509d64ed1ac4b/?dl=1\n",
        "mv index.html?dl=1 /content/treetagger/cmd/tree-tagger-georgian2\n",
        "# German2 = -no-unknown \n",
        "# note: tree-tagger-german will not work, as parameter files have not been downloaded, onlz use tree-tagger-german2\n",
        "wget https://heibox.uni-heidelberg.de/f/acb9b8a2fa4f40e08f8a/?dl=1\n",
        "mv index.html?dl=1 /content/treetagger/cmd/tree-tagger-german2\n",
        "chmod a+x /content/treetagger/cmd/tree-tagger-georgian2\n",
        "chmod a+x /content/treetagger/cmd/tree-tagger-german2\n",
        "\n",
        "# test text download\n",
        "wget https://heibox.uni-heidelberg.de/f/cdf240db84ca4718b718/?dl=1\n",
        "mv index.html?dl=1 go1984en.txt\n",
        "wget https://heibox.uni-heidelberg.de/f/ea06aa47fe2d49959a62/?dl=1\n",
        "mv index.html?dl=1 go1984de.txt\n",
        "wget https://heibox.uni-heidelberg.de/f/318b32556cdc44d38238/?dl=1\n",
        "mv index.html?dl=1 go1984ka.txt\n"
      ],
      "metadata": {
        "id": "IGnqej7-MDkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "oBSr4WXhPa6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample - Tagging\n",
        "!./treetagger/cmd/tree-tagger-english go1984en.txt >go1984en_2_vert.txt\n",
        "!./treetagger/cmd/tree-tagger-german2 go1984de.txt >go1984de_2_vert.txt\n",
        "!./treetagger/cmd/tree-tagger-georgian2 go1984ka.txt >go1984ka_2_vert.txt"
      ],
      "metadata": {
        "id": "lFrfgzINPhwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./treetagger/cmd/tree-tagger-english cord19_10k.txt >cord19_10k.vert"
      ],
      "metadata": {
        "id": "Z0EfdXOJQXMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./treetagger/cmd/tree-tagger-english cord19.txt >cord19.vert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQwqTmdIQfIb",
        "outputId": "90b70e05-2c28-4547-bd7f-d329f3889cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\treading parameters ...\n",
            "\ttagging ...\n",
            "126567000\t finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvzf cord19-vert.tgz cord19.vert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMJMxjTpSYSP",
        "outputId": "e90f934c-6f1f-475a-dcc7-8c0d165d505b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cord19.vert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!awk -F '\\t' '{if(NF==3) printf \"%s \", $3; else printf \"\\n%s\\n\", $0}' < cord19_10k.vert >cord19_10k.lem"
      ],
      "metadata": {
        "id": "I1l6ud33ZYIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !awk -F '\\t' '(NF==3){printf \"%s \", $3; if(FNR % 10000 == 0){printf \"\\n\"}}' < cord19_1k.vert >cord19_1k.lem\n",
        "# !awk -F '\\t' '{if(NF==3) printf \"%s \", $3; else printf \"\\n%s\\n\", $0}' < cord19_1k.vert >cord19_1k02.lem\n",
        "\n",
        "!awk -F '\\t' '{if(NF==3) printf \"%s \", $3; else printf \"\\n%s\\n\", $0}' < cord19.vert >cord19.lem"
      ],
      "metadata": {
        "id": "ZFASqb85Q3Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvzf cord19-lem.tgz cord19.lem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzg_XhWCY-Hm",
        "outputId": "86986b51-0596-4476-ab82-2f2ddb059d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cord19.lem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalising xml\n",
        "import os, re, sys\n",
        "FOut = open('cord19_10k.lems', 'w')\n",
        "with open('cord19_10k.lem', 'r') as file:\n",
        "# FOut = open('cord19.lems', 'w')\n",
        "# with open('cord19.lem', 'r') as file:\n",
        "\n",
        "    for line in file:\n",
        "        if line.startswith('<doc'): BStartDoc = True\n",
        "        if line.startswith('</section>'): continue\n",
        "        if line.startswith('<section') and BStartDoc == True: BStartDoc = False; FOut.write(line); continue\n",
        "        if line.startswith('<section') and BStartDoc == False: FOut.write('</section>\\n' + line); continue\n",
        "        if line.startswith('</doc>'): FOut.write('</section>\\n' + line); continue\n",
        "        FOut.write(line)\n",
        "\n",
        "    # data = file.read()\n"
      ],
      "metadata": {
        "id": "-vn4dXn6g7wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=20 cord19.lems"
      ],
      "metadata": {
        "id": "oFdrYZfFjJf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvzf cord19-lems.tgz cord19.lems"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBTgjgVCCNaq",
        "outputId": "15a02028-0a5d-40e5-cff3-d95375057ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cord19.lems\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## downloading the file which is ready..."
      ],
      "metadata": {
        "id": "7610I-RBHYDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/3bf283c19c5742d1ac3b/?dl=1\n",
        "!mv index.html?dl=1 cord19-lems.tgz"
      ],
      "metadata": {
        "id": "kiU-AngRHLfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/69e0c866bf3c4ccc8435/?dl=1\n",
        "!mv index.html?dl=1 cord19-10k.lems"
      ],
      "metadata": {
        "id": "1o-ZbIEFHsT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvzf cord19-lems.tgz\n",
        "!wc cord19.lems"
      ],
      "metadata": {
        "id": "lp93jNKuZB8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b13849-0308-4807-def7-45337bbd2792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cord19.lems\n",
            "  1437158 128869077 762343572 cord19.lems\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wc cord19-10k.lems\n",
        "!head --lines=20 cord19-10k.lems"
      ],
      "metadata": {
        "id": "K14PByNIH9lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## end: lemmatization"
      ],
      "metadata": {
        "id": "QFq8Lfq9ZoOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sections and 100-word samples\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tLABeiVpZ_hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading mapping rules\n",
        "!wget https://heibox.uni-heidelberg.de/f/32342a3aa0d04a259bf9/?dl=1\n",
        "!mv index.html?dl=1 covid-sections-and-keywords.zip\n"
      ],
      "metadata": {
        "id": "9hny9cLc2wn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!unzip covid-sections-and-keywords.zip"
      ],
      "metadata": {
        "id": "fhLLSDYy287z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, sys\n",
        "from bs4 import BeautifulSoup\n",
        "class clXml2Stat(object):\n",
        "    '''\n",
        "    The class will create statistics for documents and sections marked in the file\n",
        "    '''\n",
        "    def __init__(self, SFXmlInput, output_file = 'corpus_out_stat.txt'):\n",
        "        # FXmlInput = open(SFXmlInput, 'r')\n",
        "        with open(SFXmlInput, 'r') as file:\n",
        "            data = file.read()\n",
        "        FTsvOutput = open(output_file, 'w')\n",
        "        FTsvOutputNum = open(output_file + '_n.txt', 'w')\n",
        "\n",
        "        print('file read into memory')\n",
        "        FSectionMap = open('covid-sections.tsv', 'r')\n",
        "        self.DSectionMap = self.readTsv2dict(FSectionMap, 0, 2)\n",
        "        FSectionRules = open('covid_section_rules.tsv', 'r')\n",
        "        FSectionLexRND = open('covid-random.tsv', 'r')\n",
        "        FSectionLexMI = open('covid-mi.tsv', 'r')\n",
        "        self.LTMapRules = self.readTsv2reRules(FSectionRules, 0, 1)\n",
        "        self.LTMapLexRND = self.readTsv2reRules(FSectionLexRND, 0, 1, priority = ['KEY_N', 'M_ARG', 'EVAL'], prefix = ' ')\n",
        "        self.LTMapLexMI = self.readTsv2reRules(FSectionLexMI, 0, 1, priority = ['KEY_N', 'M_ARG', 'EVAL'], prefix = ' ')\n",
        "        count = self.runMain(data, FTsvOutput, samplesize = 100)\n",
        "        print(str(count))\n",
        "\n",
        "        return\n",
        "\n",
        "    def readTsv2dict(self, FInTSV, IFieldLeft, IFieldRight):\n",
        "        DMap = {}\n",
        "        for SLine in FInTSV:\n",
        "            SLine = SLine.strip()\n",
        "            LLine = re.split('\\t', SLine)\n",
        "            try:\n",
        "                k = LLine[IFieldLeft]\n",
        "                v = LLine[IFieldRight]\n",
        "                DMap[k] = v\n",
        "            except:\n",
        "                continue\n",
        "        return DMap\n",
        "\n",
        "    def readTsv2reRules(self, FInTSV, IFieldLeft, IFieldRight, priority = ['Introduction', 'Statements', 'Methods', 'Conclusion', 'Discussion', 'Presentation', 'Background', 'Results', 'Remove'], prefix = None, suffix = None):\n",
        "        LTMap = []\n",
        "        DRe = {}\n",
        "        # initialising dictionary where the mapping will be done, then reversing\n",
        "        for el in priority:\n",
        "            DRe[el] = [] # first the list is empty\n",
        "        for SLine in FInTSV:\n",
        "            SLine = SLine.strip()\n",
        "            LLine = re.split('\\t', SLine)\n",
        "            try:\n",
        "                v = LLine[IFieldLeft]; v = v.strip()\n",
        "                k = LLine[IFieldRight]; k = k.strip()\n",
        "                if prefix:\n",
        "                    v = prefix + v\n",
        "                if suffix:\n",
        "                    v = v + suffix\n",
        "            except:\n",
        "                sys.stdout.write('f')\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                LWords2map = DRe[k]\n",
        "                LWords2map.append(v) # adding new word for the mapping\n",
        "                DRe[k] = LWords2map\n",
        "            except:\n",
        "                sys.stdout.write('d')\n",
        "                continue\n",
        "\n",
        "        for el in priority:\n",
        "            print('el:' + el)\n",
        "            LRe = DRe[el]\n",
        "            SRe2map = '|'.join(LRe)\n",
        "            print('\\t' + SRe2map)\n",
        "            RE4map = re.compile(SRe2map)\n",
        "            TMap = (el, RE4map)\n",
        "            LTMap.append(TMap)\n",
        "\n",
        "        return LTMap\n",
        "\n",
        "    # use instead the function readTsv2reRules (above)\n",
        "    def readTsv2lexMatch(self, FInTSV, IFieldLeft, IFieldRight, priority = ['KEY_N', 'EVAL', 'M_ARG']):\n",
        "        LTMap = []\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def runMain(self, data, FTsvOutput, samplesize = 100):\n",
        "        soup = BeautifulSoup(data)\n",
        "        i = 0\n",
        "        gnorm = 0\n",
        "        grule = 0\n",
        "        gnomatch = 0\n",
        "        # FTsvOutput.write(str(docnumber) + '\\t' + str(section_number) + '\\t' + str(count_samples) + '\\t' + str(lensample) + '\\t' + str(lendict)  + '\\t' + str(SCatRnd)  + '\\t' + str(SCatMI) + '\\t' + SCatNorm + '\\t' + SSectNameNorm + '\\t' + sw + str(SCatRndWords)  + '\\t' + str(SCatMIWords) + '\\n')\n",
        "        # new string (found words in front)\n",
        "        # FTsvOutput.write(str(docnumber) + '\\t' + str(section_number) + '\\t' + str(count_samples) + '\\t' + str(lensample) + '\\t' + str(lendict)  + '\\t' + str(SCatRnd)  + '\\t' + str(SCatMI) + '\\t' + SCatNorm + '\\t' + SSectNameNorm + '\\t' + str(SCatRndWords)  + '\\t' + str(SCatMIWords) + sw + '\\n')\n",
        "\n",
        "        FTsvOutput.write('docnumber' + '\\t' + 'section_number' + '\\t' + 'count_samples' + '\\t' + 'lensample' + '\\t' + 'lendict'  + '\\t' + 'nKN_Rnd'  + '\\t' + 'nMA_Rnd'  + '\\t' + 'nEV_Rnd'  + '\\t' + 'nKN_MI'  + '\\t' + 'nMA_MI'  + '\\t' + 'nEV_MI' + '\\t' + 'SCatNorm' + '\\t' + 'SSectNameNorm' + '\\t' + 'wKN_RndWords' + '\\t' + 'wMA_RndWords' + '\\t' + 'wEV_RndWords'  + '\\t' + 'wKN_MIWords' + '\\t' + 'wMA_MIWords' + '\\t' + 'wEV_MIWords' + 'sw' + '\\n')\n",
        "        for doc in soup.find_all('doc'):\n",
        "            # print(type(str(doc)))\n",
        "            i+=1\n",
        "            norm, rule, nomatch = self.procDoc(str(doc), FTsvOutput, i, samplesize)\n",
        "            gnorm += norm; grule += rule; gnomatch += nomatch\n",
        "            if i%1000 == 0:\n",
        "                print(str(i))\n",
        "            # print(str(i))\n",
        "        print(f'\\tgnorm={gnorm}\\tgrule={grule}\\tgnonorm={gnomatch}')\n",
        "        return i\n",
        "\n",
        "    def procDoc(self, SDoc, FTsvOutput, docnumber, samplesize = 100):\n",
        "        SDoc = SDoc.replace('\\n', ' ')\n",
        "        SDoc = SDoc.replace('\\t', ' ')\n",
        "        # LSections = re.findall('<section sname=\\\"([^\\\"]+)\\\">', SDoc, re.DOTALL | re.MULTILINE | re.IGNORECASE)\n",
        "        LTSections = re.findall('<section sname=\\\"([^\\\"]+)\\\">([^<]+)</section>', SDoc, re.DOTALL | re.MULTILINE | re.IGNORECASE)\n",
        "        # LSections = re.findall('<section([^<]+)</section>', SDoc, re.DOTALL | re.MULTILINE | re.IGNORECASE)\n",
        "        norm = 0\n",
        "        nomatch = 0\n",
        "        rule = 0\n",
        "        section_number = 0\n",
        "        for TSection in LTSections:\n",
        "            SSectName, SSectionText = TSection\n",
        "            SCatFound = False\n",
        "            section_number += 1\n",
        "\n",
        "            if SSectName in self.DSectionMap.keys():\n",
        "                SSectNameNorm = self.DSectionMap[SSectName]\n",
        "                norm += 1\n",
        "                SCatNorm = 'MAP'\n",
        "                SCatFound = True\n",
        "\n",
        "            else:\n",
        "                for category, regexpression in self.LTMapRules:\n",
        "                    if re.search(regexpression, SSectName):\n",
        "                        SSectNameNorm = category\n",
        "                        SCatNorm = 'RULE'\n",
        "                        SCatFound = True\n",
        "                        rule += 1\n",
        "                        break\n",
        "                if SCatFound == False:\n",
        "                    SSectNameNorm = SSectName\n",
        "                    SCatNorm = 'NONORM'\n",
        "                    nomatch += 1\n",
        "            LSectionText = SSectionText.split(' ')\n",
        "            GLLSectionSamples = self.divide_chunks(LSectionText, samplesize)\n",
        "            LLSectionSamples = list(GLLSectionSamples)\n",
        "            LSectionSamplesLast = LLSectionSamples[-1]\n",
        "            try:\n",
        "                if len(LSectionSamplesLast) < samplesize:\n",
        "                    LLSectionSamples[-2].extend(LSectionSamplesLast)\n",
        "                    LLSectionSamples.pop()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            '''\n",
        "            print(LLSectionSamples)\n",
        "            for lw in LLSectionSamples:\n",
        "                print(len(lw))\n",
        "            print(' ')\n",
        "            '''\n",
        "            count_samples = 0\n",
        "            for lw in LLSectionSamples:\n",
        "                count_samples += 1\n",
        "                # print(len(lw))\n",
        "                # find the size of dictionary for this sample of 100 words\n",
        "                setlw = set(lw)\n",
        "                lendict = str(len(setlw))\n",
        "\n",
        "                lensample = str(len(lw))\n",
        "\n",
        "                # create string from list of 100 words\n",
        "                sw = ' '.join(lw)\n",
        "                # count key_n here!!!\n",
        "                LCatRnd = []\n",
        "                LCatMI = []\n",
        "                LCatRndWords = []\n",
        "                LCatMIWords = []\n",
        "\n",
        "                for category, regexpression in self.LTMapLexRND:\n",
        "                    LCatXr = re.findall(regexpression, sw)\n",
        "                    LCatRnd.append(str(len(LCatXr)))\n",
        "                    SCatXr = ' '.join(LCatXr)\n",
        "                    LCatRndWords.append(SCatXr)\n",
        "                for category, regexpression in self.LTMapLexMI:\n",
        "                    LCatXm = re.findall(regexpression, sw)\n",
        "                    LCatRnd.append(str(len(LCatXm)))\n",
        "                    SCatXm = ' '.join(LCatXm)\n",
        "                    LCatMIWords.append(SCatXm)\n",
        "\n",
        "                SCatRnd = '\\t'.join(LCatRnd)\n",
        "                SCatMI = '\\t'.join(LCatMI)\n",
        "                SCatRndWords = '\\t'.join(LCatRndWords)\n",
        "                SCatMIWords = '\\t'.join(LCatMIWords)\n",
        "\n",
        "                FTsvOutput.write(str(docnumber) + '\\t' + str(section_number) + '\\t' + str(count_samples) + '\\t' + str(lensample) + '\\t' + str(lendict)  + '\\t' + str(SCatRnd)  + '\\t' + str(SCatMI) + '\\t' + SCatNorm + '\\t' + SSectNameNorm + '\\t' + str(SCatRndWords)  + '\\t' + str(SCatMIWords) + sw + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "            # FTsvOutput.write(str(docnumber) + '\\t' + str(section_number) + '\\t' + SCatNorm + '\\t' + SSectNameNorm + '\\t' + SSectionText + '\\n')\n",
        "            # print(TSection)\n",
        "        print(f'norm={norm}\\trule={rule}\\tnonorm={nomatch}')\n",
        "        return norm, rule, nomatch\n",
        "\n",
        "\n",
        "    def divide_chunks(self, l, n):\n",
        "        # looping till length l\n",
        "        for i in range(0, len(l), n):\n",
        "            yield l[i:i + n]\n",
        "\n",
        "    def procSection(self, SSectionText):\n",
        "        LSectionSamples = []\n",
        "        return LSectionSamples\n",
        "\n",
        "\n",
        "    '''\n",
        "    def runMain(self, data, FTsvOutput):\n",
        "        soup = BeautifulSoup(data)\n",
        "        i = 0\n",
        "        for doc in soup.find_all('doc'):\n",
        "            # print(doc)\n",
        "            sections = soup.find_all('section', doc)\n",
        "            # children = soup.findChildren()\n",
        "            k = 0\n",
        "            for section in sections:\n",
        "                k += 1\n",
        "                # print(str(k))\n",
        "                # print(section)\n",
        "            print('\\t' + str(k))\n",
        "\n",
        "            i += 1\n",
        "            print('.')\n",
        "\n",
        "        return i\n",
        "        '''\n",
        "\n",
        "# end: class\n",
        "\n",
        "'''if __name__ == '__main__':\n",
        "    OXml2Stat = clXml2Stat('/content/cord19_1k.lem', output_file = '/content/cord19_1k_stat.txt')\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "q99nzuyFaGi1",
        "outputId": "47d3c6cc-5632-4120-b883-7ee3522237e0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"if __name__ == '__main__':\\n    OXml2Stat = clXml2Stat('/content/cord19_1k.lem', output_file = '/content/cord19_1k_stat.txt')\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OXml2Stat = clXml2Stat('/content/cord19-10k.lems', output_file = '/content/cord19_10k_stat.txt')"
      ],
      "metadata": {
        "id": "hTMIz9i1emSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=74 cord19_10k_stat.txt >cord19_1ks_stat.txt"
      ],
      "metadata": {
        "id": "76mGwVletwnX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " OXml2Stat = clXml2Stat('/content/cord19.lems', output_file = '/content/cord19_stat.txt')"
      ],
      "metadata": {
        "id": "XC1EZdLNq2TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=30 cord19_stat.txt"
      ],
      "metadata": {
        "id": "rZxCec4N0FB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Route via Json: test approach...\n",
        "## working with 100MW sample (40k texts)\n",
        "### selecting 100-word long samples, writing to JSon dictionary, mapping names, recording"
      ],
      "metadata": {
        "id": "TPVG10xsWQ0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output directory\n",
        "!mkdir document_parses/pmc_json_sample02/"
      ],
      "metadata": {
        "id": "6NGEIHrYrysv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modifying script to select 100-word long samples\n",
        "'''\n",
        "algorithm:\n",
        "    1. form sections as strings\n",
        "    2. process sections (map names), create a record\n",
        "    3. write samples in a python dictionary\n",
        "\n",
        "    ? do we need xml output to a file ?\n",
        "\n",
        "    architecture:\n",
        "      - create list of sections from list of paragraphs\n",
        "      - process each section, splitting it into samples of a pre-defined size\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "O4CjgIzOWP_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c8165460-a7b5-4b9e-bc35-57aa9476866c",
        "id": "5JlmZ1U1Wx4v"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Python script to open each file, read json input and copy to one text file for subsequent processing\n",
        "import os, re, sys\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "class clJsonDir2txtSamples(object):\n",
        "    '''\n",
        "    @author Bogdan Babych, IÜD, Heidelberg University\n",
        "    @email bogdan [dot] babych [at] iued [dot] uni-heidelberg [dot] de\n",
        "    a script for processing covid-19 corpus:\n",
        "    @url https://www.semanticscholar.org/cord19 @url https://www.semanticscholar.org/cord19/download\n",
        "        recursively reads files from a directory, and glues them together into a single corpus file\n",
        "\n",
        "    @todo:\n",
        "        working with sections - collect titles of all sections; frequent sections; select argumentative sections (e.g., discussion, analysis...)\n",
        "        - to compare descriptive and argumentative parts of the corpus\n",
        "\n",
        "        experimenting with different annotations (pos, parsing... ); MT quality evaluation...\n",
        "    '''\n",
        "    def __init__(self, SDirName, output_file = 'corpus_out.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = True, include_authors = True, tag='doc', id=1000000, split_by_docs = 0, copy_docs = 0, sample_size = 0, outjsondir = '02'): # initialising by openning the directories\n",
        "        self.SOutput_file = output_file\n",
        "        self.SOutput_file_stat = 'stat_' + output_file\n",
        "        self.STextFilter = textfilter\n",
        "        self.RFilter = re.compile(textfilter, re.IGNORECASE | re.MULTILINE)\n",
        "        self.BInclTitle = include_title # implemented\n",
        "        self.BInclSectionNames = include_sectionNames # implemented\n",
        "        self.BInclRefs = include_refs # not implemented yet\n",
        "        self.BInclAuth = include_authors # not implemented yet\n",
        "        self.STag = tag\n",
        "        self.ID = id\n",
        "        self.ISplitByDocs = int(split_by_docs)\n",
        "        self.ICopyDocs = int(copy_docs)\n",
        "        self.ISampleSize = int(sample_size)\n",
        "        self.SDirJsonOutput = outjsondir\n",
        "\n",
        "        # global dictionary of section names (to check and make rules...)\n",
        "        self.DSectNames = defaultdict(int)\n",
        "        # print(self.ISplitByDocs)\n",
        "        self.openDir(SDirName)\n",
        "        self.printDictionary(self.DSectNames, 'corpus-section-names.txt')\n",
        "        return\n",
        "\n",
        "\n",
        "    def openDir(self, path): # implementation of recursively openning directories from a given rule directory and reading each file recursively into a string\n",
        "        i = 0\n",
        "        path_sample = path + '_sample'\n",
        "        path_sample_out = path + '_sample' + self.SDirJsonOutput\n",
        "        FOutStat = open(self.SOutput_file_stat, 'w')\n",
        "        if self.ISplitByDocs:\n",
        "            SPartFile = \"part1000000\" + self.SOutput_file\n",
        "            FOut = open(SPartFile, 'w')\n",
        "        else:\n",
        "            FOut = open(self.SOutput_file, 'w')\n",
        "\n",
        "        for root,d_names,f_names in os.walk(path):\n",
        "            for f in f_names:\n",
        "                i+=1\n",
        "                if i%10000==0: print(str(i) + '. Processing: ' + f)\n",
        "                fullpath = os.path.join(root, f)\n",
        "                ## output of Json files\n",
        "                fullpathout = os.path.join(path_sample_out, f) \n",
        "                # print(fullpath)\n",
        "                print('full path output: ' + fullpathout)\n",
        "                try:\n",
        "                    ## implement : processing output file; statistics output file...\n",
        "                    FIn = open(fullpath,'r')\n",
        "                    SIn = FIn.read()\n",
        "                    # apply text filter, if not None\n",
        "                    if self.STextFilter and (re.search(self.RFilter, SIn) == None): continue\n",
        "                    DData, SText2Write = self.procFile(SIn,f,i)\n",
        "                    if SText2Write: FOut.write(SText2Write) # if the string is not empty then write to file\n",
        "                    FIn.close()\n",
        "\n",
        "                    ## writing json files with new dictionary structure\n",
        "                    ## FJsonOut = open(fullpathout, 'w')\n",
        "                    with open(fullpathout, 'w', encoding='utf-8') as jf:\n",
        "                        json.dump(DData, jf, ensure_ascii=False, indent=4)\n",
        "\n",
        "                except:\n",
        "                    print(f'file {f} cannot be read or processed')\n",
        "                finally:\n",
        "                  ## closing json output file\n",
        "                    jf.close() ## close json file\n",
        "                    # splitting output into chunks of \"split_by_docs\" size\n",
        "                    if self.ISplitByDocs and (i % self.ISplitByDocs == 0): # if self.ISplitByDocs == 0 then everything goes into one file; if this > 0 then\n",
        "                        SPartFile = \"part\" + str(1000000 + i) + self.SOutput_file # generate new file name\n",
        "                        FOut.flush()\n",
        "                        FOut.close()\n",
        "                        FOut = open(SPartFile, 'w')\n",
        "                    if self.ICopyDocs and (i >= self.ICopyDocs) and (i < (self.ICopyDocs + self.ISplitByDocs)):\n",
        "                        try:\n",
        "                            SOutputDirN = root + '_sample'\n",
        "                            SOutputFN = os.path.join(SOutputDirN, f)\n",
        "                            os.system(f'cp {fullpath} {SOutputFN}')\n",
        "                        except:\n",
        "                            print('.')\n",
        "\n",
        "        FOut.flush()\n",
        "        FOut.close()\n",
        "\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def procFile(self, SIn,SFNameIn,i): # sending each json string for extraction of text and attaching an correct tags to each output string output string\n",
        "        STagOpen = '<' + self.STag + ' id=\"' + self.STag + str(self.ID + i)  + '\">\\n'\n",
        "        STagClose = '\\n</' + self.STag + '>\\n\\n'\n",
        "        DData, SText4Corpus = self.getJson(SIn, SFNameIn)\n",
        "        STagsAndText = STagOpen + SText4Corpus + STagClose\n",
        "        if SText4Corpus:\n",
        "            return DData, STagsAndText\n",
        "        else:\n",
        "            print('\\tNo data read from: ' + SFNameIn)\n",
        "            return DData, None\n",
        "\n",
        "\n",
        "    def getJson(self, SIn, SFNameIn): # for each file-level string read from a file: managing internal structure of the covid-19 json file\n",
        "        LOut = [] # collecting a list of strings\n",
        "\n",
        "        LBodyTextSections = [] ## list of sections: Json body text\n",
        "        ### DJsonOut = {} # this will be copied from the input json structure and enriched\n",
        "\n",
        "        try:\n",
        "            DDoc = json.loads(SIn)\n",
        "        except:\n",
        "            print('\\t\\t' + SFNameIn + ' => error reading json2dictionary')\n",
        "            return {}, None\n",
        "\n",
        "        # metadata:\n",
        "        try:\n",
        "            DMetaData = DDoc['metadata']\n",
        "            if DMetaData:\n",
        "                DMetaDataOut, SMetaData = self.getJson_Metadata(DMetaData)\n",
        "                if SMetaData: LOut.append(SMetaData)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no metadata')\n",
        "            DMetaData = None\n",
        "        # body text\n",
        "        try:\n",
        "            LBodyText = DDoc['body_text']\n",
        "            if LBodyText:\n",
        "                ## implementing sampling here ~ returning list of sections first (with a paired list of section names); then sampling for sample size...\n",
        "\n",
        "                LBodyTextSections, LBodyTextSectNames, SBodyText = self.getJson_BodyText(LBodyText) ## modified function, returns 3 arguments\n",
        "                LOut.append(SBodyText)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no body_text')\n",
        "            LBodyText = None\n",
        "        # further: to implement references\n",
        "\n",
        "        SText = '\\n\\n'.join(LOut)\n",
        "        ### DJsonOut[\"body_text\"] = LBodyTextSections\n",
        "        # returning an enriched data structure\n",
        "        return DDoc, SText\n",
        "\n",
        "\n",
        "    def getJson_Metadata(self, DIn): # converts interesting parts of metadata into a string\n",
        "        SMetadata = ''\n",
        "        LMetadata = []\n",
        "        DMetaDataOut = {}\n",
        "        try: STitle = DIn[\"title\"]\n",
        "        except: STitle = None\n",
        "        if STitle and self.BInclTitle:\n",
        "            LMetadata.append(STitle)\n",
        "\n",
        "        # to implement reading of authors' names\n",
        "        '''\n",
        "        try:\n",
        "            SPaperID = DIn[\"authors\"]\n",
        "            DMetaDataOut[\"authors\"] = \"SAuthors\"\n",
        "        except:\n",
        "            print('au!')\n",
        "        '''\n",
        "\n",
        "        if LMetadata: SMetadata = '\\n\\n'.join(LMetadata)\n",
        "        return DMetaDataOut, SMetadata\n",
        "\n",
        "    ## updated function ~\n",
        "    def getJson_BodyText(self, LIn): # converts interesting parts of the body texts into a string\n",
        "        ## modified ~ for json output\n",
        "        LBodyTextSections = [] ## output - list of sections\n",
        "        LBodyTextSectionPars = [] ## element of the LBodyTextSections[] list, one section-long        \n",
        "        LBodyTextSectNames = [] ##\n",
        "        LBodyTextSectNamesM = [] ##\n",
        "\n",
        "\n",
        "        SBodyText = '' # output (old)\n",
        "        LBodyText = []\n",
        "        SSectionName0 = '' # current section name set to empty for a new text\n",
        "        # todo: later, in post-processing stage for the whole corpus, maybe after lemmatization...\n",
        "        # ISampleNumber = 0 # samples of 100 words in text; they do not cross section boundaries \n",
        "\n",
        "        for DParagraph in LIn:\n",
        "            # sections added 2022-09-28\n",
        "            # updating and adding the section name\n",
        "            try:\n",
        "                # DParagraphs[\"section\"] ## distinction between different sections....\n",
        "                SSectionName = DParagraph[\"section\"]\n",
        "                # normalizing new section name (1)\n",
        "                SSectionName = SSectionName.replace(\"\\n\", \" \")\n",
        "\n",
        "                if self.BInclSectionNames and SSectionName: # if we opted to include section names and section name is not empty\n",
        "                    if SSectionName != SSectionName0: # if we found a new section name\n",
        "                        # processing section name\n",
        "                        LBodyTextSectNames.append(SSectionName0) ## record the previous section name\n",
        "                        ## to implement here: call a function for mapping the section names\n",
        "                        LBodyTextSectNamesM.append(SSectionName0)\n",
        "\n",
        "                        SSectionName0 = SSectionName # change the current section name\n",
        "                        ## recording previous section if not empty; then adding the name of the section to a list\n",
        "\n",
        "                        SBodyTextSectionPars = '\\n'.join(LBodyTextSectionPars)\n",
        "                        LBodyTextSections.append(SBodyTextSectionPars)\n",
        "                        LBodyTextSectionPars = []\n",
        "                            \n",
        "\n",
        "                        # normalizing section name (2)\n",
        "                        SSectionNameNorm = SSectionName.lower()\n",
        "                        SSectionNameNorm = re.sub('[0-9\\.]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = re.sub('[ ]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = SSectionNameNorm.strip()\n",
        "                        \n",
        "                        self.DSectNames[SSectionNameNorm] += 1\n",
        "                        SSect4text = f'<section sName=\"{SSectionNameNorm}\">\\n{SSectionName}\\n</section>'\n",
        "\n",
        "                        LBodyText.append(SSect4text)\n",
        "            except:\n",
        "                print('S!',)\n",
        "                continue\n",
        "            # first original section (we extract text after extracting section name)\n",
        "            try:\n",
        "                ## DParagraphs[section] ## -- later on >> distinction between different sections....\n",
        "                SParagraph = DParagraph[\"text\"]\n",
        "                LBodyText.append(SParagraph)\n",
        "            except:\n",
        "                print('!',)\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "        SBodyText = '\\n\\n'.join(LBodyText)\n",
        "        return LBodyTextSections, LBodyTextSectNamesM, SBodyText\n",
        "\n",
        "    def printDictionary(self, DFreq, SFOutDict):\n",
        "        FOutDict = open(SFOutDict, 'w')\n",
        "        for key, val in sorted(DFreq.items(), key=lambda x: x[1], reverse=True):\n",
        "            FOutDict.write(f'{key}\\t{str(val)}\\n')\n",
        "        FOutDict.flush()\n",
        "        FOutDict.close()\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# arguments:\n",
        "'''\n",
        "        sys.argv[1], # obligatory: input directory name;\n",
        "            other arguments optional:\n",
        "            output_file = 'covid19corpus.txt',\n",
        "            textfilter = None, # if this is string, only texts containing it are collected, e.g., covid\n",
        "            include_title = True, # include or exclude title\n",
        "            include_refs = False, # not implemented yet: include or exclude references\n",
        "            split_by_docs=0 # split by groups of n documents; if 0 then write to one file\n",
        "\n",
        "'''\n",
        "\n",
        "'''if __name__ == '__main__':\n",
        "    OJsonDir2txt = clJsonDir2txtSamples(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=240000, sample_size = 100, outjsondir = 'document_parses/pmc_json_sample02')\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"if __name__ == '__main__':\\n    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=240000)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OJsonDir2txtSamples = clJsonDir2txtSamples(\"document_parses/pmc_json_sample\", output_file = 'cord19.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=0, sample_size = 100, outjsondir = 'document_parses/pmc_json_sample02')\n"
      ],
      "metadata": {
        "id": "zDH-FGHuh92R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}