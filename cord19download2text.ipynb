{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cord19download2text.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMkueD84gKa+XJpaFCDzFj2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/cord19/blob/main/cord19download2text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apx9q6HAbXkM"
      },
      "source": [
        "# Downloading and reading CORD19 corpus\n",
        "This notebook downloads and reads the free cord19 corpus into one file. The notebook is hosted at IÜD, Heidelberg University github repository https://github.com/iued-uni-heidelberg/cord19\n",
        "\n",
        "CORD19 (covid-19) open-source corpus is available from https://www.semanticscholar.org/cord19/download. \n",
        "\n",
        "Documentation is available at https://github.com/allenai/cord19\n",
        "\n",
        "The original files are in json format. The output file is in plain text format; documents are separated (by default) by \\<doc id=\"doc1000001\"> ... \\</doc> tags\n",
        "\n",
        "The purpose of the plain text file is for further processing, e.g., generating linguistic annotation using the TreeTagger or the Standford parser for part-of-speech annotation or dependency / constituency parsing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWGPbsx3fjpA"
      },
      "source": [
        "## Downloading CORD19 corpus\n",
        "\n",
        "The corpus is downloaded and extracted from https://www.semanticscholar.org/cord19/download\n",
        "\n",
        "Please check the link above: if you need the latest release of the corpus or if you would like to choose another release. Currently the 2021-08-30 release is downloaded.\n",
        "\n",
        "File size is ~11GB\n",
        "expected download time ~5 min\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ8alxYIfvhU",
        "outputId": "eb396dfa-cc1e-4c7b-9112-d356d3ce767d"
      },
      "source": [
        "!wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2021-08-30.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-05 10:29:02--  https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2021-08-30.tar.gz\n",
            "Resolving ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)... 52.92.128.170\n",
            "Connecting to ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)|52.92.128.170|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12069361365 (11G) [binary/octet-stream]\n",
            "Saving to: ‘cord-19_2021-08-30.tar.gz’\n",
            "\n",
            "cord-19_2021-08-30. 100%[===================>]  11.24G  34.3MB/s    in 5m 10s  \n",
            "\n",
            "2021-09-05 10:34:12 (37.2 MB/s) - ‘cord-19_2021-08-30.tar.gz’ saved [12069361365/12069361365]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FajAVEv_zgXY"
      },
      "source": [
        "Extracting cord-19 corpus, approximate time ~ 4 min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X69zODWxxYSB",
        "outputId": "d64412e6-e5cc-475a-b4b6-b8182802b0e7"
      },
      "source": [
        "!tar -xvzf cord-19_2021-08-30.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-08-30/changelog\n",
            "2021-08-30/cord_19_embeddings.tar.gz\n",
            "2021-08-30/document_parses.tar.gz\n",
            "2021-08-30/metadata.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8g4-kQyzuZN"
      },
      "source": [
        "Removing initial archive to free some disk space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOp7a1OJzHzY"
      },
      "source": [
        "!rm cord-19_2021-08-30.tar.gz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rT3UnT9zzev"
      },
      "source": [
        "Extracting document parsers, which contain individual articles in separate json files. This is expected to take ~ 12 min."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxHIVjtrzRCT"
      },
      "source": [
        "!tar -xvzf 2021-08-30/document_parses.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrJAdw7D4rOh"
      },
      "source": [
        "Removing more files to save space: ~ 9 seconds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27p3sZa04K4X"
      },
      "source": [
        "# removing more files to save space\n",
        "!rm --recursive 2021-08-30\n",
        "!rm --recursive document_parses/pdf_json"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecvdez_1e_oD"
      },
      "source": [
        "## Reading json directory and merging into text file(s)\n",
        "\n",
        "Run this cell to create the class; then run the next cell to execute on the directory \"document_parses/pmc_json\"\n",
        "\n",
        "This is a class for reading a directory with json files and writing them to a single file or split into several text file, with \"split_by_docs=N\", N documents in each file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "rW5WsBI-dNIN",
        "outputId": "a6d11369-6b5f-4c51-ffab-d82000ebc097"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Python script to open each file, read json input and copy to one text file for subsequent processing\n",
        "import os, re, sys\n",
        "import json\n",
        "\n",
        "class clJsonDir2txt(object):\n",
        "    '''\n",
        "    @author Bogdan Babych, IÜD, Heidelberg University\n",
        "    @email bogdan [dot] babych [at] iued [dot] uni-heidelberg [dot] de\n",
        "    a script for processing covid-19 corpus:\n",
        "    @url https://www.semanticscholar.org/cord19 @url https://www.semanticscholar.org/cord19/download\n",
        "        recursively reads files from a directory, and glues them together into a single corpus file\n",
        "\n",
        "    @todo:\n",
        "        working with sections - collect titles of all sections; frequent sections; select argumentative sections (e.g., discussion, analysis...)\n",
        "        - to compare descriptive and argumentative parts of the corpus\n",
        "\n",
        "        experimenting with different annotations (pos, parsing... ); MT quality evaluation...\n",
        "    '''\n",
        "    def __init__(self, SDirName, output_file = 'corpus_out.txt', include_title = True, include_refs = True, include_authors = True, tag='doc', id=1000000, split_by_docs = 0): # initialising by openning the directories\n",
        "        self.SOutput_file = output_file\n",
        "        self.BInclTitle = include_title # implemented\n",
        "        self.BInclRefs = include_refs # not implemented yet\n",
        "        self.BInclAuth = include_authors # not implemented yet\n",
        "        self.STag = tag\n",
        "        self.ID = id\n",
        "        self.ISplitByDocs = int(split_by_docs)\n",
        "        # print(self.ISplitByDocs)\n",
        "        self.openDir(SDirName)\n",
        "        return\n",
        "\n",
        "\n",
        "    def openDir(self, path): # implementation of recursively openning directories from a given rule directory and reading each file recursively into a string\n",
        "        i = 0\n",
        "        if self.ISplitByDocs:\n",
        "            SPartFile = \"part1000000\" + self.SOutput_file\n",
        "            FOut = open(SPartFile, 'w')\n",
        "        else:\n",
        "            FOut = open(self.SOutput_file, 'w')\n",
        "\n",
        "        for root,d_names,f_names in os.walk(path):\n",
        "            for f in f_names:\n",
        "                i+=1\n",
        "                if i%1000==0: print(str(i) + '. Processing: ' + f)\n",
        "                fullpath = os.path.join(root, f)\n",
        "                # print(fullpath)\n",
        "                try:\n",
        "                    FIn = open(fullpath,'r')\n",
        "                    SIn = FIn.read()\n",
        "                    SText2Write = self.procFile(SIn,f,i)\n",
        "                    if SText2Write: FOut.write(SText2Write) # if the string is not empty then write to file\n",
        "                    FIn.close()\n",
        "                except:\n",
        "                    print(f'file {f} cannot be read or processed')\n",
        "\n",
        "                # splitting output into chunks of \"split_by_docs\" size\n",
        "                if self.ISplitByDocs and (i % self.ISplitByDocs == 0): # if self.ISplitByDocs == 0 then everything goes into one file; if this > 0 then\n",
        "                    SPartFile = \"part\" + str(1000000 + i) + self.SOutput_file # generate new file name\n",
        "                    FOut.flush()\n",
        "                    FOut.close()\n",
        "                    FOut = open(SPartFile, 'w')\n",
        "        FOut.flush()\n",
        "        FOut.close()\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def procFile(self, SIn,SFNameIn,i): # sending each json string for extraction of text and attaching an correct tags to each output string output string\n",
        "        STagOpen = '<' + self.STag + ' id=\"' + self.STag + str(self.ID + i)  + '\">\\n'\n",
        "        STagClose = '\\n</' + self.STag + '>\\n\\n'\n",
        "        SText4Corpus = self.getJson(SIn, SFNameIn)\n",
        "        if SText4Corpus:\n",
        "            return STagOpen + SText4Corpus + STagClose\n",
        "        else:\n",
        "            print('\\tNo data read from: ' + SFNameIn)\n",
        "            return None\n",
        "\n",
        "\n",
        "    def getJson(self, SIn, SFNameIn): # for each file-level string read from a file: managing internal structure of the covid-19 json file\n",
        "        LOut = [] # collecting a list of strings\n",
        "        try:\n",
        "            DDoc = json.loads(SIn)\n",
        "        except:\n",
        "            print('\\t\\t' + SFNameIn + ' => error reading json2dictionary')\n",
        "            return None\n",
        "        # metadata:\n",
        "        try:\n",
        "            DMetaData = DDoc['metadata']\n",
        "            if DMetaData:\n",
        "                SMetaData = self.getJson_Metadata(DMetaData)\n",
        "                if SMetaData: LOut.append(SMetaData)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no metadata')\n",
        "            DMetaData = None\n",
        "        # body text\n",
        "        try:\n",
        "            LBodyText = DDoc['body_text']\n",
        "            if LBodyText:\n",
        "                SBodyText = self.getJson_BodyText(LBodyText)\n",
        "                LOut.append(SBodyText)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no body_text')\n",
        "            LBodyText = None\n",
        "        # further: to implement references\n",
        "\n",
        "        SText = '\\n\\n'.join(LOut)\n",
        "        return SText\n",
        "\n",
        "\n",
        "    def getJson_Metadata(self, DIn): # converts interesting parts of metadata into a string\n",
        "        SMetadata = ''\n",
        "        LMetadata = []\n",
        "        try: STitle = DIn[\"title\"]\n",
        "        except: STitle = None\n",
        "        if STitle and self.BInclTitle:\n",
        "            LMetadata.append(STitle)\n",
        "\n",
        "        # to implement reading of authors' names\n",
        "\n",
        "        if LMetadata: SMetadata = '\\n\\n'.join(LMetadata)\n",
        "        return SMetadata\n",
        "\n",
        "\n",
        "    def getJson_BodyText(self, LIn): # converts interesting parts of the body texts into a string\n",
        "        SBodyText = ''\n",
        "        LBodyText = []\n",
        "        for DParagraph in LIn:\n",
        "            try:\n",
        "                ## DParagraphs[section] ## -- later on >> distinction between different sections....\n",
        "                SParagraph = DParagraph[\"text\"]\n",
        "                LBodyText.append(SParagraph)\n",
        "            except:\n",
        "                print('!',)\n",
        "                continue\n",
        "\n",
        "        SBodyText = '\\n\\n'.join(LBodyText)\n",
        "        return SBodyText\n",
        "\n",
        "# arguments:\n",
        "'''\n",
        "        sys.argv[1], # obligatory: input directory name;\n",
        "            other arguments optional:\n",
        "            output_file = 'covid19corpus.txt',\n",
        "            include_title = True, # include or exclude title\n",
        "            include_refs = False, # not implemented yet: include or exclude references\n",
        "            split_by_docs=0 # split by groups of n documents; if 0 then write to one file\n",
        "'''\n",
        "\n",
        "'''if __name__ == '__main__':\n",
        "    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', include_title = True, include_refs = False, split_by_docs=0)\n",
        "'''\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"if __name__ == '__main__':\\n    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', include_title = True, include_refs = False, split_by_docs=0)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2zYiDkn5Vbo"
      },
      "source": [
        "This cell will executre reading of json files into a single (or multiple) files\n",
        "\n",
        "Change the value of \"split_by_docs=0\" to \"split_by_docs=10000\" or any number ; this will create several corpus files with 10000 or any required number fo documents per file, which you wish to have.\n",
        "\n",
        "\n",
        "Approximate execution time ~10 min\n",
        "\n",
        "File size to download ~4.3 GB\n",
        "\n",
        "It contains ~198.000 documents,\n",
        "\n",
        "~ 671.578.587 words\n",
        "\n",
        "~ 19.381.647 paragraphs (including empty lines, i.e., ~10M real paragraphs)\n",
        "\n",
        "~ 4.619.100.883 characters\n",
        "\n",
        "Download time can take up to 1 hour depending on your connection speed.\n",
        "\n",
        "To split into ~BNC size chunks (100MW), split into groups of ~40000 documents (in the following cell set \"split_by_docs=20000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2EUP7kS5fkW"
      },
      "source": [
        "OJsonDir2txt = clJsonDir2txt(\"document_parses/pmc_json\", output_file = 'covid19corpus.txt', include_title = True, include_refs = False, split_by_docs=40000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmZdmvgVFF1v"
      },
      "source": [
        "To see the number of words, paragraphs in your corpus you can use this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p8xxDTvAkv_",
        "outputId": "2a6f19a1-7849-42bf-a397-fd0c27e968c0"
      },
      "source": [
        "!wc covid19corpus.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  19381647  671578587 4619100883 covid19corpus.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZN2v7snE7iQ"
      },
      "source": [
        "If you have split the text into parts, you can see the number of words in each part using this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIr8NG15EVro",
        "outputId": "50d9d939-34e4-4ca1-f117-6378a61f551c"
      },
      "source": [
        "!wc part*"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   3899868  136307720  936593762 part1000000covid19corpus.txt\n",
            "   3861089  135312676  930249471 part1040000covid19corpus.txt\n",
            "   3966522  136553801  939515657 part1080000covid19corpus.txt\n",
            "   3941044  136273170  937283286 part1120000covid19corpus.txt\n",
            "   3713124  127131220  875458707 part1160000covid19corpus.txt\n",
            "  19381647  671578587 4619100883 total\n"
          ]
        }
      ]
    }
  ]
}