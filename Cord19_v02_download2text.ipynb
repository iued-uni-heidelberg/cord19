{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBEsVfHM3UACtoEKkr9fti",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/cord19/blob/main/Cord19_v02_download2text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apx9q6HAbXkM"
      },
      "source": [
        "# Downloading and reading CORD19 corpus\n",
        "This notebook downloads and reads the free cord19 corpus into one file. The notebook is hosted at IÜD, Heidelberg University github repository https://github.com/iued-uni-heidelberg/cord19\n",
        "\n",
        "CORD19 (covid-19) open-source corpus is available from https://www.semanticscholar.org/cord19/download. \n",
        "\n",
        "Documentation is available at https://github.com/allenai/cord19\n",
        "\n",
        "The original files are in json format. The output file is in plain text format; documents are separated (by default) by \\<doc id=\"doc1000001\"> ... \\</doc> tags\n",
        "\n",
        "The purpose of the plain text file is for further processing, e.g., generating linguistic annotation using the TreeTagger or the Standford parser for part-of-speech annotation or dependency / constituency parsing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWGPbsx3fjpA"
      },
      "source": [
        "## Downloading CORD19 corpus\n",
        "\n",
        "The corpus is downloaded and extracted from https://www.semanticscholar.org/cord19/download\n",
        "\n",
        "Please check the link above: if you need the latest release of the corpus or if you would like to choose another release. Currently the 2022-06-02 release is downloaded.\n",
        "\n",
        "File size is ~11GB (v2021-08-30)\n",
        "File size is ~18GB (v2022-06-02)\n",
        "expected download time ~9 min\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ8alxYIfvhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5132574-2c07-46dc-9700-dd215bb6f60a"
      },
      "source": [
        "# !wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2021-08-30.tar.gz\n",
        "!wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2022-06-02.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-04 06:58:59--  https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2022-06-02.tar.gz\n",
            "Resolving ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)... 52.92.145.162\n",
            "Connecting to ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)|52.92.145.162|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18657952487 (17G) [binary/octet-stream]\n",
            "Saving to: ‘cord-19_2022-06-02.tar.gz’\n",
            "\n",
            "cord-19_2022-06-02. 100%[===================>]  17.38G  33.1MB/s    in 8m 5s   \n",
            "\n",
            "2022-10-04 07:07:04 (36.7 MB/s) - ‘cord-19_2022-06-02.tar.gz’ saved [18657952487/18657952487]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FajAVEv_zgXY"
      },
      "source": [
        "Extracting cord-19 corpus, approximate time ~ 4 min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X69zODWxxYSB",
        "outputId": "8bcbf29a-9495-434d-c282-5542c47512dd"
      },
      "source": [
        "# !tar -xvzf cord-19_2021-08-30.tar.gz\n",
        "!tar -xvzf cord-19_2022-06-02.tar.gz 2022-06-02/document_parses.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-02/document_parses.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8g4-kQyzuZN"
      },
      "source": [
        "Removing initial archive to free some disk space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOp7a1OJzHzY"
      },
      "source": [
        "# !rm cord-19_2021-08-30.tar.gz\n",
        "!rm cord-19_2022-06-02.tar.gz\n",
        "!mv 2022-06-02/document_parses.tar.gz ./document_parses.tar.gz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrJAdw7D4rOh"
      },
      "source": [
        "Removing more files to save space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27p3sZa04K4X"
      },
      "source": [
        "# removing more files to save space\n",
        "# !rm --recursive 2021-08-30\n",
        "# !rm --recursive document_parses/pdf_json\n",
        "!rm --recursive 2022-06-02"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rT3UnT9zzev"
      },
      "source": [
        "Extracting document parsers, which contain individual articles in separate json files. This is expected to take ~ 9+ min."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !tar -tvf document_parses.tar.gz >document_parses.txt"
      ],
      "metadata": {
        "id": "B_HdqkFt1_-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxHIVjtrzRCT"
      },
      "source": [
        "# !tar -xvzf 2021-08-30/document_parses.tar.gz\n",
        "!tar -xvzf document_parses.tar.gz document_parses/pmc_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output:\n",
        "\n",
        "PMC1054884.xml.json\n",
        "\n",
        "PMC1065028.xml.json\n",
        "\n",
        "PMC1065064.xml.json\n",
        "\n",
        "PMC1065120.xml.json\n",
        "\n",
        "PMC1065257.xml.json\n",
        "\n",
        "PMC1072802.xml.json\n",
        "\n",
        "PMC1072806.xml.json\n",
        "\n",
        "PMC1072807.xml.json\n",
        "\n",
        "PMC1074505.xml.json\n",
        "\n",
        "PMC1074749.xml.json\n",
        "...\n",
        "\n",
        "~ 28G of json files\n"
      ],
      "metadata": {
        "id": "e_byMXi95YBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ls /content/document_parses/pmc_json >pms_json.txt"
      ],
      "metadata": {
        "id": "ZNK5Eoct5dTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm --recursive document_parses/pmc_json\n",
        "# !rm --recursive document_parses/pdf_json\n",
        "!rm document_parses.tar.gz"
      ],
      "metadata": {
        "id": "JsuPYd05utfQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh /content/document_parses/pmc_json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFKmfiIA9Q7a",
        "outputId": "e3f9ba80-75b9-44a3-b549-14abe1dbb96d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28G\t/content/document_parses/pmc_json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/document_parses/pmc_json_sample"
      ],
      "metadata": {
        "id": "3dstcJcr-MJ8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecvdez_1e_oD"
      },
      "source": [
        "## Reading json directory and merging into text file(s)\n",
        "\n",
        "Run this cell to create the class; then run the next cell to execute on the directory \"document_parses/pmc_json\"\n",
        "\n",
        "This is a class for reading a directory with json files and writing them to a single file or split into several text file, with \"split_by_docs=N\", N documents in each file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "rW5WsBI-dNIN",
        "outputId": "c8165460-a7b5-4b9e-bc35-57aa9476866c"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Python script to open each file, read json input and copy to one text file for subsequent processing\n",
        "import os, re, sys\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "class clJsonDir2txt(object):\n",
        "    '''\n",
        "    @author Bogdan Babych, IÜD, Heidelberg University\n",
        "    @email bogdan [dot] babych [at] iued [dot] uni-heidelberg [dot] de\n",
        "    a script for processing covid-19 corpus:\n",
        "    @url https://www.semanticscholar.org/cord19 @url https://www.semanticscholar.org/cord19/download\n",
        "        recursively reads files from a directory, and glues them together into a single corpus file\n",
        "\n",
        "    @todo:\n",
        "        working with sections - collect titles of all sections; frequent sections; select argumentative sections (e.g., discussion, analysis...)\n",
        "        - to compare descriptive and argumentative parts of the corpus\n",
        "\n",
        "        experimenting with different annotations (pos, parsing... ); MT quality evaluation...\n",
        "    '''\n",
        "    def __init__(self, SDirName, output_file = 'corpus_out.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = True, include_authors = True, tag='doc', id=1000000, split_by_docs = 0, copy_docs = 0): # initialising by openning the directories\n",
        "        self.SOutput_file = output_file\n",
        "        self.STextFilter = textfilter\n",
        "        self.RFilter = re.compile(textfilter, re.IGNORECASE | re.MULTILINE)\n",
        "        self.BInclTitle = include_title # implemented\n",
        "        self.BInclSectionNames = include_sectionNames # implemented\n",
        "        self.BInclRefs = include_refs # not implemented yet\n",
        "        self.BInclAuth = include_authors # not implemented yet\n",
        "        self.STag = tag\n",
        "        self.ID = id\n",
        "        self.ISplitByDocs = int(split_by_docs)\n",
        "        self.ICopyDocs = int(copy_docs)\n",
        "        # global dictionary of section names (to check and make rules...)\n",
        "        self.DSectNames = defaultdict(int)\n",
        "        # print(self.ISplitByDocs)\n",
        "        self.openDir(SDirName)\n",
        "        self.printDictionary(self.DSectNames, 'corpus-section-names.txt')\n",
        "        return\n",
        "\n",
        "\n",
        "    def openDir(self, path): # implementation of recursively openning directories from a given rule directory and reading each file recursively into a string\n",
        "        i = 0\n",
        "        path_sample = path + '_sample'\n",
        "        if self.ISplitByDocs:\n",
        "            SPartFile = \"part1000000\" + self.SOutput_file\n",
        "            FOut = open(SPartFile, 'w')\n",
        "        else:\n",
        "            FOut = open(self.SOutput_file, 'w')\n",
        "\n",
        "        for root,d_names,f_names in os.walk(path):\n",
        "            for f in f_names:\n",
        "                i+=1\n",
        "                if i%10000==0: print(str(i) + '. Processing: ' + f)\n",
        "                fullpath = os.path.join(root, f)\n",
        "                # print(fullpath)\n",
        "                try:\n",
        "                    FIn = open(fullpath,'r')\n",
        "                    SIn = FIn.read()\n",
        "                    # apply text filter, if not None\n",
        "                    if self.STextFilter and (re.search(self.RFilter, SIn) == None): continue\n",
        "                    SText2Write = self.procFile(SIn,f,i)\n",
        "                    if SText2Write: FOut.write(SText2Write) # if the string is not empty then write to file\n",
        "                    FIn.close()\n",
        "                except:\n",
        "                    print(f'file {f} cannot be read or processed')\n",
        "                finally:\n",
        "                    # splitting output into chunks of \"split_by_docs\" size\n",
        "                    if self.ISplitByDocs and (i % self.ISplitByDocs == 0): # if self.ISplitByDocs == 0 then everything goes into one file; if this > 0 then\n",
        "                        SPartFile = \"part\" + str(1000000 + i) + self.SOutput_file # generate new file name\n",
        "                        FOut.flush()\n",
        "                        FOut.close()\n",
        "                        FOut = open(SPartFile, 'w')\n",
        "                    if self.ICopyDocs and (i >= self.ICopyDocs) and (i < (self.ICopyDocs + self.ISplitByDocs)):\n",
        "                        try:\n",
        "                            SOutputDirN = root + '_sample'\n",
        "                            SOutputFN = os.path.join(SOutputDirN, f)\n",
        "                            os.system(f'cp {fullpath} {SOutputFN}')\n",
        "                        except:\n",
        "                            print('.')\n",
        "\n",
        "        FOut.flush()\n",
        "        FOut.close()\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def procFile(self, SIn,SFNameIn,i): # sending each json string for extraction of text and attaching an correct tags to each output string output string\n",
        "        STagOpen = '<' + self.STag + ' id=\"' + self.STag + str(self.ID + i)  + '\">\\n'\n",
        "        STagClose = '\\n</' + self.STag + '>\\n\\n'\n",
        "        SText4Corpus = self.getJson(SIn, SFNameIn)\n",
        "        if SText4Corpus:\n",
        "            return STagOpen + SText4Corpus + STagClose\n",
        "        else:\n",
        "            print('\\tNo data read from: ' + SFNameIn)\n",
        "            return None\n",
        "\n",
        "\n",
        "    def getJson(self, SIn, SFNameIn): # for each file-level string read from a file: managing internal structure of the covid-19 json file\n",
        "        LOut = [] # collecting a list of strings\n",
        "        try:\n",
        "            DDoc = json.loads(SIn)\n",
        "        except:\n",
        "            print('\\t\\t' + SFNameIn + ' => error reading json2dictionary')\n",
        "            return None\n",
        "        # metadata:\n",
        "        try:\n",
        "            DMetaData = DDoc['metadata']\n",
        "            if DMetaData:\n",
        "                SMetaData = self.getJson_Metadata(DMetaData)\n",
        "                if SMetaData: LOut.append(SMetaData)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no metadata')\n",
        "            DMetaData = None\n",
        "        # body text\n",
        "        try:\n",
        "            LBodyText = DDoc['body_text']\n",
        "            if LBodyText:\n",
        "                SBodyText = self.getJson_BodyText(LBodyText)\n",
        "                LOut.append(SBodyText)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no body_text')\n",
        "            LBodyText = None\n",
        "        # further: to implement references\n",
        "\n",
        "        SText = '\\n\\n'.join(LOut)\n",
        "        return SText\n",
        "\n",
        "\n",
        "    def getJson_Metadata(self, DIn): # converts interesting parts of metadata into a string\n",
        "        SMetadata = ''\n",
        "        LMetadata = []\n",
        "        try: STitle = DIn[\"title\"]\n",
        "        except: STitle = None\n",
        "        if STitle and self.BInclTitle:\n",
        "            LMetadata.append(STitle)\n",
        "\n",
        "        # to implement reading of authors' names\n",
        "\n",
        "        if LMetadata: SMetadata = '\\n\\n'.join(LMetadata)\n",
        "        return SMetadata\n",
        "\n",
        "\n",
        "    def getJson_BodyText(self, LIn): # converts interesting parts of the body texts into a string\n",
        "        SBodyText = ''\n",
        "        LBodyText = []\n",
        "        SSectionName0 = '' # current section name set to empty for a new text\n",
        "        # todo: later, in post-processing stage for the whole corpus, maybe after lemmatization...\n",
        "        # ISampleNumber = 0 # samples of 100 words in text; they do not cross section boundaries \n",
        "\n",
        "        for DParagraph in LIn:\n",
        "            # sections added 2022-09-28\n",
        "            try:\n",
        "                # DParagraphs[\"section\"] ## distinction between different sections....\n",
        "                SSectionName = DParagraph[\"section\"]\n",
        "                # normalizing new section name (1)\n",
        "                SSectionName = SSectionName.replace(\"\\n\", \" \")\n",
        "\n",
        "                if self.BInclSectionNames and SSectionName: # if we opted to include section names and section name is not empty\n",
        "                    if SSectionName != SSectionName0: # if we found a new section name\n",
        "                        # processing section name\n",
        "                        SSectionName0 = SSectionName # change the current section name\n",
        "\n",
        "                        # normalizing section name (2)\n",
        "                        SSectionNameNorm = SSectionName.lower()\n",
        "                        SSectionNameNorm = re.sub('[0-9\\.]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = re.sub('[ ]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = SSectionNameNorm.strip()\n",
        "                        \n",
        "                        self.DSectNames[SSectionNameNorm] += 1\n",
        "                        SSect4text = f'<section sName=\"{SSectionNameNorm}\">\\n{SSectionName}\\n</section>'\n",
        "\n",
        "                        LBodyText.append(SSect4text)\n",
        "            except:\n",
        "                print('S!',)\n",
        "                continue\n",
        "            # first original section (we extract text after extracting section name)\n",
        "            try:\n",
        "                ## DParagraphs[section] ## -- later on >> distinction between different sections....\n",
        "                SParagraph = DParagraph[\"text\"]\n",
        "                LBodyText.append(SParagraph)\n",
        "            except:\n",
        "                print('!',)\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "        SBodyText = '\\n\\n'.join(LBodyText)\n",
        "        return SBodyText\n",
        "\n",
        "    def printDictionary(self, DFreq, SFOutDict):\n",
        "        FOutDict = open(SFOutDict, 'w')\n",
        "        for key, val in sorted(DFreq.items(), key=lambda x: x[1], reverse=True):\n",
        "            FOutDict.write(f'{key}\\t{str(val)}\\n')\n",
        "        FOutDict.flush()\n",
        "        FOutDict.close()\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# arguments:\n",
        "'''\n",
        "        sys.argv[1], # obligatory: input directory name;\n",
        "            other arguments optional:\n",
        "            output_file = 'covid19corpus.txt',\n",
        "            textfilter = None, # if this is string, only texts containing it are collected, e.g., covid\n",
        "            include_title = True, # include or exclude title\n",
        "            include_refs = False, # not implemented yet: include or exclude references\n",
        "            split_by_docs=0 # split by groups of n documents; if 0 then write to one file\n",
        "\n",
        "'''\n",
        "\n",
        "'''if __name__ == '__main__':\n",
        "    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=240000)\n",
        "'''\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"if __name__ == '__main__':\\n    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=240000)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2zYiDkn5Vbo"
      },
      "source": [
        "## numbers from previous version\n",
        "This cell will executre reading of json files into a single (or multiple) files\n",
        "\n",
        "Change the value of \"split_by_docs=0\" to \"split_by_docs=10000\" or any number ; this will create several corpus files with 10000 or any required number fo documents per file, which you wish to have.\n",
        "\n",
        "\n",
        "Approximate execution time ~10 min\n",
        "\n",
        "\n",
        "File size to download ~4.3 GB\n",
        "\n",
        "It contains ~198.000 documents,\n",
        "\n",
        "~ 671.578.587 words\n",
        "\n",
        "~ 19.381.647 paragraphs (including empty lines, i.e., ~10M real paragraphs)\n",
        "\n",
        "~ 4.619.100.883 characters\n",
        "\n",
        "## numbers for the last version\n",
        "Approximate execution time ~20 min\n",
        "\n",
        "8 BNC-size (100mw) files are generted, containg together\n",
        "\n",
        "~ 315.000 documents\n",
        "\n",
        "  827.118.629 words\n",
        "\n",
        "   22.094.653 paragraphs\n",
        "\n",
        "5.650.921.315 characters\n",
        "\n",
        "\n",
        "Download time can take up to 1 hour depending on your connection speed.\n",
        "\n",
        "To split into ~BNC size chunks (100MW), split into groups of ~40000 documents (in the following cell set \"split_by_docs=20000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2EUP7kS5fkW"
      },
      "source": [
        "# remove parameter textfilter='covid', to return all documents\n",
        "# change parameter split_by_docs=40000 to split_by_docs=0 to return a single file instead of ~5 parts with <40000 in each\n",
        "OJsonDir2txt = clJsonDir2txt(\"document_parses/pmc_json\", output_file = 'cord19.txt', textfilter='covid', include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=40000, copy_docs=240000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "writing frequent section names (ordered by descending frequency, from highest to 1)"
      ],
      "metadata": {
        "id": "eTbi965g2zqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls document_parses/pmc_json_sample | wc -l\n",
        "!du -sh document_parses/pmc_json_sample"
      ],
      "metadata": {
        "id": "wkPWJaJLOG_e",
        "outputId": "4dbe8a19-b61f-4c12-b8d8-8662d476eb3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40000\n",
            "3.6G\tdocument_parses/pmc_json_sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvzf document_parses_sample.tar.gz document_parses/pmc_json_sample"
      ],
      "metadata": {
        "id": "ZCW92xAFPQtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=75 /content/corpus-section-names.txt"
      ],
      "metadata": {
        "id": "kJeGtnHK5BJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=1000 /content/corpus-section-names.txt > corpus-selection-names-top1000.txt"
      ],
      "metadata": {
        "id": "6xEeB_bip1sj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmZdmvgVFF1v"
      },
      "source": [
        "To see the number of words, paragraphs in your corpus you can use this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p8xxDTvAkv_"
      },
      "source": [
        "# !wc covid19corpus.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZN2v7snE7iQ"
      },
      "source": [
        "If you have split the text into parts, you can see the number of words in each part using this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIr8NG15EVro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24190963-c9aa-40be-9711-0de7a45380f5"
      },
      "source": [
        "!wc part*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   4046267  109976546  758142306 part1000000cord19.txt\n",
            "   4071521  109845759  758310406 part1040000cord19.txt\n",
            "   4113561  110019721  759706810 part1080000cord19.txt\n",
            "   4099076  109716696  757381748 part1120000cord19.txt\n",
            "   4059810  109054692  752368164 part1160000cord19.txt\n",
            "   4138586  110435615  762458277 part1200000cord19.txt\n",
            "   4125613  110233019  761062284 part1240000cord19.txt\n",
            "   3676023   98148739  678626560 part1280000cord19.txt\n",
            "  32330457  867430787 5988056555 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip part1000000cord19.txt\n",
        "!gzip part1040000cord19.txt\n",
        "!gzip part1080000cord19.txt\n",
        "!gzip part1120000cord19.txt\n",
        "!gzip part1160000cord19.txt\n",
        "!gzip part1200000cord19.txt\n",
        "!gzip part1240000cord19.txt\n",
        "!gzip part1280000cord19.txt"
      ],
      "metadata": {
        "id": "TB_sGebiyb08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=500000 part1240000cord19.txt >part1240000cord19-500k.txt"
      ],
      "metadata": {
        "id": "0ELJw_EvK133"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "selecting some examples to experiment with..."
      ],
      "metadata": {
        "id": "I8j1MY6I2uSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/document_parses/pmc_json/PMC9034168.xml.json PMC9034168.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7128104.xml.json PMC7128104.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8769777.xml.json PMC8769777.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7926205.xml.json PMC7926205.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8799642.xml.json PMC8799642.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7124374.xml.json PMC7124374.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8812323.xml.json PMC8812323.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7446676.xml.json PMC7446676.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7436596.xml.json PMC7436596.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8808276.xml.json PMC8808276.xml.json"
      ],
      "metadata": {
        "id": "L3fUdPdlJZrL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}