{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN5DLd71FG4nRUQPfp7qHCw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/cord19/blob/main/Cord19_v02_download2text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apx9q6HAbXkM"
      },
      "source": [
        "# Downloading and reading CORD19 corpus\n",
        "This notebook downloads and reads the free cord19 corpus into one file. The notebook is hosted at IÜD, Heidelberg University github repository https://github.com/iued-uni-heidelberg/cord19\n",
        "\n",
        "CORD19 (covid-19) open-source corpus is available from https://www.semanticscholar.org/cord19/download. \n",
        "\n",
        "Documentation is available at https://github.com/allenai/cord19\n",
        "\n",
        "The original files are in json format. The output file is in plain text format; documents are separated (by default) by \\<doc id=\"doc1000001\"> ... \\</doc> tags\n",
        "\n",
        "The purpose of the plain text file is for further processing, e.g., generating linguistic annotation using the TreeTagger or the Standford parser for part-of-speech annotation or dependency / constituency parsing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWGPbsx3fjpA"
      },
      "source": [
        "## Downloading CORD19 corpus\n",
        "\n",
        "The corpus is downloaded and extracted from https://www.semanticscholar.org/cord19/download\n",
        "\n",
        "Please check the link above: if you need the latest release of the corpus or if you would like to choose another release. Currently the 2022-06-02 release is downloaded.\n",
        "\n",
        "File size is ~11GB (v2021-08-30)\n",
        "File size is ~18GB (v2022-06-02)\n",
        "expected download time ~9 min\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ8alxYIfvhU",
        "outputId": "99d828b2-b5e1-43c1-fb77-9110d349b1c2"
      },
      "source": [
        "# !wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2021-08-30.tar.gz\n",
        "!wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2022-06-02.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-27 09:14:35--  https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2022-06-02.tar.gz\n",
            "Resolving ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)... 52.218.137.105\n",
            "Connecting to ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)|52.218.137.105|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18657952487 (17G) [binary/octet-stream]\n",
            "Saving to: ‘cord-19_2022-06-02.tar.gz’\n",
            "\n",
            "          cord-19_2  11%[=>                  ]   2.03G  33.0MB/s    eta 7m 38s "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FajAVEv_zgXY"
      },
      "source": [
        "Extracting cord-19 corpus, approximate time ~ 4 min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X69zODWxxYSB",
        "outputId": "d64412e6-e5cc-475a-b4b6-b8182802b0e7"
      },
      "source": [
        "!tar -xvzf cord-19_2021-08-30.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-08-30/changelog\n",
            "2021-08-30/cord_19_embeddings.tar.gz\n",
            "2021-08-30/document_parses.tar.gz\n",
            "2021-08-30/metadata.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8g4-kQyzuZN"
      },
      "source": [
        "Removing initial archive to free some disk space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOp7a1OJzHzY"
      },
      "source": [
        "!rm cord-19_2021-08-30.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rT3UnT9zzev"
      },
      "source": [
        "Extracting document parsers, which contain individual articles in separate json files. This is expected to take ~ 12 min."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxHIVjtrzRCT"
      },
      "source": [
        "!tar -xvzf 2021-08-30/document_parses.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrJAdw7D4rOh"
      },
      "source": [
        "Removing more files to save space: ~ 9 seconds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27p3sZa04K4X"
      },
      "source": [
        "# removing more files to save space\n",
        "!rm --recursive 2021-08-30\n",
        "!rm --recursive document_parses/pdf_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecvdez_1e_oD"
      },
      "source": [
        "## Reading json directory and merging into text file(s)\n",
        "\n",
        "Run this cell to create the class; then run the next cell to execute on the directory \"document_parses/pmc_json\"\n",
        "\n",
        "This is a class for reading a directory with json files and writing them to a single file or split into several text file, with \"split_by_docs=N\", N documents in each file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "rW5WsBI-dNIN",
        "outputId": "6c796e0c-1202-4446-e9da-de3fe69deb8b"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Python script to open each file, read json input and copy to one text file for subsequent processing\n",
        "import os, re, sys\n",
        "import json\n",
        "\n",
        "class clJsonDir2txt(object):\n",
        "    '''\n",
        "    @author Bogdan Babych, IÜD, Heidelberg University\n",
        "    @email bogdan [dot] babych [at] iued [dot] uni-heidelberg [dot] de\n",
        "    a script for processing covid-19 corpus:\n",
        "    @url https://www.semanticscholar.org/cord19 @url https://www.semanticscholar.org/cord19/download\n",
        "        recursively reads files from a directory, and glues them together into a single corpus file\n",
        "\n",
        "    @todo:\n",
        "        working with sections - collect titles of all sections; frequent sections; select argumentative sections (e.g., discussion, analysis...)\n",
        "        - to compare descriptive and argumentative parts of the corpus\n",
        "\n",
        "        experimenting with different annotations (pos, parsing... ); MT quality evaluation...\n",
        "    '''\n",
        "    def __init__(self, SDirName, output_file = 'corpus_out.txt', textfilter=None, include_title = True, include_refs = True, include_authors = True, tag='doc', id=1000000, split_by_docs = 0): # initialising by openning the directories\n",
        "        self.SOutput_file = output_file\n",
        "        self.STextFilter = textfilter\n",
        "        self.RFilter = re.compile(textfilter, re.IGNORECASE | re.MULTILINE)\n",
        "        self.BInclTitle = include_title # implemented\n",
        "        self.BInclRefs = include_refs # not implemented yet\n",
        "        self.BInclAuth = include_authors # not implemented yet\n",
        "        self.STag = tag\n",
        "        self.ID = id\n",
        "        self.ISplitByDocs = int(split_by_docs)\n",
        "        # print(self.ISplitByDocs)\n",
        "        self.openDir(SDirName)\n",
        "        return\n",
        "\n",
        "\n",
        "    def openDir(self, path): # implementation of recursively openning directories from a given rule directory and reading each file recursively into a string\n",
        "        i = 0\n",
        "        if self.ISplitByDocs:\n",
        "            SPartFile = \"part1000000\" + self.SOutput_file\n",
        "            FOut = open(SPartFile, 'w')\n",
        "        else:\n",
        "            FOut = open(self.SOutput_file, 'w')\n",
        "\n",
        "        for root,d_names,f_names in os.walk(path):\n",
        "            for f in f_names:\n",
        "                i+=1\n",
        "                if i%1000==0: print(str(i) + '. Processing: ' + f)\n",
        "                fullpath = os.path.join(root, f)\n",
        "                # print(fullpath)\n",
        "                try:\n",
        "                    FIn = open(fullpath,'r')\n",
        "                    SIn = FIn.read()\n",
        "                    # apply text filter, if not None\n",
        "                    if self.STextFilter and (re.search(self.RFilter, SIn) == None): continue\n",
        "                    SText2Write = self.procFile(SIn,f,i)\n",
        "                    if SText2Write: FOut.write(SText2Write) # if the string is not empty then write to file\n",
        "                    FIn.close()\n",
        "                except:\n",
        "                    print(f'file {f} cannot be read or processed')\n",
        "                finally:\n",
        "                    # splitting output into chunks of \"split_by_docs\" size\n",
        "                    if self.ISplitByDocs and (i % self.ISplitByDocs == 0): # if self.ISplitByDocs == 0 then everything goes into one file; if this > 0 then\n",
        "                        SPartFile = \"part\" + str(1000000 + i) + self.SOutput_file # generate new file name\n",
        "                        FOut.flush()\n",
        "                        FOut.close()\n",
        "                        FOut = open(SPartFile, 'w')\n",
        "        FOut.flush()\n",
        "        FOut.close()\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def procFile(self, SIn,SFNameIn,i): # sending each json string for extraction of text and attaching an correct tags to each output string output string\n",
        "        STagOpen = '<' + self.STag + ' id=\"' + self.STag + str(self.ID + i)  + '\">\\n'\n",
        "        STagClose = '\\n</' + self.STag + '>\\n\\n'\n",
        "        SText4Corpus = self.getJson(SIn, SFNameIn)\n",
        "        if SText4Corpus:\n",
        "            return STagOpen + SText4Corpus + STagClose\n",
        "        else:\n",
        "            print('\\tNo data read from: ' + SFNameIn)\n",
        "            return None\n",
        "\n",
        "\n",
        "    def getJson(self, SIn, SFNameIn): # for each file-level string read from a file: managing internal structure of the covid-19 json file\n",
        "        LOut = [] # collecting a list of strings\n",
        "        try:\n",
        "            DDoc = json.loads(SIn)\n",
        "        except:\n",
        "            print('\\t\\t' + SFNameIn + ' => error reading json2dictionary')\n",
        "            return None\n",
        "        # metadata:\n",
        "        try:\n",
        "            DMetaData = DDoc['metadata']\n",
        "            if DMetaData:\n",
        "                SMetaData = self.getJson_Metadata(DMetaData)\n",
        "                if SMetaData: LOut.append(SMetaData)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no metadata')\n",
        "            DMetaData = None\n",
        "        # body text\n",
        "        try:\n",
        "            LBodyText = DDoc['body_text']\n",
        "            if LBodyText:\n",
        "                SBodyText = self.getJson_BodyText(LBodyText)\n",
        "                LOut.append(SBodyText)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no body_text')\n",
        "            LBodyText = None\n",
        "        # further: to implement references\n",
        "\n",
        "        SText = '\\n\\n'.join(LOut)\n",
        "        return SText\n",
        "\n",
        "\n",
        "    def getJson_Metadata(self, DIn): # converts interesting parts of metadata into a string\n",
        "        SMetadata = ''\n",
        "        LMetadata = []\n",
        "        try: STitle = DIn[\"title\"]\n",
        "        except: STitle = None\n",
        "        if STitle and self.BInclTitle:\n",
        "            LMetadata.append(STitle)\n",
        "\n",
        "        # to implement reading of authors' names\n",
        "\n",
        "        if LMetadata: SMetadata = '\\n\\n'.join(LMetadata)\n",
        "        return SMetadata\n",
        "\n",
        "\n",
        "    def getJson_BodyText(self, LIn): # converts interesting parts of the body texts into a string\n",
        "        SBodyText = ''\n",
        "        LBodyText = []\n",
        "        for DParagraph in LIn:\n",
        "            try:\n",
        "                ## DParagraphs[section] ## -- later on >> distinction between different sections....\n",
        "                SParagraph = DParagraph[\"text\"]\n",
        "                LBodyText.append(SParagraph)\n",
        "            except:\n",
        "                print('!',)\n",
        "                continue\n",
        "\n",
        "        SBodyText = '\\n\\n'.join(LBodyText)\n",
        "        return SBodyText\n",
        "\n",
        "# arguments:\n",
        "'''\n",
        "        sys.argv[1], # obligatory: input directory name;\n",
        "            other arguments optional:\n",
        "            output_file = 'covid19corpus.txt',\n",
        "            textfilter = None, # if this is string, only texts containing it are collected, e.g., covid\n",
        "            include_title = True, # include or exclude title\n",
        "            include_refs = False, # not implemented yet: include or exclude references\n",
        "            split_by_docs=0 # split by groups of n documents; if 0 then write to one file\n",
        "\n",
        "'''\n",
        "\n",
        "'''if __name__ == '__main__':\n",
        "    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_refs = False, split_by_docs=0)\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"if __name__ == '__main__':\\n    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_refs = False, split_by_docs=0)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2zYiDkn5Vbo"
      },
      "source": [
        "This cell will executre reading of json files into a single (or multiple) files\n",
        "\n",
        "Change the value of \"split_by_docs=0\" to \"split_by_docs=10000\" or any number ; this will create several corpus files with 10000 or any required number fo documents per file, which you wish to have.\n",
        "\n",
        "\n",
        "Approximate execution time ~10 min\n",
        "\n",
        "File size to download ~4.3 GB\n",
        "\n",
        "It contains ~198.000 documents,\n",
        "\n",
        "~ 671.578.587 words\n",
        "\n",
        "~ 19.381.647 paragraphs (including empty lines, i.e., ~10M real paragraphs)\n",
        "\n",
        "~ 4.619.100.883 characters\n",
        "\n",
        "Download time can take up to 1 hour depending on your connection speed.\n",
        "\n",
        "To split into ~BNC size chunks (100MW), split into groups of ~40000 documents (in the following cell set \"split_by_docs=20000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2EUP7kS5fkW",
        "outputId": "184953b6-a52c-41ec-f457-e5dc66a5e481"
      },
      "source": [
        "# remove parameter textfilter='covid', to return all documents\n",
        "# change parameter split_by_docs=40000 to split_by_docs=0 to return a single file instead of ~5 parts with <40000 in each\n",
        "OJsonDir2txt = clJsonDir2txt(\"document_parses/pmc_json\", output_file = 'covid19corpFiltCOVID.txt', textfilter='covid', include_title = True, include_refs = False, split_by_docs=40000)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000. Processing: PMC8328126.xml.json\n",
            "2000. Processing: PMC8378831.xml.json\n",
            "3000. Processing: PMC7885334.xml.json\n",
            "4000. Processing: PMC7123675.xml.json\n",
            "5000. Processing: PMC8237894.xml.json\n",
            "6000. Processing: PMC4629194.xml.json\n",
            "7000. Processing: PMC7423848.xml.json\n",
            "8000. Processing: PMC8284347.xml.json\n",
            "9000. Processing: PMC8278374.xml.json\n",
            "10000. Processing: PMC1570461.xml.json\n",
            "11000. Processing: PMC7744420.xml.json\n",
            "12000. Processing: PMC7158367.xml.json\n",
            "13000. Processing: PMC7869758.xml.json\n",
            "14000. Processing: PMC7585733.xml.json\n",
            "15000. Processing: PMC7871039.xml.json\n",
            "16000. Processing: PMC7119026.xml.json\n",
            "17000. Processing: PMC7705351.xml.json\n",
            "18000. Processing: PMC3657891.xml.json\n",
            "19000. Processing: PMC7443920.xml.json\n",
            "20000. Processing: PMC7152080.xml.json\n",
            "21000. Processing: PMC8012999.xml.json\n",
            "22000. Processing: PMC8204832.xml.json\n",
            "23000. Processing: PMC8330914.xml.json\n",
            "24000. Processing: PMC7156231.xml.json\n",
            "25000. Processing: PMC7195042.xml.json\n",
            "26000. Processing: PMC8341344.xml.json\n",
            "27000. Processing: PMC7962418.xml.json\n",
            "28000. Processing: PMC7972699.xml.json\n",
            "29000. Processing: PMC7516084.xml.json\n",
            "30000. Processing: PMC8065505.xml.json\n",
            "31000. Processing: PMC8139434.xml.json\n",
            "32000. Processing: PMC8310257.xml.json\n",
            "33000. Processing: PMC7114247.xml.json\n",
            "34000. Processing: PMC5707224.xml.json\n",
            "35000. Processing: PMC7107964.xml.json\n",
            "36000. Processing: PMC8216795.xml.json\n",
            "37000. Processing: PMC8242644.xml.json\n",
            "38000. Processing: PMC7688298.xml.json\n",
            "39000. Processing: PMC8110188.xml.json\n",
            "40000. Processing: PMC7495188.xml.json\n",
            "41000. Processing: PMC6270622.xml.json\n",
            "42000. Processing: PMC7547551.xml.json\n",
            "43000. Processing: PMC7162744.xml.json\n",
            "44000. Processing: PMC7218365.xml.json\n",
            "45000. Processing: PMC7498113.xml.json\n",
            "46000. Processing: PMC7167173.xml.json\n",
            "47000. Processing: PMC8122205.xml.json\n",
            "48000. Processing: PMC8379572.xml.json\n",
            "49000. Processing: PMC7596573.xml.json\n",
            "50000. Processing: PMC8393523.xml.json\n",
            "51000. Processing: PMC8185154.xml.json\n",
            "52000. Processing: PMC8233587.xml.json\n",
            "53000. Processing: PMC7957463.xml.json\n",
            "54000. Processing: PMC7650208.xml.json\n",
            "55000. Processing: PMC7817669.xml.json\n",
            "56000. Processing: PMC8122682.xml.json\n",
            "57000. Processing: PMC7971905.xml.json\n",
            "58000. Processing: PMC7167020.xml.json\n",
            "59000. Processing: PMC8090526.xml.json\n",
            "60000. Processing: PMC5856488.xml.json\n",
            "61000. Processing: PMC7117251.xml.json\n",
            "62000. Processing: PMC7169139.xml.json\n",
            "63000. Processing: PMC7521072.xml.json\n",
            "64000. Processing: PMC7491024.xml.json\n",
            "65000. Processing: PMC7548063.xml.json\n",
            "66000. Processing: PMC7095898.xml.json\n",
            "67000. Processing: PMC7974840.xml.json\n",
            "68000. Processing: PMC7978783.xml.json\n",
            "69000. Processing: PMC8001669.xml.json\n",
            "70000. Processing: PMC5097846.xml.json\n",
            "71000. Processing: PMC7340084.xml.json\n",
            "72000. Processing: PMC7923186.xml.json\n",
            "73000. Processing: PMC7523158.xml.json\n",
            "74000. Processing: PMC7920223.xml.json\n",
            "75000. Processing: PMC7565235.xml.json\n",
            "76000. Processing: PMC8395041.xml.json\n",
            "77000. Processing: PMC8043565.xml.json\n",
            "78000. Processing: PMC7404464.xml.json\n",
            "79000. Processing: PMC7901760.xml.json\n",
            "80000. Processing: PMC8103302.xml.json\n",
            "81000. Processing: PMC7132659.xml.json\n",
            "82000. Processing: PMC8007320.xml.json\n",
            "83000. Processing: PMC7368899.xml.json\n",
            "84000. Processing: PMC8193012.xml.json\n",
            "85000. Processing: PMC7682772.xml.json\n",
            "86000. Processing: PMC7801046.xml.json\n",
            "87000. Processing: PMC7094105.xml.json\n",
            "88000. Processing: PMC7872935.xml.json\n",
            "89000. Processing: PMC7158175.xml.json\n",
            "90000. Processing: PMC7912416.xml.json\n",
            "91000. Processing: PMC7251350.xml.json\n",
            "92000. Processing: PMC8276696.xml.json\n",
            "93000. Processing: PMC7503345.xml.json\n",
            "94000. Processing: PMC7575419.xml.json\n",
            "95000. Processing: PMC7303610.xml.json\n",
            "96000. Processing: PMC7561703.xml.json\n",
            "97000. Processing: PMC6942430.xml.json\n",
            "98000. Processing: PMC7089399.xml.json\n",
            "99000. Processing: PMC7669911.xml.json\n",
            "100000. Processing: PMC7149989.xml.json\n",
            "101000. Processing: PMC8215884.xml.json\n",
            "102000. Processing: PMC8007413.xml.json\n",
            "103000. Processing: PMC7267673.xml.json\n",
            "104000. Processing: PMC4768256.xml.json\n",
            "105000. Processing: PMC8107953.xml.json\n",
            "106000. Processing: PMC7983986.xml.json\n",
            "107000. Processing: PMC8192105.xml.json\n",
            "108000. Processing: PMC7172400.xml.json\n",
            "109000. Processing: PMC4322817.xml.json\n",
            "110000. Processing: PMC8006198.xml.json\n",
            "111000. Processing: PMC7203719.xml.json\n",
            "112000. Processing: PMC7601200.xml.json\n",
            "113000. Processing: PMC7727606.xml.json\n",
            "114000. Processing: PMC7529057.xml.json\n",
            "115000. Processing: PMC7485637.xml.json\n",
            "116000. Processing: PMC8280662.xml.json\n",
            "117000. Processing: PMC8358439.xml.json\n",
            "118000. Processing: PMC7543786.xml.json\n",
            "119000. Processing: PMC7746989.xml.json\n",
            "120000. Processing: PMC8006669.xml.json\n",
            "121000. Processing: PMC7858040.xml.json\n",
            "122000. Processing: PMC7498226.xml.json\n",
            "123000. Processing: PMC8092326.xml.json\n",
            "124000. Processing: PMC7416023.xml.json\n",
            "125000. Processing: PMC7105060.xml.json\n",
            "126000. Processing: PMC8186798.xml.json\n",
            "127000. Processing: PMC7834269.xml.json\n",
            "128000. Processing: PMC8382194.xml.json\n",
            "129000. Processing: PMC7912123.xml.json\n",
            "130000. Processing: PMC8219317.xml.json\n",
            "131000. Processing: PMC7432467.xml.json\n",
            "132000. Processing: PMC7088171.xml.json\n",
            "133000. Processing: PMC7903365.xml.json\n",
            "134000. Processing: PMC7508231.xml.json\n",
            "135000. Processing: PMC7802986.xml.json\n",
            "136000. Processing: PMC8169340.xml.json\n",
            "137000. Processing: PMC7991370.xml.json\n",
            "138000. Processing: PMC7514286.xml.json\n",
            "139000. Processing: PMC7382241.xml.json\n",
            "140000. Processing: PMC5017074.xml.json\n",
            "141000. Processing: PMC7173099.xml.json\n",
            "142000. Processing: PMC7199785.xml.json\n",
            "143000. Processing: PMC7334984.xml.json\n",
            "144000. Processing: PMC8035613.xml.json\n",
            "145000. Processing: PMC3083680.xml.json\n",
            "146000. Processing: PMC8043830.xml.json\n",
            "147000. Processing: PMC8398060.xml.json\n",
            "148000. Processing: PMC8202008.xml.json\n",
            "149000. Processing: PMC8395413.xml.json\n",
            "150000. Processing: PMC2582734.xml.json\n",
            "151000. Processing: PMC8239833.xml.json\n",
            "152000. Processing: PMC7606072.xml.json\n",
            "153000. Processing: PMC8237057.xml.json\n",
            "154000. Processing: PMC7706120.xml.json\n",
            "155000. Processing: PMC4456770.xml.json\n",
            "156000. Processing: PMC7293761.xml.json\n",
            "157000. Processing: PMC5789847.xml.json\n",
            "158000. Processing: PMC7240265.xml.json\n",
            "159000. Processing: PMC7123743.xml.json\n",
            "160000. Processing: PMC7194622.xml.json\n",
            "161000. Processing: PMC7500538.xml.json\n",
            "162000. Processing: PMC7662584.xml.json\n",
            "163000. Processing: PMC7724668.xml.json\n",
            "164000. Processing: PMC8356220.xml.json\n",
            "165000. Processing: PMC3651868.xml.json\n",
            "166000. Processing: PMC7435286.xml.json\n",
            "167000. Processing: PMC7115809.xml.json\n",
            "168000. Processing: PMC7440090.xml.json\n",
            "169000. Processing: PMC7605181.xml.json\n",
            "170000. Processing: PMC7972461.xml.json\n",
            "171000. Processing: PMC8150955.xml.json\n",
            "172000. Processing: PMC7206655.xml.json\n",
            "173000. Processing: PMC8121884.xml.json\n",
            "174000. Processing: PMC7887714.xml.json\n",
            "175000. Processing: PMC7172456.xml.json\n",
            "176000. Processing: PMC8344578.xml.json\n",
            "177000. Processing: PMC8101992.xml.json\n",
            "178000. Processing: PMC7128819.xml.json\n",
            "179000. Processing: PMC7423874.xml.json\n",
            "180000. Processing: PMC7119551.xml.json\n",
            "181000. Processing: PMC8312222.xml.json\n",
            "182000. Processing: PMC7731629.xml.json\n",
            "183000. Processing: PMC7273261.xml.json\n",
            "184000. Processing: PMC8079127.xml.json\n",
            "185000. Processing: PMC7832731.xml.json\n",
            "186000. Processing: PMC7972658.xml.json\n",
            "187000. Processing: PMC4429008.xml.json\n",
            "188000. Processing: PMC7875171.xml.json\n",
            "189000. Processing: PMC8219292.xml.json\n",
            "190000. Processing: PMC2958806.xml.json\n",
            "191000. Processing: PMC7510183.xml.json\n",
            "192000. Processing: PMC8270108.xml.json\n",
            "193000. Processing: PMC7654320.xml.json\n",
            "194000. Processing: PMC7795268.xml.json\n",
            "195000. Processing: PMC6677726.xml.json\n",
            "196000. Processing: PMC7405317.xml.json\n",
            "197000. Processing: PMC7115285.xml.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmZdmvgVFF1v"
      },
      "source": [
        "To see the number of words, paragraphs in your corpus you can use this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p8xxDTvAkv_",
        "outputId": "16b13713-a883-4408-a14e-090470d2fa48"
      },
      "source": [
        "!wc covid19corpus.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  11353308  428465423 2922833953 covid19corpus.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZN2v7snE7iQ"
      },
      "source": [
        "If you have split the text into parts, you can see the number of words in each part using this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIr8NG15EVro",
        "outputId": "6c0e1a96-0051-45bf-878d-9f8deae44651"
      },
      "source": [
        "!wc part*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    2330707    87381483   596272676 part1000000covid19corpFiltCOVID.txt\n",
            "    2330707    87381483   596272676 part1000000covid19corpusFilterCOVID.txt\n",
            "    3899868   136307720   936593762 part1000000covid19corpus.txt\n",
            "    2263357    86800310   591503574 part1040000covid19corpFiltCOVID.txt\n",
            "    2263357    86800310   591503574 part1040000covid19corpusFilterCOVID.txt\n",
            "    3861089   135312676   930249471 part1040000covid19corpus.txt\n",
            "    2296496    86650535   591056082 part1080000covid19corpFiltCOVID.txt\n",
            "    2296496    86650535   591056082 part1080000covid19corpusFilterCOVID.txt\n",
            "    3966522   136553801   939515657 part1080000covid19corpus.txt\n",
            "    2282277    86724659   590829890 part1120000covid19corpFiltCOVID.txt\n",
            "    2282277    86724659   590829890 part1120000covid19corpusFilterCOVID.txt\n",
            "    3941044   136273170   937283286 part1120000covid19corpus.txt\n",
            "    2180471    80908436   553171731 part1160000covid19corpFiltCOVID.txt\n",
            "    2180471    80908436   553171731 part1160000covid19corpusFilterCOVID.txt\n",
            "    3713124   127131220   875458707 part1160000covid19corpus.txt\n",
            "   42088263  1528509433 10464768789 total\n"
          ]
        }
      ]
    }
  ]
}