{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM92P2HYaKzTu/s78HhYPtf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/cord19/blob/main/Cord19_v02_download2text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apx9q6HAbXkM"
      },
      "source": [
        "# Downloading and reading CORD19 corpus\n",
        "This notebook downloads and reads the free cord19 corpus into one file. The notebook is hosted at IÜD, Heidelberg University github repository https://github.com/iued-uni-heidelberg/cord19\n",
        "\n",
        "CORD19 (covid-19) open-source corpus is available from https://www.semanticscholar.org/cord19/download. \n",
        "\n",
        "Documentation is available at https://github.com/allenai/cord19\n",
        "\n",
        "The original files are in json format. The output file is in plain text format; documents are separated (by default) by \\<doc id=\"doc1000001\"> ... \\</doc> tags\n",
        "\n",
        "The purpose of the plain text file is for further processing, e.g., generating linguistic annotation using the TreeTagger or the Standford parser for part-of-speech annotation or dependency / constituency parsing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWGPbsx3fjpA"
      },
      "source": [
        "## Downloading CORD19 corpus\n",
        "\n",
        "The corpus is downloaded and extracted from https://www.semanticscholar.org/cord19/download\n",
        "\n",
        "Please check the link above: if you need the latest release of the corpus or if you would like to choose another release. Currently the 2022-06-02 release is downloaded.\n",
        "\n",
        "File size is ~11GB (v2021-08-30)\n",
        "File size is ~18GB (v2022-06-02)\n",
        "expected download time ~9 min\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ8alxYIfvhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32763cc1-4635-47a6-ccb1-77df2d0cee21"
      },
      "source": [
        "# !wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2021-08-30.tar.gz\n",
        "!wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2022-06-02.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-28 07:59:24--  https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2022-06-02.tar.gz\n",
            "Resolving ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)... 52.218.180.153\n",
            "Connecting to ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)|52.218.180.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18657952487 (17G) [binary/octet-stream]\n",
            "Saving to: ‘cord-19_2022-06-02.tar.gz’\n",
            "\n",
            "cord-19_2022-06-02. 100%[===================>]  17.38G  33.9MB/s    in 8m 31s  \n",
            "\n",
            "2022-09-28 08:07:55 (34.8 MB/s) - ‘cord-19_2022-06-02.tar.gz’ saved [18657952487/18657952487]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FajAVEv_zgXY"
      },
      "source": [
        "Extracting cord-19 corpus, approximate time ~ 4 min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X69zODWxxYSB",
        "outputId": "fae2d5f5-b066-4925-d7c1-64014596e475"
      },
      "source": [
        "# !tar -xvzf cord-19_2021-08-30.tar.gz\n",
        "!tar -xvzf cord-19_2022-06-02.tar.gz 2022-06-02/document_parses.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-02/document_parses.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8g4-kQyzuZN"
      },
      "source": [
        "Removing initial archive to free some disk space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOp7a1OJzHzY"
      },
      "source": [
        "# !rm cord-19_2021-08-30.tar.gz\n",
        "!rm cord-19_2022-06-02.tar.gz\n",
        "!mv 2022-06-02/document_parses.tar.gz ./document_parses.tar.gz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrJAdw7D4rOh"
      },
      "source": [
        "Removing more files to save space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27p3sZa04K4X"
      },
      "source": [
        "# removing more files to save space\n",
        "# !rm --recursive 2021-08-30\n",
        "# !rm --recursive document_parses/pdf_json\n",
        "!rm --recursive 2022-06-02"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rT3UnT9zzev"
      },
      "source": [
        "Extracting document parsers, which contain individual articles in separate json files. This is expected to take ~ 9+ min."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !tar -tvf document_parses.tar.gz >document_parses.txt"
      ],
      "metadata": {
        "id": "B_HdqkFt1_-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxHIVjtrzRCT"
      },
      "source": [
        "# !tar -xvzf 2021-08-30/document_parses.tar.gz\n",
        "!tar -xvzf document_parses.tar.gz document_parses/pmc_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output:\n",
        "\n",
        "PMC1054884.xml.json\n",
        "\n",
        "PMC1065028.xml.json\n",
        "\n",
        "PMC1065064.xml.json\n",
        "\n",
        "PMC1065120.xml.json\n",
        "\n",
        "PMC1065257.xml.json\n",
        "\n",
        "PMC1072802.xml.json\n",
        "\n",
        "PMC1072806.xml.json\n",
        "\n",
        "PMC1072807.xml.json\n",
        "\n",
        "PMC1074505.xml.json\n",
        "\n",
        "PMC1074749.xml.json\n",
        "...\n",
        "\n",
        "~ 28G of json files\n"
      ],
      "metadata": {
        "id": "e_byMXi95YBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ls /content/document_parses/pmc_json >pms_json.txt"
      ],
      "metadata": {
        "id": "ZNK5Eoct5dTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm --recursive document_parses/pmc_json\n",
        "# !rm --recursive document_parses/pdf_json\n",
        "!rm document_parses.tar.gz"
      ],
      "metadata": {
        "id": "JsuPYd05utfQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh /content/document_parses/pmc_json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFKmfiIA9Q7a",
        "outputId": "9607a3e5-06a8-46dd-ec83-e17aa430ad7f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28G\t/content/document_parses/pmc_json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecvdez_1e_oD"
      },
      "source": [
        "## Reading json directory and merging into text file(s)\n",
        "\n",
        "Run this cell to create the class; then run the next cell to execute on the directory \"document_parses/pmc_json\"\n",
        "\n",
        "This is a class for reading a directory with json files and writing them to a single file or split into several text file, with \"split_by_docs=N\", N documents in each file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "rW5WsBI-dNIN",
        "outputId": "80bfe33e-a8f6-4a28-fc58-a305796d6804"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Python script to open each file, read json input and copy to one text file for subsequent processing\n",
        "import os, re, sys\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "class clJsonDir2txt(object):\n",
        "    '''\n",
        "    @author Bogdan Babych, IÜD, Heidelberg University\n",
        "    @email bogdan [dot] babych [at] iued [dot] uni-heidelberg [dot] de\n",
        "    a script for processing covid-19 corpus:\n",
        "    @url https://www.semanticscholar.org/cord19 @url https://www.semanticscholar.org/cord19/download\n",
        "        recursively reads files from a directory, and glues them together into a single corpus file\n",
        "\n",
        "    @todo:\n",
        "        working with sections - collect titles of all sections; frequent sections; select argumentative sections (e.g., discussion, analysis...)\n",
        "        - to compare descriptive and argumentative parts of the corpus\n",
        "\n",
        "        experimenting with different annotations (pos, parsing... ); MT quality evaluation...\n",
        "    '''\n",
        "    def __init__(self, SDirName, output_file = 'corpus_out.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = True, include_authors = True, tag='doc', id=1000000, split_by_docs = 0): # initialising by openning the directories\n",
        "        self.SOutput_file = output_file\n",
        "        self.STextFilter = textfilter\n",
        "        self.RFilter = re.compile(textfilter, re.IGNORECASE | re.MULTILINE)\n",
        "        self.BInclTitle = include_title # implemented\n",
        "        self.BInclSectionNames = include_sectionNames # implemented\n",
        "        self.BInclRefs = include_refs # not implemented yet\n",
        "        self.BInclAuth = include_authors # not implemented yet\n",
        "        self.STag = tag\n",
        "        self.ID = id\n",
        "        self.ISplitByDocs = int(split_by_docs)\n",
        "        # global dictionary of section names (to check and make rules...)\n",
        "        self.DSectNames = defaultdict(int)\n",
        "        # print(self.ISplitByDocs)\n",
        "        self.openDir(SDirName)\n",
        "        self.printDictionary(self.DSectNames, 'corpus-section-names.txt')\n",
        "        return\n",
        "\n",
        "\n",
        "    def openDir(self, path): # implementation of recursively openning directories from a given rule directory and reading each file recursively into a string\n",
        "        i = 0\n",
        "        if self.ISplitByDocs:\n",
        "            SPartFile = \"part1000000\" + self.SOutput_file\n",
        "            FOut = open(SPartFile, 'w')\n",
        "        else:\n",
        "            FOut = open(self.SOutput_file, 'w')\n",
        "\n",
        "        for root,d_names,f_names in os.walk(path):\n",
        "            for f in f_names:\n",
        "                i+=1\n",
        "                if i%10000==0: print(str(i) + '. Processing: ' + f)\n",
        "                fullpath = os.path.join(root, f)\n",
        "                # print(fullpath)\n",
        "                try:\n",
        "                    FIn = open(fullpath,'r')\n",
        "                    SIn = FIn.read()\n",
        "                    # apply text filter, if not None\n",
        "                    if self.STextFilter and (re.search(self.RFilter, SIn) == None): continue\n",
        "                    SText2Write = self.procFile(SIn,f,i)\n",
        "                    if SText2Write: FOut.write(SText2Write) # if the string is not empty then write to file\n",
        "                    FIn.close()\n",
        "                except:\n",
        "                    print(f'file {f} cannot be read or processed')\n",
        "                finally:\n",
        "                    # splitting output into chunks of \"split_by_docs\" size\n",
        "                    if self.ISplitByDocs and (i % self.ISplitByDocs == 0): # if self.ISplitByDocs == 0 then everything goes into one file; if this > 0 then\n",
        "                        SPartFile = \"part\" + str(1000000 + i) + self.SOutput_file # generate new file name\n",
        "                        FOut.flush()\n",
        "                        FOut.close()\n",
        "                        FOut = open(SPartFile, 'w')\n",
        "        FOut.flush()\n",
        "        FOut.close()\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def procFile(self, SIn,SFNameIn,i): # sending each json string for extraction of text and attaching an correct tags to each output string output string\n",
        "        STagOpen = '<' + self.STag + ' id=\"' + self.STag + str(self.ID + i)  + '\">\\n'\n",
        "        STagClose = '\\n</' + self.STag + '>\\n\\n'\n",
        "        SText4Corpus = self.getJson(SIn, SFNameIn)\n",
        "        if SText4Corpus:\n",
        "            return STagOpen + SText4Corpus + STagClose\n",
        "        else:\n",
        "            print('\\tNo data read from: ' + SFNameIn)\n",
        "            return None\n",
        "\n",
        "\n",
        "    def getJson(self, SIn, SFNameIn): # for each file-level string read from a file: managing internal structure of the covid-19 json file\n",
        "        LOut = [] # collecting a list of strings\n",
        "        try:\n",
        "            DDoc = json.loads(SIn)\n",
        "        except:\n",
        "            print('\\t\\t' + SFNameIn + ' => error reading json2dictionary')\n",
        "            return None\n",
        "        # metadata:\n",
        "        try:\n",
        "            DMetaData = DDoc['metadata']\n",
        "            if DMetaData:\n",
        "                SMetaData = self.getJson_Metadata(DMetaData)\n",
        "                if SMetaData: LOut.append(SMetaData)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no metadata')\n",
        "            DMetaData = None\n",
        "        # body text\n",
        "        try:\n",
        "            LBodyText = DDoc['body_text']\n",
        "            if LBodyText:\n",
        "                SBodyText = self.getJson_BodyText(LBodyText)\n",
        "                LOut.append(SBodyText)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no body_text')\n",
        "            LBodyText = None\n",
        "        # further: to implement references\n",
        "\n",
        "        SText = '\\n\\n'.join(LOut)\n",
        "        return SText\n",
        "\n",
        "\n",
        "    def getJson_Metadata(self, DIn): # converts interesting parts of metadata into a string\n",
        "        SMetadata = ''\n",
        "        LMetadata = []\n",
        "        try: STitle = DIn[\"title\"]\n",
        "        except: STitle = None\n",
        "        if STitle and self.BInclTitle:\n",
        "            LMetadata.append(STitle)\n",
        "\n",
        "        # to implement reading of authors' names\n",
        "\n",
        "        if LMetadata: SMetadata = '\\n\\n'.join(LMetadata)\n",
        "        return SMetadata\n",
        "\n",
        "\n",
        "    def getJson_BodyText(self, LIn): # converts interesting parts of the body texts into a string\n",
        "        SBodyText = ''\n",
        "        LBodyText = []\n",
        "        SSectionName0 = '' # current section name set to empty for a new text\n",
        "        # todo: later, in post-processing stage for the whole corpus, maybe after lemmatization...\n",
        "        # ISampleNumber = 0 # samples of 100 words in text; they do not cross section boundaries \n",
        "\n",
        "        for DParagraph in LIn:\n",
        "            # sections added 2022-09-28\n",
        "            try:\n",
        "                # DParagraphs[\"section\"] ## distinction between different sections....\n",
        "                SSectionName = DParagraph[\"section\"]\n",
        "                # normalizing new section name (1)\n",
        "                SSectionName = SSectionName.replace(\"\\n\", \" \")\n",
        "\n",
        "                if self.BInclSectionNames and SSectionName: # if we opted to include section names and section name is not empty\n",
        "                    if SSectionName != SSectionName0: # if we found a new section name\n",
        "                        # processing section name\n",
        "                        SSectionName0 = SSectionName # change the current section name\n",
        "\n",
        "                        # normalizing section name (2)\n",
        "                        SSectionNameNorm = SSectionName.lower()\n",
        "                        SSectionNameNorm = re.sub('[0-9\\.]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = re.sub('[ ]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = SSectionName.strip()\n",
        "                        \n",
        "                        self.DSectNames[SSectionNameNorm] += 1\n",
        "                        SSect4text = f'<section sName=\"{SSectionNameNorm}\">\\n{SSectionName}\\n</section>'\n",
        "\n",
        "                        LBodyText.append(SSect4text)\n",
        "            except:\n",
        "                print('S!',)\n",
        "                continue\n",
        "            # first original section (we extract text after extracting section name)\n",
        "            try:\n",
        "                ## DParagraphs[section] ## -- later on >> distinction between different sections....\n",
        "                SParagraph = DParagraph[\"text\"]\n",
        "                LBodyText.append(SParagraph)\n",
        "            except:\n",
        "                print('!',)\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "        SBodyText = '\\n\\n'.join(LBodyText)\n",
        "        return SBodyText\n",
        "\n",
        "    def printDictionary(self, DFreq, SFOutDict):\n",
        "        FOutDict = open(SFOutDict, 'w')\n",
        "        for key, val in sorted(DFreq.items(), key=lambda x: x[1], reverse=True):\n",
        "            FOutDict.write(f'{key}\\t{str(val)}\\n')\n",
        "        FOutDict.flush()\n",
        "        FOutDict.close()\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# arguments:\n",
        "'''\n",
        "        sys.argv[1], # obligatory: input directory name;\n",
        "            other arguments optional:\n",
        "            output_file = 'covid19corpus.txt',\n",
        "            textfilter = None, # if this is string, only texts containing it are collected, e.g., covid\n",
        "            include_title = True, # include or exclude title\n",
        "            include_refs = False, # not implemented yet: include or exclude references\n",
        "            split_by_docs=0 # split by groups of n documents; if 0 then write to one file\n",
        "\n",
        "'''\n",
        "\n",
        "'''if __name__ == '__main__':\n",
        "    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0)\n",
        "'''\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"if __name__ == '__main__':\\n    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2zYiDkn5Vbo"
      },
      "source": [
        "## numbers from previous version\n",
        "This cell will executre reading of json files into a single (or multiple) files\n",
        "\n",
        "Change the value of \"split_by_docs=0\" to \"split_by_docs=10000\" or any number ; this will create several corpus files with 10000 or any required number fo documents per file, which you wish to have.\n",
        "\n",
        "\n",
        "Approximate execution time ~10 min\n",
        "\n",
        "\n",
        "File size to download ~4.3 GB\n",
        "\n",
        "It contains ~198.000 documents,\n",
        "\n",
        "~ 671.578.587 words\n",
        "\n",
        "~ 19.381.647 paragraphs (including empty lines, i.e., ~10M real paragraphs)\n",
        "\n",
        "~ 4.619.100.883 characters\n",
        "\n",
        "## numbers for the last version\n",
        "Approximate execution time ~20 min\n",
        "\n",
        "8 BNC-size (100mw) files are generted, containg together\n",
        "\n",
        "~ 315.000 documents\n",
        "\n",
        "  827.118.629 words\n",
        "\n",
        "   22.094.653 paragraphs\n",
        "\n",
        "5.650.921.315 characters\n",
        "\n",
        "\n",
        "Download time can take up to 1 hour depending on your connection speed.\n",
        "\n",
        "To split into ~BNC size chunks (100MW), split into groups of ~40000 documents (in the following cell set \"split_by_docs=20000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2EUP7kS5fkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bfdb9fb-ae6c-4b79-ef76-b8f93f038737"
      },
      "source": [
        "# remove parameter textfilter='covid', to return all documents\n",
        "# change parameter split_by_docs=40000 to split_by_docs=0 to return a single file instead of ~5 parts with <40000 in each\n",
        "OJsonDir2txt = clJsonDir2txt(\"document_parses/pmc_json\", output_file = 'cord19.txt', textfilter='covid', include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=40000)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000. Processing: PMC8444872.xml.json\n",
            "20000. Processing: PMC8410421.xml.json\n",
            "30000. Processing: PMC3731014.xml.json\n",
            "40000. Processing: PMC7992063.xml.json\n",
            "50000. Processing: PMC8757646.xml.json\n",
            "60000. Processing: PMC8065667.xml.json\n",
            "70000. Processing: PMC8010268.xml.json\n",
            "80000. Processing: PMC7390520.xml.json\n",
            "90000. Processing: PMC8547401.xml.json\n",
            "100000. Processing: PMC7091702.xml.json\n",
            "110000. Processing: PMC7428357.xml.json\n",
            "120000. Processing: PMC8831869.xml.json\n",
            "130000. Processing: PMC7299119.xml.json\n",
            "140000. Processing: PMC8996994.xml.json\n",
            "150000. Processing: PMC8723907.xml.json\n",
            "160000. Processing: PMC7149318.xml.json\n",
            "170000. Processing: PMC8468281.xml.json\n",
            "180000. Processing: PMC7119139.xml.json\n",
            "190000. Processing: PMC7129760.xml.json\n",
            "200000. Processing: PMC7121762.xml.json\n",
            "210000. Processing: PMC7917725.xml.json\n",
            "220000. Processing: PMC7201396.xml.json\n",
            "230000. Processing: PMC8844019.xml.json\n",
            "240000. Processing: PMC8139375.xml.json\n",
            "250000. Processing: PMC7736911.xml.json\n",
            "260000. Processing: PMC8295010.xml.json\n",
            "270000. Processing: PMC7127314.xml.json\n",
            "280000. Processing: PMC7655906.xml.json\n",
            "290000. Processing: PMC8805138.xml.json\n",
            "300000. Processing: PMC8175632.xml.json\n",
            "310000. Processing: PMC8672020.xml.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=200 /content/corpus-section-names.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xEeB_bip1sj",
        "outputId": "7441e4c0-2ef1-4861-cd41-eb3d8e538784"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "introduction\t122535\n",
            "discussion\t83532\n",
            "conclusion\t54913\n",
            "conclusions\t34511\n",
            "results\t32179\n",
            " introduction\t29710\n",
            "author contributions\t25240\n",
            "conflict of interest\t24608\n",
            " conclusions\t23331\n",
            "declaration of competing interest\t19576\n",
            " discussion\t16295\n",
            "statistical analysis ::: methods\t15517\n",
            "funding\t14610\n",
            "background\t13515\n",
            "data availability statement\t12312\n",
            "methods\t10806\n",
            "ethics statement\t9610\n",
            "limitations ::: discussion\t9521\n",
            "statistical analysis ::: materials and methods\t9294\n",
            "data analysis ::: methods\t5695\n",
            "credit authorship contribution statement\t5610\n",
            " statistical analysis ::: materials and methods\t5298\n",
            "data collection ::: methods\t5226\n",
            " results\t5116\n",
            "study design ::: methods\t5017\n",
            "materials and methods\t4824\n",
            "publisher's note\t4669\n",
            "conflicts of interest\t4125\n",
            "participants ::: methods\t4067\n",
            "publisher’s note\t4065\n",
            "limitations\t3989\n",
            "strengths and limitations ::: discussion\t3825\n",
            "case presentation\t3511\n",
            "an event is serious (based on the ich definition) when the patient outcome is:\t2874\n",
            "ethical approval\t2810\n",
            "conclusions ::: discussion\t2608\n",
            "statistical analyses ::: methods\t2472\n",
            "uncited references\t2470\n",
            "case report\t2349\n",
            "data analysis ::: materials and methods\t2059\n",
            "conflict of interests\t2048\n",
            "principal findings ::: discussion\t1994\n",
            " materials and methods\t1968\n",
            "outcomes ::: methods\t1926\n",
            "results and discussion\t1923\n",
            "study population ::: methods\t1917\n",
            "declaration of interests\t1875\n",
            "study design and participants ::: methods\t1841\n",
            " conclusion\t1833\n",
            " data analysis ::: materials and methods\t1807\n",
            "participants ::: materials and methods\t1776\n",
            "author contribution\t1712\n",
            "background ::: introduction\t1702\n",
            "patient and public involvement ::: methods\t1670\n",
            "disclosures\t1667\n",
            "concluding remarks\t1632\n",
            "study design ::: materials and methods\t1624\n",
            "conclusion ::: discussion\t1624\n",
            "data collection ::: materials and methods\t1617\n",
            "analysis ::: methods\t1602\n",
            " participants ::: materials and methods\t1496\n",
            "measures ::: methods\t1458\n",
            "methodology\t1456\n",
            "ethical considerations ::: methods\t1453\n",
            "participants ::: method\t1419\n",
            "uncited reference\t1415\n",
            "funding sources\t1401\n",
            "statistical analyses ::: materials and methods\t1381\n",
            "summary\t1361\n",
            "search strategy ::: methods\t1343\n",
            " study design ::: materials and methods\t1321\n",
            "literature review\t1314\n",
            "conflict of interest statement\t1288\n",
            "statistical analysis ::: material and methods\t1266\n",
            "data extraction ::: methods\t1249\n",
            "conflicts of interest ::: conclusion\t1242\n",
            "ethics ::: methods\t1239\n",
            "role of the funding source ::: methods\t1238\n",
            "financial support and sponsorship ::: conclusion\t1233\n",
            "reporting summary ::: methods\t1224\n",
            "data availability\t1219\n",
            "design ::: methods\t1212\n",
            "competing interests\t1180\n",
            "procedures ::: methods\t1150\n",
            "setting ::: methods\t1126\n",
            "discussion and conclusion\t1114\n",
            "authors’ contributions\t1105\n",
            "patient characteristics ::: results\t1093\n",
            "procedure ::: methods\t1093\n",
            " data collection ::: materials and methods\t1092\n",
            "data sources ::: methods\t1020\n",
            "study limitations ::: discussion\t1019\n",
            "statistics ::: methods\t921\n",
            "discussion and conclusions\t901\n",
            "disclosure\t900\n",
            "data analysis ::: method\t896\n",
            "participants ::: results\t891\n",
            "eligibility criteria ::: methods\t886\n",
            "acknowledgments\t882\n",
            " limitations ::: discussion\t877\n",
            "sources of funding\t852\n",
            "ethics statement ::: methods\t847\n",
            "contributors\t837\n",
            "statistical methods ::: methods\t833\n",
            " statistical analysis ::: methods\t828\n",
            " statistical analyses ::: materials and methods\t822\n",
            "consent\t804\n",
            "data sharing\t802\n",
            "patients ::: methods\t800\n",
            "implications ::: discussion\t792\n",
            "study selection ::: methods\t791\n",
            "sample characteristics ::: results\t790\n",
            "descriptive statistics ::: results\t789\n",
            "credit author statement\t780\n",
            "procedure ::: method\t779\n",
            "objectives ::: introduction\t770\n",
            "method\t766\n",
            "ethical approval ::: methods\t764\n",
            "participant characteristics ::: results\t752\n",
            "study design and setting ::: methods\t751\n",
            "inclusion and exclusion criteria ::: methods\t748\n",
            "sample size ::: methods\t742\n",
            "baseline characteristics ::: results\t736\n",
            "author statement\t729\n",
            "demographics ::: results\t729\n",
            "case \t727\n",
            "guarantor\t726\n",
            "study population ::: materials and methods\t726\n",
            "ethics statement ::: materials and methods\t726\n",
            "data ::: methods\t725\n",
            "material and methods\t722\n",
            "study population ::: results\t718\n",
            "study characteristics ::: results\t708\n",
            "ethics approval ::: methods\t695\n",
            "uncited references:\t694\n",
            "study setting ::: methods\t689\n",
            "statistical analysis ::: patients and methods\t683\n",
            "financial support and sponsorship\t682\n",
            "author contribution statement ::: declarations\t681\n",
            "funding statement ::: declarations\t679\n",
            "related work\t678\n",
            "limitations of the study ::: discussion\t678\n",
            "additional information ::: declarations\t677\n",
            "provenance and peer review\t673\n",
            " limitations\t672\n",
            "procedure ::: materials and methods\t670\n",
            "fazit für die praxis\t655\n",
            "funding source\t650\n",
            "consent for publication\t649\n",
            "statistics ::: materials and methods\t640\n",
            " methods\t640\n",
            "ethical statement\t636\n",
            "key points\t619\n",
            "data availability statement ::: declarations\t597\n",
            "recruitment ::: methods\t595\n",
            "study design and participants ::: materials and methods\t587\n",
            "peer review ::: peer review\t580\n",
            "data source ::: methods\t579\n",
            "authors' contributions\t575\n",
            "declaration of interests statement ::: declarations\t566\n",
            "limitations and future directions ::: discussion\t559\n",
            "statistical analysis\t554\n",
            "sample ::: methods\t551\n",
            " procedure ::: materials and methods\t549\n",
            "ethics approval\t547\n",
            "ethical considerations ::: materials and methods\t546\n",
            "patients ::: materials and methods\t543\n",
            "outcome measures ::: methods\t542\n",
            "statement of ethics\t542\n",
            "treatment\t542\n",
            "statistical analysis ::: method\t540\n",
            "ethics approval and consent to participate\t534\n",
            "materials ::: materials and methods\t534\n",
            "recommendations\t532\n",
            "intervention ::: methods\t528\n",
            "discussions\t527\n",
            "study design and population ::: methods\t522\n",
            " materials ::: materials and methods\t522\n",
            " ethical considerations ::: materials and methods\t518\n",
            "strengths and limitations\t511\n",
            "acknowledgements\t508\n",
            "sensitivity analysis ::: results\t495\n",
            "study participants ::: methods\t493\n",
            " strengths and limitations ::: discussion\t491\n",
            "practical implications ::: discussion\t489\n",
            "outcome and follow-up\t485\n",
            "availability of data and materials\t485\n",
            "inclusion criteria ::: methods\t485\n",
            "overview ::: methods\t484\n",
            " measures ::: materials and methods\t470\n",
            "covariates ::: methods\t470\n",
            " study population ::: materials and methods\t467\n",
            "data analysis ::: methodology\t465\n",
            "measures ::: materials and methods\t460\n",
            "definitions ::: methods\t450\n",
            " background\t448\n",
            "main findings ::: discussion\t445\n",
            "introduction \t443\n",
            "quality assessment ::: methods\t440\n",
            "informed consent\t440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmZdmvgVFF1v"
      },
      "source": [
        "To see the number of words, paragraphs in your corpus you can use this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p8xxDTvAkv_"
      },
      "source": [
        "# !wc covid19corpus.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZN2v7snE7iQ"
      },
      "source": [
        "If you have split the text into parts, you can see the number of words in each part using this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIr8NG15EVro"
      },
      "source": [
        "!wc part*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/document_parses/pmc_json/PMC9034168.xml.json PMC9034168.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7128104.xml.json PMC7128104.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8769777.xml.json PMC8769777.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7926205.xml.json PMC7926205.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8799642.xml.json PMC8799642.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7124374.xml.json PMC7124374.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8812323.xml.json PMC8812323.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7446676.xml.json PMC7446676.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7436596.xml.json PMC7436596.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8808276.xml.json PMC8808276.xml.json"
      ],
      "metadata": {
        "id": "L3fUdPdlJZrL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}