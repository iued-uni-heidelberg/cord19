{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQBjJbzTkYIOhIGrOglXsa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/cord19/blob/main/Cord19_v02_download2text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apx9q6HAbXkM"
      },
      "source": [
        "# Downloading and reading CORD19 corpus\n",
        "This notebook downloads and reads the free cord19 corpus into one file. The notebook is hosted at IÜD, Heidelberg University github repository https://github.com/iued-uni-heidelberg/cord19\n",
        "\n",
        "CORD19 (covid-19) open-source corpus is available from https://www.semanticscholar.org/cord19/download. \n",
        "\n",
        "Documentation is available at https://github.com/allenai/cord19\n",
        "\n",
        "The original files are in json format. The output file is in plain text format; documents are separated (by default) by \\<doc id=\"doc1000001\"> ... \\</doc> tags\n",
        "\n",
        "The purpose of the plain text file is for further processing, e.g., generating linguistic annotation using the TreeTagger or the Standford parser for part-of-speech annotation or dependency / constituency parsing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWGPbsx3fjpA"
      },
      "source": [
        "## Downloading CORD19 corpus\n",
        "\n",
        "The corpus is downloaded and extracted from https://www.semanticscholar.org/cord19/download\n",
        "\n",
        "Please check the link above: if you need the latest release of the corpus or if you would like to choose another release. Currently the 2022-06-02 release is downloaded.\n",
        "\n",
        "File size is ~11GB (v2021-08-30)\n",
        "File size is ~18GB (v2022-06-02)\n",
        "expected download time ~9 min\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ8alxYIfvhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4010e537-6970-4f2c-8aee-22a272b82005"
      },
      "source": [
        "# !wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2021-08-30.tar.gz\n",
        "!wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2022-06-02.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-05 08:35:20--  https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2022-06-02.tar.gz\n",
            "Resolving ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)... 52.92.165.58\n",
            "Connecting to ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)|52.92.165.58|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18657952487 (17G) [binary/octet-stream]\n",
            "Saving to: ‘cord-19_2022-06-02.tar.gz’\n",
            "\n",
            "cord-19_2022-06-02. 100%[===================>]  17.38G  40.5MB/s    in 7m 20s  \n",
            "\n",
            "2022-10-05 08:42:40 (40.5 MB/s) - ‘cord-19_2022-06-02.tar.gz’ saved [18657952487/18657952487]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FajAVEv_zgXY"
      },
      "source": [
        "Extracting cord-19 corpus, approximate time ~ 4 min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X69zODWxxYSB",
        "outputId": "aa7493d6-b3c1-4c92-f5f2-49992e2bef02"
      },
      "source": [
        "# !tar -xvzf cord-19_2021-08-30.tar.gz\n",
        "!tar -xvzf cord-19_2022-06-02.tar.gz 2022-06-02/document_parses.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-02/document_parses.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8g4-kQyzuZN"
      },
      "source": [
        "Removing initial archive to free some disk space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOp7a1OJzHzY"
      },
      "source": [
        "# !rm cord-19_2021-08-30.tar.gz\n",
        "!rm cord-19_2022-06-02.tar.gz\n",
        "!mv 2022-06-02/document_parses.tar.gz ./document_parses.tar.gz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrJAdw7D4rOh"
      },
      "source": [
        "Removing more files to save space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27p3sZa04K4X"
      },
      "source": [
        "# removing more files to save space\n",
        "# !rm --recursive 2021-08-30\n",
        "# !rm --recursive document_parses/pdf_json\n",
        "!rm --recursive 2022-06-02"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rT3UnT9zzev"
      },
      "source": [
        "Extracting document parsers, which contain individual articles in separate json files. This is expected to take ~ 9+ min."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !tar -tvf document_parses.tar.gz >document_parses.txt"
      ],
      "metadata": {
        "id": "B_HdqkFt1_-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxHIVjtrzRCT"
      },
      "source": [
        "# !tar -xvzf 2021-08-30/document_parses.tar.gz\n",
        "!tar -xvzf document_parses.tar.gz document_parses/pmc_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output:\n",
        "\n",
        "PMC1054884.xml.json\n",
        "\n",
        "PMC1065028.xml.json\n",
        "\n",
        "PMC1065064.xml.json\n",
        "\n",
        "PMC1065120.xml.json\n",
        "\n",
        "PMC1065257.xml.json\n",
        "\n",
        "PMC1072802.xml.json\n",
        "\n",
        "PMC1072806.xml.json\n",
        "\n",
        "PMC1072807.xml.json\n",
        "\n",
        "PMC1074505.xml.json\n",
        "\n",
        "PMC1074749.xml.json\n",
        "...\n",
        "\n",
        "~ 28G of json files\n"
      ],
      "metadata": {
        "id": "e_byMXi95YBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ls /content/document_parses/pmc_json >pms_json.txt"
      ],
      "metadata": {
        "id": "ZNK5Eoct5dTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm --recursive document_parses/pmc_json\n",
        "# !rm --recursive document_parses/pdf_json\n",
        "!rm document_parses.tar.gz"
      ],
      "metadata": {
        "id": "JsuPYd05utfQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh /content/document_parses/pmc_json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFKmfiIA9Q7a",
        "outputId": "7ec13849-2abf-4eea-cfeb-bf47c3b80d88"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28G\t/content/document_parses/pmc_json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/document_parses/pmc_json_sample"
      ],
      "metadata": {
        "id": "3dstcJcr-MJ8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecvdez_1e_oD"
      },
      "source": [
        "## Reading json directory and merging into text file(s)\n",
        "\n",
        "Run this cell to create the class; then run the next cell to execute on the directory \"document_parses/pmc_json\"\n",
        "\n",
        "This is a class for reading a directory with json files and writing them to a single file or split into several text file, with \"split_by_docs=N\", N documents in each file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW5WsBI-dNIN"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Python script to open each file, read json input and copy to one text file for subsequent processing\n",
        "import os, re, sys\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "class clJsonDir2txt(object):\n",
        "    '''\n",
        "    @author Bogdan Babych, IÜD, Heidelberg University\n",
        "    @email bogdan [dot] babych [at] iued [dot] uni-heidelberg [dot] de\n",
        "    a script for processing covid-19 corpus:\n",
        "    @url https://www.semanticscholar.org/cord19 @url https://www.semanticscholar.org/cord19/download\n",
        "        recursively reads files from a directory, and glues them together into a single corpus file\n",
        "\n",
        "    @todo:\n",
        "        working with sections - collect titles of all sections; frequent sections; select argumentative sections (e.g., discussion, analysis...)\n",
        "        - to compare descriptive and argumentative parts of the corpus\n",
        "\n",
        "        experimenting with different annotations (pos, parsing... ); MT quality evaluation...\n",
        "    '''\n",
        "    def __init__(self, SDirName, output_file = 'corpus_out.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = True, include_authors = True, tag='doc', id=1000000, split_by_docs = 0, copy_docs = 0): # initialising by openning the directories\n",
        "        self.SOutput_file = output_file\n",
        "        self.STextFilter = textfilter\n",
        "        self.RFilter = re.compile(textfilter, re.IGNORECASE | re.MULTILINE)\n",
        "        self.BInclTitle = include_title # implemented\n",
        "        self.BInclSectionNames = include_sectionNames # implemented\n",
        "        self.BInclRefs = include_refs # not implemented yet\n",
        "        self.BInclAuth = include_authors # not implemented yet\n",
        "        self.STag = tag\n",
        "        self.ID = id\n",
        "        self.ISplitByDocs = int(split_by_docs)\n",
        "        self.ICopyDocs = int(copy_docs)\n",
        "        # global dictionary of section names (to check and make rules...)\n",
        "        self.DSectNames = defaultdict(int)\n",
        "        # print(self.ISplitByDocs)\n",
        "        self.openDir(SDirName)\n",
        "        self.printDictionary(self.DSectNames, 'corpus-section-names.txt')\n",
        "        return\n",
        "\n",
        "\n",
        "    def openDir(self, path): # implementation of recursively openning directories from a given rule directory and reading each file recursively into a string\n",
        "        i = 0\n",
        "        path_sample = path + '_sample'\n",
        "        if self.ISplitByDocs:\n",
        "            SPartFile = \"part1000000\" + self.SOutput_file\n",
        "            FOut = open(SPartFile, 'w')\n",
        "        else:\n",
        "            FOut = open(self.SOutput_file, 'w')\n",
        "\n",
        "        for root,d_names,f_names in os.walk(path):\n",
        "            for f in f_names:\n",
        "                i+=1\n",
        "                if i%10000==0: print(str(i) + '. Processing: ' + f)\n",
        "                fullpath = os.path.join(root, f)\n",
        "                # print(fullpath)\n",
        "                try:\n",
        "                    FIn = open(fullpath,'r')\n",
        "                    SIn = FIn.read()\n",
        "                    # apply text filter, if not None\n",
        "                    if self.STextFilter and (re.search(self.RFilter, SIn) == None): continue\n",
        "                    SText2Write = self.procFile(SIn,f,i)\n",
        "                    if SText2Write: FOut.write(SText2Write) # if the string is not empty then write to file\n",
        "                    FIn.close()\n",
        "                except:\n",
        "                    print(f'file {f} cannot be read or processed')\n",
        "                finally:\n",
        "                    # splitting output into chunks of \"split_by_docs\" size\n",
        "                    if self.ISplitByDocs and (i % self.ISplitByDocs == 0): # if self.ISplitByDocs == 0 then everything goes into one file; if this > 0 then\n",
        "                        SPartFile = \"part\" + str(1000000 + i) + self.SOutput_file # generate new file name\n",
        "                        FOut.flush()\n",
        "                        FOut.close()\n",
        "                        FOut = open(SPartFile, 'w')\n",
        "                    if self.ICopyDocs and (i >= self.ICopyDocs) and (i < (self.ICopyDocs + self.ISplitByDocs)):\n",
        "                        try:\n",
        "                            SOutputDirN = root + '_sample'\n",
        "                            SOutputFN = os.path.join(SOutputDirN, f)\n",
        "                            os.system(f'cp {fullpath} {SOutputFN}')\n",
        "                        except:\n",
        "                            print('.')\n",
        "\n",
        "        FOut.flush()\n",
        "        FOut.close()\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def procFile(self, SIn,SFNameIn,i): # sending each json string for extraction of text and attaching an correct tags to each output string output string\n",
        "        STagOpen = '<' + self.STag + ' id=\"' + self.STag + str(self.ID + i)  + '\">\\n'\n",
        "        STagClose = '\\n</' + self.STag + '>\\n\\n'\n",
        "        SText4Corpus = self.getJson(SIn, SFNameIn)\n",
        "        if SText4Corpus:\n",
        "            return STagOpen + SText4Corpus + STagClose\n",
        "        else:\n",
        "            print('\\tNo data read from: ' + SFNameIn)\n",
        "            return None\n",
        "\n",
        "\n",
        "    def getJson(self, SIn, SFNameIn): # for each file-level string read from a file: managing internal structure of the covid-19 json file\n",
        "        LOut = [] # collecting a list of strings\n",
        "        try:\n",
        "            DDoc = json.loads(SIn)\n",
        "        except:\n",
        "            print('\\t\\t' + SFNameIn + ' => error reading json2dictionary')\n",
        "            return None\n",
        "        # metadata:\n",
        "        try:\n",
        "            DMetaData = DDoc['metadata']\n",
        "            if DMetaData:\n",
        "                SMetaData = self.getJson_Metadata(DMetaData)\n",
        "                if SMetaData: LOut.append(SMetaData)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no metadata')\n",
        "            DMetaData = None\n",
        "        # body text\n",
        "        try:\n",
        "            LBodyText = DDoc['body_text']\n",
        "            if LBodyText:\n",
        "                SBodyText = self.getJson_BodyText(LBodyText)\n",
        "                LOut.append(SBodyText)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no body_text')\n",
        "            LBodyText = None\n",
        "        # further: to implement references\n",
        "\n",
        "        SText = '\\n\\n'.join(LOut)\n",
        "        return SText\n",
        "\n",
        "\n",
        "    def getJson_Metadata(self, DIn): # converts interesting parts of metadata into a string\n",
        "        SMetadata = ''\n",
        "        LMetadata = []\n",
        "        try: STitle = DIn[\"title\"]\n",
        "        except: STitle = None\n",
        "        if STitle and self.BInclTitle:\n",
        "            LMetadata.append(STitle)\n",
        "\n",
        "        # to implement reading of authors' names\n",
        "\n",
        "        if LMetadata: SMetadata = '\\n\\n'.join(LMetadata)\n",
        "        return SMetadata\n",
        "\n",
        "\n",
        "    def getJson_BodyText(self, LIn): # converts interesting parts of the body texts into a string\n",
        "        SBodyText = ''\n",
        "        LBodyText = []\n",
        "        SSectionName0 = '' # current section name set to empty for a new text\n",
        "        # todo: later, in post-processing stage for the whole corpus, maybe after lemmatization...\n",
        "        # ISampleNumber = 0 # samples of 100 words in text; they do not cross section boundaries \n",
        "\n",
        "        for DParagraph in LIn:\n",
        "            # sections added 2022-09-28\n",
        "            try:\n",
        "                # DParagraphs[\"section\"] ## distinction between different sections....\n",
        "                SSectionName = DParagraph[\"section\"]\n",
        "                # normalizing new section name (1)\n",
        "                SSectionName = SSectionName.replace(\"\\n\", \" \")\n",
        "\n",
        "                if self.BInclSectionNames and SSectionName: # if we opted to include section names and section name is not empty\n",
        "                    if SSectionName != SSectionName0: # if we found a new section name\n",
        "                        # processing section name\n",
        "                        SSectionName0 = SSectionName # change the current section name\n",
        "\n",
        "                        # normalizing section name (2)\n",
        "                        SSectionNameNorm = SSectionName.lower()\n",
        "                        SSectionNameNorm = re.sub('[0-9\\.]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = re.sub('[ ]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = SSectionNameNorm.strip()\n",
        "                        \n",
        "                        self.DSectNames[SSectionNameNorm] += 1\n",
        "                        SSect4text = f'<section sName=\"{SSectionNameNorm}\">\\n{SSectionName}\\n</section>'\n",
        "\n",
        "                        LBodyText.append(SSect4text)\n",
        "            except:\n",
        "                print('S!',)\n",
        "                continue\n",
        "            # first original section (we extract text after extracting section name)\n",
        "            try:\n",
        "                ## DParagraphs[section] ## -- later on >> distinction between different sections....\n",
        "                SParagraph = DParagraph[\"text\"]\n",
        "                LBodyText.append(SParagraph)\n",
        "            except:\n",
        "                print('!',)\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "        SBodyText = '\\n\\n'.join(LBodyText)\n",
        "        return SBodyText\n",
        "\n",
        "    def printDictionary(self, DFreq, SFOutDict):\n",
        "        FOutDict = open(SFOutDict, 'w')\n",
        "        for key, val in sorted(DFreq.items(), key=lambda x: x[1], reverse=True):\n",
        "            FOutDict.write(f'{key}\\t{str(val)}\\n')\n",
        "        FOutDict.flush()\n",
        "        FOutDict.close()\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# arguments:\n",
        "'''\n",
        "        sys.argv[1], # obligatory: input directory name;\n",
        "            other arguments optional:\n",
        "            output_file = 'covid19corpus.txt',\n",
        "            textfilter = None, # if this is string, only texts containing it are collected, e.g., covid\n",
        "            include_title = True, # include or exclude title\n",
        "            include_refs = False, # not implemented yet: include or exclude references\n",
        "            split_by_docs=0 # split by groups of n documents; if 0 then write to one file\n",
        "\n",
        "'''\n",
        "\n",
        "'''if __name__ == '__main__':\n",
        "    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=240000)\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2zYiDkn5Vbo"
      },
      "source": [
        "## numbers from previous version\n",
        "This cell will executre reading of json files into a single (or multiple) files\n",
        "\n",
        "Change the value of \"split_by_docs=0\" to \"split_by_docs=10000\" or any number ; this will create several corpus files with 10000 or any required number fo documents per file, which you wish to have.\n",
        "\n",
        "\n",
        "Approximate execution time ~10 min\n",
        "\n",
        "\n",
        "File size to download ~4.3 GB\n",
        "\n",
        "It contains ~198.000 documents,\n",
        "\n",
        "~ 671.578.587 words\n",
        "\n",
        "~ 19.381.647 paragraphs (including empty lines, i.e., ~10M real paragraphs)\n",
        "\n",
        "~ 4.619.100.883 characters\n",
        "\n",
        "## numbers for the last version\n",
        "Approximate execution time ~20 min\n",
        "\n",
        "8 BNC-size (100mw) files are generted, containg together\n",
        "\n",
        "~ 315.000 documents\n",
        "\n",
        "  827.118.629 words\n",
        "\n",
        "   22.094.653 paragraphs\n",
        "\n",
        "5.650.921.315 characters\n",
        "\n",
        "\n",
        "Download time can take up to 1 hour depending on your connection speed.\n",
        "\n",
        "To split into ~BNC size chunks (100MW), split into groups of ~40000 documents (in the following cell set \"split_by_docs=20000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2EUP7kS5fkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2e1cba-9e5d-4f66-cd0f-315886ac58a6"
      },
      "source": [
        "# remove parameter textfilter='covid', to return all documents\n",
        "# change parameter split_by_docs=40000 to split_by_docs=0 to return a single file instead of ~5 parts with <40000 in each\n",
        "OJsonDir2txt = clJsonDir2txt(\"document_parses/pmc_json\", output_file = 'cord19.txt', textfilter='covid', include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=40000, copy_docs=240000)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000. Processing: PMC7050215.xml.json\n",
            "20000. Processing: PMC8831013.xml.json\n",
            "30000. Processing: PMC9088537.xml.json\n",
            "40000. Processing: PMC2779497.xml.json\n",
            "50000. Processing: PMC8666100.xml.json\n",
            "60000. Processing: PMC4318920.xml.json\n",
            "70000. Processing: PMC8871413.xml.json\n",
            "80000. Processing: PMC8798676.xml.json\n",
            "90000. Processing: PMC8931271.xml.json\n",
            "100000. Processing: PMC8688729.xml.json\n",
            "110000. Processing: PMC7407558.xml.json\n",
            "120000. Processing: PMC8686689.xml.json\n",
            "130000. Processing: PMC8557104.xml.json\n",
            "140000. Processing: PMC8669852.xml.json\n",
            "150000. Processing: PMC6100019.xml.json\n",
            "160000. Processing: PMC8117966.xml.json\n",
            "170000. Processing: PMC8638798.xml.json\n",
            "180000. Processing: PMC8233624.xml.json\n",
            "190000. Processing: PMC7675378.xml.json\n",
            "200000. Processing: PMC8196470.xml.json\n",
            "210000. Processing: PMC5733625.xml.json\n",
            "220000. Processing: PMC8609235.xml.json\n",
            "230000. Processing: PMC7316319.xml.json\n",
            "240000. Processing: PMC8491259.xml.json\n",
            "250000. Processing: PMC9146581.xml.json\n",
            "260000. Processing: PMC7834811.xml.json\n",
            "270000. Processing: PMC7092836.xml.json\n",
            "280000. Processing: PMC7307051.xml.json\n",
            "290000. Processing: PMC5610321.xml.json\n",
            "300000. Processing: PMC8224259.xml.json\n",
            "310000. Processing: PMC8360807.xml.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "writing frequent section names (ordered by descending frequency, from highest to 1)"
      ],
      "metadata": {
        "id": "eTbi965g2zqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls document_parses/pmc_json_sample | wc -l\n",
        "!du -sh document_parses/pmc_json_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkPWJaJLOG_e",
        "outputId": "eb830fb2-2f6a-4e21-8570-a5836bbc26ed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40000\n",
            "3.6G\tdocument_parses/pmc_json_sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvzf document_parses_sample.tar.gz document_parses/pmc_json_sample"
      ],
      "metadata": {
        "id": "ZCW92xAFPQtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=75 /content/corpus-section-names.txt"
      ],
      "metadata": {
        "id": "kJeGtnHK5BJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=1000 /content/corpus-section-names.txt > corpus-selection-names-top1000.txt"
      ],
      "metadata": {
        "id": "6xEeB_bip1sj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmZdmvgVFF1v"
      },
      "source": [
        "To see the number of words, paragraphs in your corpus you can use this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p8xxDTvAkv_"
      },
      "source": [
        "# !wc covid19corpus.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZN2v7snE7iQ"
      },
      "source": [
        "If you have split the text into parts, you can see the number of words in each part using this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIr8NG15EVro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb9dcf5-4a10-4836-dad6-3d9e90568209"
      },
      "source": [
        "!wc part*"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   4085687  108914903  752390539 part1000000cord19.txt\n",
            "   4083605  109685695  756986287 part1040000cord19.txt\n",
            "   4117431  110113881  760911812 part1080000cord19.txt\n",
            "   4085114  109782614  757565426 part1120000cord19.txt\n",
            "   4156737  110454560  764015682 part1160000cord19.txt\n",
            "   4051419  109593018  756536541 part1200000cord19.txt\n",
            "   4110526  110115301  761073243 part1240000cord19.txt\n",
            "   3639938   98300665  678104874 part1280000cord19.txt\n",
            "  32330457  866960637 5987584404 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=500000 part1240000cord19.txt >part1240000cord19-500k.txt"
      ],
      "metadata": {
        "id": "0ELJw_EvK133"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip part1000000cord19.txt\n",
        "!gzip part1040000cord19.txt\n",
        "!gzip part1080000cord19.txt\n",
        "!gzip part1120000cord19.txt\n",
        "!gzip part1160000cord19.txt\n",
        "!gzip part1200000cord19.txt\n",
        "!gzip part1240000cord19.txt\n",
        "!gzip part1280000cord19.txt"
      ],
      "metadata": {
        "id": "TB_sGebiyb08"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "selecting some examples to experiment with..."
      ],
      "metadata": {
        "id": "I8j1MY6I2uSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/document_parses/pmc_json/PMC9034168.xml.json PMC9034168.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7128104.xml.json PMC7128104.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8769777.xml.json PMC8769777.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7926205.xml.json PMC7926205.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8799642.xml.json PMC8799642.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7124374.xml.json PMC7124374.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8812323.xml.json PMC8812323.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7446676.xml.json PMC7446676.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7436596.xml.json PMC7436596.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8808276.xml.json PMC8808276.xml.json"
      ],
      "metadata": {
        "id": "L3fUdPdlJZrL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc /content/document_parses/pmc_json/PMC9034168.xml.json PMC9034168.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7128104.xml.json PMC7128104.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC8769777.xml.json PMC8769777.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7926205.xml.json PMC7926205.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC8799642.xml.json PMC8799642.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7124374.xml.json PMC7124374.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC8812323.xml.json PMC8812323.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7446676.xml.json PMC7446676.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7436596.xml.json PMC7436596.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC8808276.xml.json PMC8808276.xml.json"
      ],
      "metadata": {
        "id": "JJMZorcJ5gKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## working with 100MW sample (40k texts)\n",
        "### selecting 100-word long samples, writing to JSon dictionary, mapping names, recording"
      ],
      "metadata": {
        "id": "TPVG10xsWQ0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output directory\n",
        "!mkdir document_parses/pmc_json_sample02/"
      ],
      "metadata": {
        "id": "6NGEIHrYrysv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modifying script to select 100-word long samples\n",
        "'''\n",
        "algorithm:\n",
        "    1. form sections as strings\n",
        "    2. process sections (map names), create a record\n",
        "    3. write samples in a python dictionary\n",
        "\n",
        "    ? do we need xml output to a file ?\n",
        "\n",
        "    architecture:\n",
        "      - create list of sections from list of paragraphs\n",
        "      - process each section, splitting it into samples of a pre-defined size\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "O4CjgIzOWP_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c8165460-a7b5-4b9e-bc35-57aa9476866c",
        "id": "5JlmZ1U1Wx4v"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Python script to open each file, read json input and copy to one text file for subsequent processing\n",
        "import os, re, sys\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "class clJsonDir2txtSamples(object):\n",
        "    '''\n",
        "    @author Bogdan Babych, IÜD, Heidelberg University\n",
        "    @email bogdan [dot] babych [at] iued [dot] uni-heidelberg [dot] de\n",
        "    a script for processing covid-19 corpus:\n",
        "    @url https://www.semanticscholar.org/cord19 @url https://www.semanticscholar.org/cord19/download\n",
        "        recursively reads files from a directory, and glues them together into a single corpus file\n",
        "\n",
        "    @todo:\n",
        "        working with sections - collect titles of all sections; frequent sections; select argumentative sections (e.g., discussion, analysis...)\n",
        "        - to compare descriptive and argumentative parts of the corpus\n",
        "\n",
        "        experimenting with different annotations (pos, parsing... ); MT quality evaluation...\n",
        "    '''\n",
        "    def __init__(self, SDirName, output_file = 'corpus_out.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = True, include_authors = True, tag='doc', id=1000000, split_by_docs = 0, copy_docs = 0, sample_size = 0, outjsondir = '02'): # initialising by openning the directories\n",
        "        self.SOutput_file = output_file\n",
        "        self.SOutput_file_stat = 'stat_' + output_file\n",
        "        self.STextFilter = textfilter\n",
        "        self.RFilter = re.compile(textfilter, re.IGNORECASE | re.MULTILINE)\n",
        "        self.BInclTitle = include_title # implemented\n",
        "        self.BInclSectionNames = include_sectionNames # implemented\n",
        "        self.BInclRefs = include_refs # not implemented yet\n",
        "        self.BInclAuth = include_authors # not implemented yet\n",
        "        self.STag = tag\n",
        "        self.ID = id\n",
        "        self.ISplitByDocs = int(split_by_docs)\n",
        "        self.ICopyDocs = int(copy_docs)\n",
        "        self.ISampleSize = int(sample_size)\n",
        "        self.SDirJsonOutput = outjsondir\n",
        "\n",
        "        # global dictionary of section names (to check and make rules...)\n",
        "        self.DSectNames = defaultdict(int)\n",
        "        # print(self.ISplitByDocs)\n",
        "        self.openDir(SDirName)\n",
        "        self.printDictionary(self.DSectNames, 'corpus-section-names.txt')\n",
        "        return\n",
        "\n",
        "\n",
        "    def openDir(self, path): # implementation of recursively openning directories from a given rule directory and reading each file recursively into a string\n",
        "        i = 0\n",
        "        path_sample = path + '_sample'\n",
        "        path_sample_out = path + '_sample' + self.SDirJsonOutput\n",
        "        FOutStat = open(self.SOutput_file_stat, 'w')\n",
        "        if self.ISplitByDocs:\n",
        "            SPartFile = \"part1000000\" + self.SOutput_file\n",
        "            FOut = open(SPartFile, 'w')\n",
        "        else:\n",
        "            FOut = open(self.SOutput_file, 'w')\n",
        "\n",
        "        for root,d_names,f_names in os.walk(path):\n",
        "            for f in f_names:\n",
        "                i+=1\n",
        "                if i%10000==0: print(str(i) + '. Processing: ' + f)\n",
        "                fullpath = os.path.join(root, f)\n",
        "                ## output of Json files\n",
        "                fullpathout = os.path.join(path_sample_out, f) \n",
        "                # print(fullpath)\n",
        "                print('full path output: ' + fullpathout)\n",
        "                try:\n",
        "                    ## implement : processing output file; statistics output file...\n",
        "                    FIn = open(fullpath,'r')\n",
        "                    SIn = FIn.read()\n",
        "                    # apply text filter, if not None\n",
        "                    if self.STextFilter and (re.search(self.RFilter, SIn) == None): continue\n",
        "                    DData, SText2Write = self.procFile(SIn,f,i)\n",
        "                    if SText2Write: FOut.write(SText2Write) # if the string is not empty then write to file\n",
        "                    FIn.close()\n",
        "\n",
        "                    ## writing json files with new dictionary structure\n",
        "                    ## FJsonOut = open(fullpathout, 'w')\n",
        "                    with open(fullpathout, 'w', encoding='utf-8') as jf:\n",
        "                        json.dump(DData, jf, ensure_ascii=False, indent=4)\n",
        "\n",
        "                except:\n",
        "                    print(f'file {f} cannot be read or processed')\n",
        "                finally:\n",
        "                  ## closing json output file\n",
        "                    jf.close() ## close json file\n",
        "                    # splitting output into chunks of \"split_by_docs\" size\n",
        "                    if self.ISplitByDocs and (i % self.ISplitByDocs == 0): # if self.ISplitByDocs == 0 then everything goes into one file; if this > 0 then\n",
        "                        SPartFile = \"part\" + str(1000000 + i) + self.SOutput_file # generate new file name\n",
        "                        FOut.flush()\n",
        "                        FOut.close()\n",
        "                        FOut = open(SPartFile, 'w')\n",
        "                    if self.ICopyDocs and (i >= self.ICopyDocs) and (i < (self.ICopyDocs + self.ISplitByDocs)):\n",
        "                        try:\n",
        "                            SOutputDirN = root + '_sample'\n",
        "                            SOutputFN = os.path.join(SOutputDirN, f)\n",
        "                            os.system(f'cp {fullpath} {SOutputFN}')\n",
        "                        except:\n",
        "                            print('.')\n",
        "\n",
        "        FOut.flush()\n",
        "        FOut.close()\n",
        "\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def procFile(self, SIn,SFNameIn,i): # sending each json string for extraction of text and attaching an correct tags to each output string output string\n",
        "        STagOpen = '<' + self.STag + ' id=\"' + self.STag + str(self.ID + i)  + '\">\\n'\n",
        "        STagClose = '\\n</' + self.STag + '>\\n\\n'\n",
        "        DData, SText4Corpus = self.getJson(SIn, SFNameIn)\n",
        "        STagsAndText = STagOpen + SText4Corpus + STagClose\n",
        "        if SText4Corpus:\n",
        "            return DData, STagsAndText\n",
        "        else:\n",
        "            print('\\tNo data read from: ' + SFNameIn)\n",
        "            return DData, None\n",
        "\n",
        "\n",
        "    def getJson(self, SIn, SFNameIn): # for each file-level string read from a file: managing internal structure of the covid-19 json file\n",
        "        LOut = [] # collecting a list of strings\n",
        "\n",
        "        LBodyTextSections = [] ## list of sections: Json body text\n",
        "        ### DJsonOut = {} # this will be copied from the input json structure and enriched\n",
        "\n",
        "        try:\n",
        "            DDoc = json.loads(SIn)\n",
        "        except:\n",
        "            print('\\t\\t' + SFNameIn + ' => error reading json2dictionary')\n",
        "            return {}, None\n",
        "\n",
        "        # metadata:\n",
        "        try:\n",
        "            DMetaData = DDoc['metadata']\n",
        "            if DMetaData:\n",
        "                DMetaDataOut, SMetaData = self.getJson_Metadata(DMetaData)\n",
        "                if SMetaData: LOut.append(SMetaData)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no metadata')\n",
        "            DMetaData = None\n",
        "        # body text\n",
        "        try:\n",
        "            LBodyText = DDoc['body_text']\n",
        "            if LBodyText:\n",
        "                ## implementing sampling here ~ returning list of sections first (with a paired list of section names); then sampling for sample size...\n",
        "\n",
        "                LBodyTextSections, LBodyTextSectNames, SBodyText = self.getJson_BodyText(LBodyText) ## modified function, returns 3 arguments\n",
        "                LOut.append(SBodyText)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no body_text')\n",
        "            LBodyText = None\n",
        "        # further: to implement references\n",
        "\n",
        "        SText = '\\n\\n'.join(LOut)\n",
        "        ### DJsonOut[\"body_text\"] = LBodyTextSections\n",
        "        # returning an enriched data structure\n",
        "        return DDoc, SText\n",
        "\n",
        "\n",
        "    def getJson_Metadata(self, DIn): # converts interesting parts of metadata into a string\n",
        "        SMetadata = ''\n",
        "        LMetadata = []\n",
        "        DMetaDataOut = {}\n",
        "        try: STitle = DIn[\"title\"]\n",
        "        except: STitle = None\n",
        "        if STitle and self.BInclTitle:\n",
        "            LMetadata.append(STitle)\n",
        "\n",
        "        # to implement reading of authors' names\n",
        "        '''\n",
        "        try:\n",
        "            SPaperID = DIn[\"authors\"]\n",
        "            DMetaDataOut[\"authors\"] = \"SAuthors\"\n",
        "        except:\n",
        "            print('au!')\n",
        "        '''\n",
        "\n",
        "        if LMetadata: SMetadata = '\\n\\n'.join(LMetadata)\n",
        "        return DMetaDataOut, SMetadata\n",
        "\n",
        "    ## updated function ~\n",
        "    def getJson_BodyText(self, LIn): # converts interesting parts of the body texts into a string\n",
        "        ## modified ~ for json output\n",
        "        LBodyTextSections = [] ## output - list of sections\n",
        "        LBodyTextSectionPars = [] ## element of the LBodyTextSections[] list, one section-long        \n",
        "        LBodyTextSectNames = [] ##\n",
        "        LBodyTextSectNamesM = [] ##\n",
        "\n",
        "\n",
        "        SBodyText = '' # output (old)\n",
        "        LBodyText = []\n",
        "        SSectionName0 = '' # current section name set to empty for a new text\n",
        "        # todo: later, in post-processing stage for the whole corpus, maybe after lemmatization...\n",
        "        # ISampleNumber = 0 # samples of 100 words in text; they do not cross section boundaries \n",
        "\n",
        "        for DParagraph in LIn:\n",
        "            # sections added 2022-09-28\n",
        "            # updating and adding the section name\n",
        "            try:\n",
        "                # DParagraphs[\"section\"] ## distinction between different sections....\n",
        "                SSectionName = DParagraph[\"section\"]\n",
        "                # normalizing new section name (1)\n",
        "                SSectionName = SSectionName.replace(\"\\n\", \" \")\n",
        "\n",
        "                if self.BInclSectionNames and SSectionName: # if we opted to include section names and section name is not empty\n",
        "                    if SSectionName != SSectionName0: # if we found a new section name\n",
        "                        # processing section name\n",
        "                        LBodyTextSectNames.append(SSectionName0) ## record the previous section name\n",
        "                        ## to implement here: call a function for mapping the section names\n",
        "                        LBodyTextSectNamesM.append(SSectionName0)\n",
        "\n",
        "                        SSectionName0 = SSectionName # change the current section name\n",
        "                        ## recording previous section if not empty; then adding the name of the section to a list\n",
        "\n",
        "                        SBodyTextSectionPars = '\\n'.join(LBodyTextSectionPars)\n",
        "                        LBodyTextSections.append(SBodyTextSectionPars)\n",
        "                        LBodyTextSectionPars = []\n",
        "                            \n",
        "\n",
        "                        # normalizing section name (2)\n",
        "                        SSectionNameNorm = SSectionName.lower()\n",
        "                        SSectionNameNorm = re.sub('[0-9\\.]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = re.sub('[ ]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = SSectionNameNorm.strip()\n",
        "                        \n",
        "                        self.DSectNames[SSectionNameNorm] += 1\n",
        "                        SSect4text = f'<section sName=\"{SSectionNameNorm}\">\\n{SSectionName}\\n</section>'\n",
        "\n",
        "                        LBodyText.append(SSect4text)\n",
        "            except:\n",
        "                print('S!',)\n",
        "                continue\n",
        "            # first original section (we extract text after extracting section name)\n",
        "            try:\n",
        "                ## DParagraphs[section] ## -- later on >> distinction between different sections....\n",
        "                SParagraph = DParagraph[\"text\"]\n",
        "                LBodyText.append(SParagraph)\n",
        "            except:\n",
        "                print('!',)\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "        SBodyText = '\\n\\n'.join(LBodyText)\n",
        "        return LBodyTextSections, LBodyTextSectNamesM, SBodyText\n",
        "\n",
        "    def printDictionary(self, DFreq, SFOutDict):\n",
        "        FOutDict = open(SFOutDict, 'w')\n",
        "        for key, val in sorted(DFreq.items(), key=lambda x: x[1], reverse=True):\n",
        "            FOutDict.write(f'{key}\\t{str(val)}\\n')\n",
        "        FOutDict.flush()\n",
        "        FOutDict.close()\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# arguments:\n",
        "'''\n",
        "        sys.argv[1], # obligatory: input directory name;\n",
        "            other arguments optional:\n",
        "            output_file = 'covid19corpus.txt',\n",
        "            textfilter = None, # if this is string, only texts containing it are collected, e.g., covid\n",
        "            include_title = True, # include or exclude title\n",
        "            include_refs = False, # not implemented yet: include or exclude references\n",
        "            split_by_docs=0 # split by groups of n documents; if 0 then write to one file\n",
        "\n",
        "'''\n",
        "\n",
        "'''if __name__ == '__main__':\n",
        "    OJsonDir2txt = clJsonDir2txtSamples(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=240000, sample_size = 100, outjsondir = 'document_parses/pmc_json_sample02')\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"if __name__ == '__main__':\\n    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=240000)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OJsonDir2txtSamples = clJsonDir2txtSamples(\"document_parses/pmc_json_sample\", output_file = 'cord19.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=0, sample_size = 100, outjsondir = 'document_parses/pmc_json_sample02')\n"
      ],
      "metadata": {
        "id": "zDH-FGHuh92R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}