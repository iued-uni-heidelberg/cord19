{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/cord19/blob/main/Cord19_v06_download2text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apx9q6HAbXkM"
      },
      "source": [
        "# Downloading and reading CORD19 corpus\n",
        "This notebook downloads and reads the free cord19 corpus into one file. The notebook is hosted at IÜD, Heidelberg University github repository https://github.com/iued-uni-heidelberg/cord19\n",
        "\n",
        "CORD19 (covid-19) open-source corpus is available from https://www.semanticscholar.org/cord19/download. \n",
        "\n",
        "Documentation is available at https://github.com/allenai/cord19\n",
        "\n",
        "The original files are in json format. The output file is in plain text format; documents are separated (by default) by \\<doc id=\"doc1000001\"> ... \\</doc> tags\n",
        "\n",
        "The purpose of the plain text file is for further processing, e.g., generating linguistic annotation using the TreeTagger or the Standford parser for part-of-speech annotation or dependency / constituency parsing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWGPbsx3fjpA"
      },
      "source": [
        "## Downloading CORD19 corpus\n",
        "\n",
        "The corpus is downloaded and extracted from https://www.semanticscholar.org/cord19/download\n",
        "\n",
        "Please check the link above: if you need the latest release of the corpus or if you would like to choose another release. Currently the 2022-06-02 release is downloaded.\n",
        "\n",
        "File size is ~11GB (v2021-08-30)\n",
        "File size is ~18GB (v2022-06-02)\n",
        "expected download time ~9 min\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ8alxYIfvhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e069c0a5-d8f1-46a6-8ced-ea6c8c149da7"
      },
      "source": [
        "# !wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2021-08-30.tar.gz\n",
        "!wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2022-06-02.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-06 06:33:31--  https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2022-06-02.tar.gz\n",
            "Resolving ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)... 52.92.194.90\n",
            "Connecting to ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)|52.92.194.90|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18657952487 (17G) [binary/octet-stream]\n",
            "Saving to: ‘cord-19_2022-06-02.tar.gz’\n",
            "\n",
            "cord-19_2022-06-02. 100%[===================>]  17.38G  42.3MB/s    in 7m 27s  \n",
            "\n",
            "2022-10-06 06:40:59 (39.8 MB/s) - ‘cord-19_2022-06-02.tar.gz’ saved [18657952487/18657952487]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FajAVEv_zgXY"
      },
      "source": [
        "Extracting cord-19 corpus, approximate time ~ 4 min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X69zODWxxYSB",
        "outputId": "515b9ae6-e1bc-40b2-9222-14b26094af6d"
      },
      "source": [
        "# !tar -xvzf cord-19_2021-08-30.tar.gz\n",
        "!tar -xvzf cord-19_2022-06-02.tar.gz 2022-06-02/document_parses.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-02/document_parses.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8g4-kQyzuZN"
      },
      "source": [
        "Removing initial archive to free some disk space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOp7a1OJzHzY"
      },
      "source": [
        "# !rm cord-19_2021-08-30.tar.gz\n",
        "!rm cord-19_2022-06-02.tar.gz\n",
        "!mv 2022-06-02/document_parses.tar.gz ./document_parses.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrJAdw7D4rOh"
      },
      "source": [
        "Removing more files to save space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27p3sZa04K4X"
      },
      "source": [
        "# removing more files to save space\n",
        "# !rm --recursive 2021-08-30\n",
        "# !rm --recursive document_parses/pdf_json\n",
        "!rm --recursive 2022-06-02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rT3UnT9zzev"
      },
      "source": [
        "Extracting document parsers, which contain individual articles in separate json files. This is expected to take ~ 9+ min."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !tar -tvf document_parses.tar.gz >document_parses.txt"
      ],
      "metadata": {
        "id": "B_HdqkFt1_-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxHIVjtrzRCT"
      },
      "source": [
        "# !tar -xvzf 2021-08-30/document_parses.tar.gz\n",
        "!tar -xvzf document_parses.tar.gz document_parses/pmc_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output:\n",
        "\n",
        "PMC1054884.xml.json\n",
        "\n",
        "PMC1065028.xml.json\n",
        "\n",
        "PMC1065064.xml.json\n",
        "\n",
        "PMC1065120.xml.json\n",
        "\n",
        "PMC1065257.xml.json\n",
        "\n",
        "PMC1072802.xml.json\n",
        "\n",
        "PMC1072806.xml.json\n",
        "\n",
        "PMC1072807.xml.json\n",
        "\n",
        "PMC1074505.xml.json\n",
        "\n",
        "PMC1074749.xml.json\n",
        "...\n",
        "\n",
        "~ 28G of json files\n"
      ],
      "metadata": {
        "id": "e_byMXi95YBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/document_parses/pmc_json/PMC9034168.xml.json PMC9034168.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7128104.xml.json PMC7128104.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8769777.xml.json PMC8769777.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7926205.xml.json PMC7926205.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8799642.xml.json PMC8799642.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7124374.xml.json PMC7124374.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8812323.xml.json PMC8812323.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7446676.xml.json PMC7446676.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC7436596.xml.json PMC7436596.xml.json\n",
        "!cp /content/document_parses/pmc_json/PMC8808276.xml.json PMC8808276.xml.json"
      ],
      "metadata": {
        "id": "L3fUdPdlJZrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc /content/document_parses/pmc_json/PMC9034168.xml.json PMC9034168.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7128104.xml.json PMC7128104.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC8769777.xml.json PMC8769777.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7926205.xml.json PMC7926205.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC8799642.xml.json PMC8799642.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7124374.xml.json PMC7124374.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC8812323.xml.json PMC8812323.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7446676.xml.json PMC7446676.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC7436596.xml.json PMC7436596.xml.json\n",
        "!wc /content/document_parses/pmc_json/PMC8808276.xml.json PMC8808276.xml.json"
      ],
      "metadata": {
        "id": "JJMZorcJ5gKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ls /content/document_parses/pmc_json >pms_json.txt"
      ],
      "metadata": {
        "id": "ZNK5Eoct5dTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm --recursive document_parses/pmc_json\n",
        "# !rm --recursive document_parses/pdf_json\n",
        "!rm document_parses.tar.gz"
      ],
      "metadata": {
        "id": "JsuPYd05utfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh /content/document_parses/pmc_json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFKmfiIA9Q7a",
        "outputId": "87458a6b-0df7-4419-d35f-eb04d816c50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28G\t/content/document_parses/pmc_json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/document_parses/pmc_json_sample"
      ],
      "metadata": {
        "id": "3dstcJcr-MJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Alternative: working with sample 100mw"
      ],
      "metadata": {
        "id": "EvojGBd6D-1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/b420d407463d4a728feb/?dl=1\n",
        "!mv index.html?dl=1 document_parses_sample0.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsWWcMBwEEpj",
        "outputId": "b5deb0dc-d9a9-43bc-b1e4-2985cdc4aea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-06 20:09:29--  https://heibox.uni-heidelberg.de/f/b420d407463d4a728feb/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/999fa69c-301d-4614-ad48-30cc4b5b15b6/document_parses_sample.tar.gz [following]\n",
            "--2022-10-06 20:09:30--  https://heibox.uni-heidelberg.de/seafhttp/files/999fa69c-301d-4614-ad48-30cc4b5b15b6/document_parses_sample.tar.gz\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 611303079 (583M) [application/octet-stream]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 582.98M  14.9MB/s    in 39s     \n",
            "\n",
            "2022-10-06 20:10:09 (14.9 MB/s) - ‘index.html?dl=1’ saved [611303079/611303079]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvzf document_parses_sample0.tar.gz"
      ],
      "metadata": {
        "id": "4a1Mo4mHE2YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm document_parses_sample0.tar.gz"
      ],
      "metadata": {
        "id": "v7ci8O-CFSUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh /content/document_parses/pmc_json_sample/"
      ],
      "metadata": {
        "id": "VgP2hRdOFi88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecvdez_1e_oD"
      },
      "source": [
        "## Reading json directory and merging into text file(s)\n",
        "\n",
        "Run this cell to create the class; then run the next cell to execute on the directory \"document_parses/pmc_json\"\n",
        "\n",
        "This is a class for reading a directory with json files and writing them to a single file or split into several text file, with \"split_by_docs=N\", N documents in each file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW5WsBI-dNIN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "b15b0de3-88db-42c3-962c-db31ca8d2dab"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Python script to open each file, read json input and copy to one text file for subsequent processing\n",
        "import os, re, sys\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "class clJsonDir2txt(object):\n",
        "    '''\n",
        "    @author Bogdan Babych, IÜD, Heidelberg University\n",
        "    @email bogdan [dot] babych [at] iued [dot] uni-heidelberg [dot] de\n",
        "    a script for processing covid-19 corpus:\n",
        "    @url https://www.semanticscholar.org/cord19 @url https://www.semanticscholar.org/cord19/download\n",
        "        recursively reads files from a directory, and glues them together into a single corpus file\n",
        "\n",
        "    @todo:\n",
        "        working with sections - collect titles of all sections; frequent sections; select argumentative sections (e.g., discussion, analysis...)\n",
        "        - to compare descriptive and argumentative parts of the corpus\n",
        "\n",
        "        experimenting with different annotations (pos, parsing... ); MT quality evaluation...\n",
        "    '''\n",
        "    def __init__(self, SDirName, output_file = 'corpus_out.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = True, include_authors = True, tag='doc', id=1000000, split_by_docs = 0, copy_docs = 0): # initialising by openning the directories\n",
        "        self.SOutput_file = output_file\n",
        "        self.STextFilter = textfilter\n",
        "        self.RFilter = re.compile(textfilter, re.IGNORECASE | re.MULTILINE)\n",
        "        self.BInclTitle = include_title # implemented\n",
        "        self.BInclSectionNames = include_sectionNames # implemented\n",
        "        self.BInclRefs = include_refs # not implemented yet\n",
        "        self.BInclAuth = include_authors # not implemented yet\n",
        "        self.STag = tag\n",
        "        self.ID = id\n",
        "        self.ISplitByDocs = int(split_by_docs)\n",
        "        self.ICopyDocs = int(copy_docs)\n",
        "        # global dictionary of section names (to check and make rules...)\n",
        "        self.DSectNames = defaultdict(int)\n",
        "        # print(self.ISplitByDocs)\n",
        "        self.openDir(SDirName)\n",
        "        self.printDictionary(self.DSectNames, 'corpus-section-names.txt')\n",
        "        return\n",
        "\n",
        "\n",
        "    def openDir(self, path): # implementation of recursively openning directories from a given rule directory and reading each file recursively into a string\n",
        "        i = 0\n",
        "        path_sample = path + '_sample'\n",
        "        if self.ISplitByDocs:\n",
        "            SPartFile = \"part1000000\" + self.SOutput_file\n",
        "            FOut = open(SPartFile, 'w')\n",
        "        else:\n",
        "            FOut = open(self.SOutput_file, 'w')\n",
        "\n",
        "        for root,d_names,f_names in os.walk(path):\n",
        "            for f in f_names:\n",
        "                i+=1\n",
        "                if i%10000==0: print(str(i) + '. Processing: ' + f)\n",
        "                fullpath = os.path.join(root, f)\n",
        "                # print(fullpath)\n",
        "                try:\n",
        "                    FIn = open(fullpath,'r')\n",
        "                    SIn = FIn.read()\n",
        "                    # apply text filter, if not None\n",
        "                    if self.STextFilter and (re.search(self.RFilter, SIn) == None): continue\n",
        "                    SText2Write = self.procFile(SIn,f,i)\n",
        "                    if SText2Write: FOut.write(SText2Write) # if the string is not empty then write to file\n",
        "                    FIn.close()\n",
        "                except:\n",
        "                    print(f'file {f} cannot be read or processed')\n",
        "                finally:\n",
        "                    # splitting output into chunks of \"split_by_docs\" size\n",
        "                    if self.ISplitByDocs and (i % self.ISplitByDocs == 0): # if self.ISplitByDocs == 0 then everything goes into one file; if this > 0 then\n",
        "                        SPartFile = \"part\" + str(1000000 + i) + self.SOutput_file # generate new file name\n",
        "                        FOut.flush()\n",
        "                        FOut.close()\n",
        "                        FOut = open(SPartFile, 'w')\n",
        "                    if self.ICopyDocs and (i >= self.ICopyDocs) and (i < (self.ICopyDocs + self.ISplitByDocs)):\n",
        "                        try:\n",
        "                            SOutputDirN = root + '_sample'\n",
        "                            SOutputFN = os.path.join(SOutputDirN, f)\n",
        "                            os.system(f'cp {fullpath} {SOutputFN}')\n",
        "                        except:\n",
        "                            print('.')\n",
        "\n",
        "        FOut.flush()\n",
        "        FOut.close()\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def procFile(self, SIn,SFNameIn,i): # sending each json string for extraction of text and attaching an correct tags to each output string output string\n",
        "        STagOpen = '<' + self.STag + ' id=\"' + self.STag + str(self.ID + i)  + '\">\\n'\n",
        "        STagClose = '\\n</' + self.STag + '>\\n\\n'\n",
        "        SText4Corpus = self.getJson(SIn, SFNameIn)\n",
        "        if SText4Corpus:\n",
        "            return STagOpen + SText4Corpus + STagClose\n",
        "        else:\n",
        "            print('\\tNo data read from: ' + SFNameIn)\n",
        "            return None\n",
        "\n",
        "\n",
        "    def getJson(self, SIn, SFNameIn): # for each file-level string read from a file: managing internal structure of the covid-19 json file\n",
        "        LOut = [] # collecting a list of strings\n",
        "        try:\n",
        "            DDoc = json.loads(SIn)\n",
        "        except:\n",
        "            print('\\t\\t' + SFNameIn + ' => error reading json2dictionary')\n",
        "            return None\n",
        "        # metadata:\n",
        "        try:\n",
        "            DMetaData = DDoc['metadata']\n",
        "            if DMetaData:\n",
        "                SMetaData = self.getJson_Metadata(DMetaData)\n",
        "                if SMetaData: LOut.append(SMetaData)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no metadata')\n",
        "            DMetaData = None\n",
        "        # body text\n",
        "        try:\n",
        "            LBodyText = DDoc['body_text']\n",
        "            if LBodyText:\n",
        "                SBodyText = self.getJson_BodyText(LBodyText)\n",
        "                LOut.append(SBodyText)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no body_text')\n",
        "            LBodyText = None\n",
        "        # further: to implement references\n",
        "\n",
        "        SText = '\\n\\n'.join(LOut)\n",
        "        return SText\n",
        "\n",
        "\n",
        "    def getJson_Metadata(self, DIn): # converts interesting parts of metadata into a string\n",
        "        SMetadata = ''\n",
        "        LMetadata = []\n",
        "        try: STitle = DIn[\"title\"]\n",
        "        except: STitle = None\n",
        "        if STitle and self.BInclTitle:\n",
        "            LMetadata.append(STitle)\n",
        "\n",
        "        # to implement reading of authors' names\n",
        "\n",
        "        if LMetadata: SMetadata = '\\n\\n'.join(LMetadata)\n",
        "        return SMetadata\n",
        "\n",
        "\n",
        "    def getJson_BodyText(self, LIn): # converts interesting parts of the body texts into a string\n",
        "        SBodyText = ''\n",
        "        LBodyText = []\n",
        "        SSectionName0 = '' # current section name set to empty for a new text\n",
        "        # todo: later, in post-processing stage for the whole corpus, maybe after lemmatization...\n",
        "        # ISampleNumber = 0 # samples of 100 words in text; they do not cross section boundaries \n",
        "\n",
        "        for DParagraph in LIn:\n",
        "            # sections added 2022-09-28\n",
        "            try:\n",
        "                # DParagraphs[\"section\"] ## distinction between different sections....\n",
        "                SSectionName = DParagraph[\"section\"]\n",
        "                # normalizing new section name (1)\n",
        "                SSectionName = SSectionName.replace(\"\\n\", \" \")\n",
        "\n",
        "                if self.BInclSectionNames and SSectionName: # if we opted to include section names and section name is not empty\n",
        "                    if SSectionName != SSectionName0: # if we found a new section name\n",
        "                        # processing section name\n",
        "                        SSectionName0 = SSectionName # change the current section name\n",
        "\n",
        "                        # normalizing section name (2)\n",
        "                        SSectionNameNorm = SSectionName.lower()\n",
        "                        SSectionNameNorm = re.sub('[0-9\\.]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = re.sub('[ ]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = SSectionNameNorm.strip()\n",
        "                        \n",
        "                        self.DSectNames[SSectionNameNorm] += 1\n",
        "                        SSect4text = f'<section sName=\"{SSectionNameNorm}\">\\n{SSectionName}\\n</section>'\n",
        "\n",
        "                        LBodyText.append(SSect4text)\n",
        "            except:\n",
        "                print('S!',)\n",
        "                continue\n",
        "            # first original section (we extract text after extracting section name)\n",
        "            try:\n",
        "                ## DParagraphs[section] ## -- later on >> distinction between different sections....\n",
        "                SParagraph = DParagraph[\"text\"]\n",
        "                LBodyText.append(SParagraph)\n",
        "            except:\n",
        "                print('!',)\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "        SBodyText = '\\n\\n'.join(LBodyText)\n",
        "        return SBodyText\n",
        "\n",
        "    def printDictionary(self, DFreq, SFOutDict):\n",
        "        FOutDict = open(SFOutDict, 'w')\n",
        "        for key, val in sorted(DFreq.items(), key=lambda x: x[1], reverse=True):\n",
        "            FOutDict.write(f'{key}\\t{str(val)}\\n')\n",
        "        FOutDict.flush()\n",
        "        FOutDict.close()\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# arguments:\n",
        "'''\n",
        "        sys.argv[1], # obligatory: input directory name;\n",
        "            other arguments optional:\n",
        "            output_file = 'covid19corpus.txt',\n",
        "            textfilter = None, # if this is string, only texts containing it are collected, e.g., covid\n",
        "            include_title = True, # include or exclude title\n",
        "            include_refs = False, # not implemented yet: include or exclude references\n",
        "            split_by_docs=0 # split by groups of n documents; if 0 then write to one file\n",
        "\n",
        "'''\n",
        "\n",
        "'''if __name__ == '__main__':\n",
        "    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=240000)\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"if __name__ == '__main__':\\n    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=240000)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## full corpus:"
      ],
      "metadata": {
        "id": "FJOv50sk7yDR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2EUP7kS5fkW"
      },
      "source": [
        "# remove parameter textfilter='covid', to return all documents\n",
        "# change parameter split_by_docs=40000 to split_by_docs=0 to return a single file instead of ~5 parts with <40000 in each\n",
        "OJsonDir2txt = clJsonDir2txt(\"document_parses/pmc_json\", output_file = 'cord19.txt', textfilter='covid', include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=40000, copy_docs=240000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2zYiDkn5Vbo"
      },
      "source": [
        "## numbers from previous version\n",
        "This cell will executre reading of json files into a single (or multiple) files\n",
        "\n",
        "Change the value of \"split_by_docs=0\" to \"split_by_docs=10000\" or any number ; this will create several corpus files with 10000 or any required number fo documents per file, which you wish to have.\n",
        "\n",
        "\n",
        "Approximate execution time ~10 min\n",
        "\n",
        "\n",
        "File size to download ~4.3 GB\n",
        "\n",
        "It contains ~198.000 documents,\n",
        "\n",
        "~ 671.578.587 words\n",
        "\n",
        "~ 19.381.647 paragraphs (including empty lines, i.e., ~10M real paragraphs)\n",
        "\n",
        "~ 4.619.100.883 characters\n",
        "\n",
        "## numbers for the last version\n",
        "Approximate execution time ~20 min\n",
        "\n",
        "8 BNC-size (100mw) files are generted, containg together\n",
        "\n",
        "~ 315.000 documents\n",
        "\n",
        "  827.118.629 words\n",
        "\n",
        "   22.094.653 paragraphs\n",
        "\n",
        "5.650.921.315 characters\n",
        "\n",
        "\n",
        "Download time can take up to 1 hour depending on your connection speed.\n",
        "\n",
        "To split into ~BNC size chunks (100MW), split into groups of ~40000 documents (in the following cell set \"split_by_docs=20000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VRGnoGMTH_gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "writing frequent section names (ordered by descending frequency, from highest to 1)"
      ],
      "metadata": {
        "id": "eTbi965g2zqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=500000 part1240000cord19.txt >part1240000cord19-500k.txt"
      ],
      "metadata": {
        "id": "0ELJw_EvK133"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls document_parses/pmc_json_sample | wc -l\n",
        "!du -sh document_parses/pmc_json_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkPWJaJLOG_e",
        "outputId": "eb830fb2-2f6a-4e21-8570-a5836bbc26ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40000\n",
            "3.6G\tdocument_parses/pmc_json_sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvzf document_parses_sample.tar.gz document_parses/pmc_json_sample"
      ],
      "metadata": {
        "id": "ZCW92xAFPQtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=75 /content/corpus-section-names.txt"
      ],
      "metadata": {
        "id": "kJeGtnHK5BJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=1000 /content/corpus-section-names.txt > corpus-selection-names-top1000.txt"
      ],
      "metadata": {
        "id": "6xEeB_bip1sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZN2v7snE7iQ"
      },
      "source": [
        "If you have split the text into parts, you can see the number of words in each part using this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIr8NG15EVro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb9dcf5-4a10-4836-dad6-3d9e90568209"
      },
      "source": [
        "!wc part*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   4085687  108914903  752390539 part1000000cord19.txt\n",
            "   4083605  109685695  756986287 part1040000cord19.txt\n",
            "   4117431  110113881  760911812 part1080000cord19.txt\n",
            "   4085114  109782614  757565426 part1120000cord19.txt\n",
            "   4156737  110454560  764015682 part1160000cord19.txt\n",
            "   4051419  109593018  756536541 part1200000cord19.txt\n",
            "   4110526  110115301  761073243 part1240000cord19.txt\n",
            "   3639938   98300665  678104874 part1280000cord19.txt\n",
            "  32330457  866960637 5987584404 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip part1000000cord19.txt\n",
        "!gzip part1040000cord19.txt\n",
        "!gzip part1080000cord19.txt\n",
        "!gzip part1120000cord19.txt\n",
        "!gzip part1160000cord19.txt\n",
        "!gzip part1200000cord19.txt\n",
        "!gzip part1240000cord19.txt\n",
        "!gzip part1280000cord19.txt"
      ],
      "metadata": {
        "id": "TB_sGebiyb08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternative: working with sample 100mw"
      ],
      "metadata": {
        "id": "oi-DSCshH8a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OJsonDir2txt = clJsonDir2txt(\"document_parses/pmc_json_sample\", output_file = 'cord19.txt', textfilter='covid', include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqNr0807I7tx",
        "outputId": "f4377b95-4951-46ec-9f61-f54905e53092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000. Processing: PMC7531324.xml.json\n",
            "20000. Processing: PMC8588348.xml.json\n",
            "30000. Processing: PMC8897106.xml.json\n",
            "40000. Processing: PMC8634281.xml.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=10000 cord19.txt >cord19_10k.txt"
      ],
      "metadata": {
        "id": "MIQbWMZ2KAF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmZdmvgVFF1v"
      },
      "source": [
        "To see the number of words, paragraphs in your corpus you can use this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p8xxDTvAkv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e4ab86-a1d1-49bf-851b-b7ca51f4b7d8"
      },
      "source": [
        "!wc cord19_10k.txt\n",
        "!wc cord19.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  10000  278029 1915103 cord19_10k.txt\n",
            "  4110512 110114834 761070060 cord19.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TreeTagger run on corpus"
      ],
      "metadata": {
        "id": "DiP3Lo53MFV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm -r treetagger/"
      ],
      "metadata": {
        "id": "ojknZk14OaOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# downloading and testing TreeTagger\n",
        "mkdir treetagger\n",
        "cd treetagger\n",
        "# Download the tagger package for your system (PC-Linux, Mac OS-X, ARM64, ARMHF, ARM-Android, PPC64le-Linux).\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/tree-tagger-linux-3.2.4.tar.gz\n",
        "tar -xzvf tree-tagger-linux-3.2.4.tar.gz\n",
        "# Download the tagging scripts into the same directory.\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/tagger-scripts.tar.gz\n",
        "gunzip tagger-scripts.tar.gz\n",
        "# Download the installation script install-tagger.sh.\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/install-tagger.sh\n",
        "# Download the parameter files for the languages you want to process.\n",
        "# list of all files (parameter files) https://cis.lmu.de/~schmid/tools/TreeTagger/#parfiles\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/english.par.gz\n",
        "sh install-tagger.sh\n",
        "cd ..\n",
        "sudo pip install treetaggerwrapper\n",
        "# changing options: no-unknown, sgml, lemma\n",
        "mv /content/treetagger/cmd/tree-tagger-english /content/tree-tagger-english0\n",
        "awk '{ if (NR == 9) print \"OPTIONS=\\\"-token -lemma -sgml -no-unknown\\\"\"; else print $0}' /content/tree-tagger-english0 > /content/treetagger/cmd/tree-tagger-english\n",
        "chmod a+x ./treetagger/cmd/tree-tagger-english\n",
        "\n",
        "# downloading German and Georgian \n",
        "wget https://heibox.uni-heidelberg.de/f/ec8226edebb64a359407/?dl=1\n",
        "mv index.html?dl=1 /content/treetagger/lib/german-utf8.par\n",
        "wget https://heibox.uni-heidelberg.de/f/9183090d2bdb41e09055/?dl=1\n",
        "mv index.html?dl=1 /content/treetagger/lib/georgian.par\n",
        "\n",
        "wget https://heibox.uni-heidelberg.de/f/9cafab0509d64ed1ac4b/?dl=1\n",
        "mv index.html?dl=1 /content/treetagger/cmd/tree-tagger-georgian2\n",
        "# German2 = -no-unknown \n",
        "# note: tree-tagger-german will not work, as parameter files have not been downloaded, onlz use tree-tagger-german2\n",
        "wget https://heibox.uni-heidelberg.de/f/acb9b8a2fa4f40e08f8a/?dl=1\n",
        "mv index.html?dl=1 /content/treetagger/cmd/tree-tagger-german2\n",
        "chmod a+x /content/treetagger/cmd/tree-tagger-georgian2\n",
        "chmod a+x /content/treetagger/cmd/tree-tagger-german2\n",
        "\n",
        "# test text download\n",
        "wget https://heibox.uni-heidelberg.de/f/cdf240db84ca4718b718/?dl=1\n",
        "mv index.html?dl=1 go1984en.txt\n",
        "wget https://heibox.uni-heidelberg.de/f/ea06aa47fe2d49959a62/?dl=1\n",
        "mv index.html?dl=1 go1984de.txt\n",
        "wget https://heibox.uni-heidelberg.de/f/318b32556cdc44d38238/?dl=1\n",
        "mv index.html?dl=1 go1984ka.txt\n"
      ],
      "metadata": {
        "id": "IGnqej7-MDkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "oBSr4WXhPa6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample - Tagging\n",
        "!./treetagger/cmd/tree-tagger-english go1984en.txt >go1984en_2_vert.txt\n",
        "!./treetagger/cmd/tree-tagger-german2 go1984de.txt >go1984de_2_vert.txt\n",
        "!./treetagger/cmd/tree-tagger-georgian2 go1984ka.txt >go1984ka_2_vert.txt"
      ],
      "metadata": {
        "id": "lFrfgzINPhwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./treetagger/cmd/tree-tagger-english cord19_10k.txt >cord19_10k.vert"
      ],
      "metadata": {
        "id": "Z0EfdXOJQXMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./treetagger/cmd/tree-tagger-english cord19.txt >cord19.vert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQwqTmdIQfIb",
        "outputId": "90b70e05-2c28-4547-bd7f-d329f3889cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\treading parameters ...\n",
            "\ttagging ...\n",
            "126567000\t finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvzf cord19-vert.tgz cord19.vert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMJMxjTpSYSP",
        "outputId": "e90f934c-6f1f-475a-dcc7-8c0d165d505b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cord19.vert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!awk -F '\\t' '{if(NF==3) printf \"%s \", $3; else printf \"\\n%s\\n\", $0}' < cord19_10k.vert >cord19_10k.lem"
      ],
      "metadata": {
        "id": "I1l6ud33ZYIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !awk -F '\\t' '(NF==3){printf \"%s \", $3; if(FNR % 10000 == 0){printf \"\\n\"}}' < cord19_1k.vert >cord19_1k.lem\n",
        "# !awk -F '\\t' '{if(NF==3) printf \"%s \", $3; else printf \"\\n%s\\n\", $0}' < cord19_1k.vert >cord19_1k02.lem\n",
        "\n",
        "!awk -F '\\t' '{if(NF==3) printf \"%s \", $3; else printf \"\\n%s\\n\", $0}' < cord19.vert >cord19.lem"
      ],
      "metadata": {
        "id": "ZFASqb85Q3Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvzf cord19-lem.tgz cord19.lem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzg_XhWCY-Hm",
        "outputId": "86986b51-0596-4476-ab82-2f2ddb059d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cord19.lem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalising xml\n",
        "import os, re, sys\n",
        "FOut = open('cord19_10k.lems', 'w')\n",
        "with open('cord19_10k.lem', 'r') as file:\n",
        "# FOut = open('cord19.lems', 'w')\n",
        "# with open('cord19.lem', 'r') as file:\n",
        "\n",
        "    for line in file:\n",
        "        if line.startswith('<doc'): BStartDoc = True\n",
        "        if line.startswith('</section>'): continue\n",
        "        if line.startswith('<section') and BStartDoc == True: BStartDoc = False; FOut.write(line); continue\n",
        "        if line.startswith('<section') and BStartDoc == False: FOut.write('</section>\\n' + line); continue\n",
        "        if line.startswith('</doc>'): FOut.write('</section>\\n' + line); continue\n",
        "        FOut.write(line)\n",
        "\n",
        "    # data = file.read()\n"
      ],
      "metadata": {
        "id": "-vn4dXn6g7wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=20 cord19.lems"
      ],
      "metadata": {
        "id": "oFdrYZfFjJf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvzf cord19-lems.tgz cord19.lems"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBTgjgVCCNaq",
        "outputId": "15a02028-0a5d-40e5-cff3-d95375057ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cord19.lems\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## downloading the file which is ready...\n",
        "### test file"
      ],
      "metadata": {
        "id": "7610I-RBHYDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/69e0c866bf3c4ccc8435/?dl=1\n",
        "!mv index.html?dl=1 cord19-10k.lems"
      ],
      "metadata": {
        "id": "1o-ZbIEFHsT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc cord19-100k.lems\n",
        "!head --lines=20 cord19-10k.lems"
      ],
      "metadata": {
        "id": "K14PByNIH9lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### full-size file\n"
      ],
      "metadata": {
        "id": "c3nAgMiv_z7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/3bf283c19c5742d1ac3b/?dl=1\n",
        "!mv index.html?dl=1 cord19-lems.tgz"
      ],
      "metadata": {
        "id": "kiU-AngRHLfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvzf cord19-lems.tgz\n",
        "!wc cord19.lems"
      ],
      "metadata": {
        "id": "lp93jNKuZB8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "804d2385-69e1-4438-e9a7-2087f0e7f9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cord19.lems\n",
            "  1437158 128869077 762343572 cord19.lems\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=150000 cord19.lems >cord19-100k.lems\n",
        "!wc cord19-100k.lems\n"
      ],
      "metadata": {
        "id": "gYetK2Vig5ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## end: lemmatization"
      ],
      "metadata": {
        "id": "QFq8Lfq9ZoOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sections and 100-word samples\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tLABeiVpZ_hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading mapping rules\n",
        "!wget https://heibox.uni-heidelberg.de/f/32342a3aa0d04a259bf9/?dl=1\n",
        "!mv index.html?dl=1 covid-sections-and-keywords.zip\n"
      ],
      "metadata": {
        "id": "9hny9cLc2wn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b042e876-8c07-4365-bb37-b16e75fa4447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-11 09:13:39--  https://heibox.uni-heidelberg.de/f/32342a3aa0d04a259bf9/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/e3521076-e0a6-49c1-8a81-16fea2d3036f/covid-sections-and-keywords.zip [following]\n",
            "--2022-10-11 09:13:40--  https://heibox.uni-heidelberg.de/seafhttp/files/e3521076-e0a6-49c1-8a81-16fea2d3036f/covid-sections-and-keywords.zip\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12673 (12K) [application/zip]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>]  12.38K  72.0KB/s    in 0.2s    \n",
            "\n",
            "2022-10-11 09:13:40 (72.0 KB/s) - ‘index.html?dl=1’ saved [12673/12673]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip covid-sections-and-keywords.zip"
      ],
      "metadata": {
        "id": "fhLLSDYy287z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/5297df1ee5dc4e7eb69f/?dl=1\n",
        "!mv index.html?dl=1 connectors-2022-en-v05.csv"
      ],
      "metadata": {
        "id": "sw8VTQharpZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, sys\n",
        "from bs4 import BeautifulSoup\n",
        "class clXml2Stat(object):\n",
        "    '''\n",
        "    The class will create statistics for documents and sections marked in the file\n",
        "    '''\n",
        "    def __init__(self, SFXmlInput, output_file = 'corpus_out_stat.txt'):\n",
        "        # FXmlInput = open(SFXmlInput, 'r')\n",
        "        with open(SFXmlInput, 'r') as file:\n",
        "            data = file.read()\n",
        "        FTsvOutput = open(output_file, 'w')\n",
        "        FTsvOutputNum = open(output_file + '_n.txt', 'w')\n",
        "\n",
        "        print('file read into memory')\n",
        "        FSectionMap = open('covid-sections.tsv', 'r')\n",
        "        self.DSectionMap = self.readTsv2dict(FSectionMap, 0, 2)\n",
        "        FSectionRules = open('covid_section_rules.tsv', 'r')\n",
        "        FSectionLexRND = open('covid-random.tsv', 'r')\n",
        "        FSectionLexMI = open('covid-mi.tsv', 'r')\n",
        "        FSectionConn = open('connectors-2022-en-v05.csv', 'r')\n",
        "\n",
        "        self.LTMapRules = self.readTsv2reRules(FSectionRules, 0, 1)\n",
        "        self.LTMapLexRND = self.readTsv2reRules(FSectionLexRND, 0, 1, priority = ['KEY_N', 'M_ARG', 'EVAL'], prefix = ' ')\n",
        "        self.LTMapLexMI = self.readTsv2reRules(FSectionLexMI, 0, 1, priority = ['KEY_N', 'M_ARG', 'EVAL'], prefix = ' ')\n",
        "\n",
        "        self.LTMapConnTypes = self.readTsv2reRules(FSectionConn, 0, 2, flags = re.IGNORECASE, priority = ['cTime', 'cReason', 'cResult', 'cCondition', 'cSequence', 'cAddition', 'cPurpose', 'cConcession', 'cComparison', 'cContrast', 'cClarification', 'cReference', 'cViewpoint', 'cPossibility', 'cAttitude', 'cEmphasis'], prefix = ' ', suffix = '(?= )')\n",
        "        FSectionConn.close()\n",
        "        FSectionConn = open('connectors-2022-en-v05.csv', 'r')\n",
        "        self.LTMapConnGroups = self.readTsv2reRules(FSectionConn, 0, 1, flags = re.IGNORECASE, priority = ['ca', 'co', 'cs'], prefix = ' ', suffix = '(?= )')\n",
        "\n",
        "        count = self.runMain(data, FTsvOutput, FTsvOutputNum, samplesize = 100)\n",
        "        print(str(count))\n",
        "\n",
        "        return\n",
        "\n",
        "    def readTsv2dict(self, FInTSV, IFieldLeft, IFieldRight):\n",
        "        DMap = {}\n",
        "        for SLine in FInTSV:\n",
        "            SLine = SLine.strip()\n",
        "            LLine = re.split('\\t', SLine)\n",
        "            try:\n",
        "                k = LLine[IFieldLeft]\n",
        "                v = LLine[IFieldRight]\n",
        "                DMap[k] = v\n",
        "            except:\n",
        "                continue\n",
        "        return DMap\n",
        "\n",
        "    def readTsv2reRules(self, FInTSV, IFieldLeft, IFieldRight, flags = None, priority = ['Introduction', 'Statements', 'Methods', 'Conclusion', 'Discussion', 'Presentation', 'Background', 'Results', 'Remove'], prefix = None, suffix = None):\n",
        "        LTMap = []\n",
        "        DRe = {}\n",
        "        # initialising dictionary where the mapping will be done, then reversing\n",
        "        for el in priority:\n",
        "            DRe[el] = [] # first the list is empty\n",
        "        for SLine in FInTSV:\n",
        "            SLine = SLine.strip()\n",
        "            LLine = re.split('\\t', SLine)\n",
        "            try:\n",
        "                v = LLine[IFieldLeft]; v = v.strip()\n",
        "                k = LLine[IFieldRight]; k = k.strip()\n",
        "                if prefix:\n",
        "                    v = prefix + v\n",
        "                if suffix:\n",
        "                    v = v + suffix\n",
        "            except:\n",
        "                sys.stdout.write('f')\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                LWords2map = DRe[k]\n",
        "                LWords2map.append(v) # adding new word for the mapping\n",
        "                DRe[k] = LWords2map\n",
        "            except:\n",
        "                sys.stdout.write('d')\n",
        "                continue\n",
        "\n",
        "        for el in priority:\n",
        "            print('el:' + el)\n",
        "            LRe = DRe[el]\n",
        "            SRe2map = '|'.join(LRe)\n",
        "            print('\\t' + SRe2map)\n",
        "            if flags:\n",
        "                RE4map = re.compile(SRe2map, flags)\n",
        "            else:\n",
        "                RE4map = re.compile(SRe2map)\n",
        "            TMap = (el, RE4map)\n",
        "            LTMap.append(TMap)\n",
        "\n",
        "        return LTMap\n",
        "\n",
        "    # use instead the function readTsv2reRules (above)\n",
        "    def readTsv2lexMatch(self, FInTSV, IFieldLeft, IFieldRight, priority = ['KEY_N', 'EVAL', 'M_ARG']):\n",
        "        LTMap = []\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def runMain(self, data, FTsvOutput, FTsvOutputNum, samplesize = 100):\n",
        "        soup = BeautifulSoup(data)\n",
        "        i = 0\n",
        "        gnorm = 0\n",
        "        grule = 0\n",
        "        gnomatch = 0\n",
        "        # FTsvOutput.write(str(docnumber) + '\\t' + str(section_number) + '\\t' + str(count_samples) + '\\t' + str(lensample) + '\\t' + str(lendict)  + '\\t' + str(SCatRnd)  + '\\t' + str(SCatMI) + '\\t' + SCatNorm + '\\t' + SSectNameNorm + '\\t' + sw + str(SCatRndWords)  + '\\t' + str(SCatMIWords) + '\\n')\n",
        "        # new string (found words in front)\n",
        "        # FTsvOutput.write(str(docnumber) + '\\t' + str(section_number) + '\\t' + str(count_samples) + '\\t' + str(lensample) + '\\t' + str(lendict)  + '\\t' + str(SCatRnd)  + '\\t' + str(SCatMI) + '\\t' + SCatNorm + '\\t' + SSectNameNorm + '\\t' + str(SCatRndWords)  + '\\t' + str(SCatMIWords) + sw + '\\n')\n",
        "\n",
        "        # second file without actual string (to save space)\n",
        "        # 16 + 16 headers for types of connectors\n",
        "        LHeaders = ['cTime', 'cReason', 'cResult', 'cCondition', 'cSequence', 'cAddition', 'cPurpose', 'cConcession', 'cComparison', 'cContrast', 'cClarification', 'cReference', 'cViewpoint', 'cPossibility', 'cAttitude', 'cEmphasis']\n",
        "        LCountHeaders = []\n",
        "        LFoundHeaders = []\n",
        "\n",
        "        for el in LHeaders:\n",
        "            cHead = 'c' + el\n",
        "            wHead = 'w' + el\n",
        "            LCountHeaders.append(cHead)\n",
        "            LFoundHeaders.append(wHead)\n",
        "\n",
        "        SCountHeaders = '\\t'.join(LCountHeaders)\n",
        "        SFoundHeaders = '\\t'.join(LFoundHeaders)\n",
        "\n",
        "        FTsvOutput.write('doc_n' + '\\t' + 'sec_n' + '\\t' + 'smp_n' + '\\t' + 'smp_l' + '\\t' + 'dct_l'  + '\\t' + 'nC_a'  + '\\t' + 'nC_o'  + '\\t' + 'nC_s'   + '\\t' + 'wdC_a'  + '\\t' + 'wdC_o'  + '\\t' + 'wdC_s' + '\\t' + SCountHeaders + '\\t' + SFoundHeaders + '\\t' + 'nKN_R'  + '\\t' + 'nMA_R'  + '\\t' + 'nEV_R'  + '\\t' + 'nKN_MI'  + '\\t' + 'nMA_MI'  + '\\t' + 'nEV_MI' + '\\t' + 'SCatNorm' + '\\t' + 'SSectNameNorm' + '\\t' + 'wKN_RndWords' + '\\t' + 'wMA_RndWords' + '\\t' + 'wEV_RndWords'  + '\\t' + 'wKN_MIWords' + '\\t' + 'wMA_MIWords' + '\\t' + 'wEV_MIWords' + '\\t' + 'sw' + '\\n')\n",
        "        FTsvOutputNum.write('doc_n' + '\\t' + 'sec_n' + '\\t' + 'smp_n' + '\\t' + 'smp_l' + '\\t' + 'dct_l'  + '\\t' + 'nC_a'  + '\\t' + 'nC_o'  + '\\t' + 'nC_s'   + '\\t' + 'wdC_a'  + '\\t' + 'wdC_o'  + '\\t' + 'wdC_s' + '\\t' + SCountHeaders + '\\t' + SFoundHeaders + '\\t' + 'nKN_R'  + '\\t' + 'nMA_R'  + '\\t' + 'nEV_R'  + '\\t' + 'nKN_MI'  + '\\t' + 'nMA_MI'  + '\\t' + 'nEV_MI' + '\\t' + 'SCatNorm' + '\\t' + 'SSectNameNorm' + '\\t' + 'wKN_RndWords' + '\\t' + 'wMA_RndWords' + '\\t' + 'wEV_RndWords'  + '\\t' + 'wKN_MIWords' + '\\t' + 'wMA_MIWords' + '\\t' + 'wEV_MIWords' + '\\n')\n",
        "\n",
        "        for doc in soup.find_all('doc'):\n",
        "            # print(type(str(doc)))\n",
        "            i+=1\n",
        "            norm, rule, nomatch = self.procDoc(str(doc), FTsvOutput, FTsvOutputNum, i, samplesize)\n",
        "            gnorm += norm; grule += rule; gnomatch += nomatch\n",
        "            if i%1000 == 0:\n",
        "                print(str(i))\n",
        "            # print(str(i))\n",
        "        print(f'\\tgnorm={gnorm}\\tgrule={grule}\\tgnonorm={gnomatch}')\n",
        "        return i\n",
        "\n",
        "    def procDoc(self, SDoc, FTsvOutput, FTsvOutputNum, docnumber, samplesize = 100):\n",
        "        SDoc = SDoc.replace('\\n', ' ')\n",
        "        SDoc = SDoc.replace('\\t', ' ')\n",
        "        # LSections = re.findall('<section sname=\\\"([^\\\"]+)\\\">', SDoc, re.DOTALL | re.MULTILINE | re.IGNORECASE)\n",
        "        LTSections = re.findall('<section sname=\\\"([^\\\"]+)\\\">([^<]+)</section>', SDoc, re.DOTALL | re.MULTILINE | re.IGNORECASE)\n",
        "        # LSections = re.findall('<section([^<]+)</section>', SDoc, re.DOTALL | re.MULTILINE | re.IGNORECASE)\n",
        "        norm = 0\n",
        "        nomatch = 0\n",
        "        rule = 0\n",
        "        section_number = 0\n",
        "        for TSection in LTSections:\n",
        "            SSectName, SSectionText = TSection\n",
        "            SCatFound = False\n",
        "            section_number += 1\n",
        "\n",
        "            if SSectName in self.DSectionMap.keys():\n",
        "                SSectNameNorm = self.DSectionMap[SSectName]\n",
        "                norm += 1\n",
        "                SCatNorm = 'MAP'\n",
        "                SCatFound = True\n",
        "\n",
        "            else:\n",
        "                for category, regexpression in self.LTMapRules:\n",
        "                    if re.search(regexpression, SSectName):\n",
        "                        SSectNameNorm = category\n",
        "                        SCatNorm = 'RULE'\n",
        "                        SCatFound = True\n",
        "                        rule += 1\n",
        "                        break\n",
        "                if SCatFound == False:\n",
        "                    SSectNameNorm = SSectName\n",
        "                    SCatNorm = 'NONORM'\n",
        "                    nomatch += 1\n",
        "            LSectionText = SSectionText.split(' ')\n",
        "            GLLSectionSamples = self.divide_chunks(LSectionText, samplesize)\n",
        "            LLSectionSamples = list(GLLSectionSamples)\n",
        "            LSectionSamplesLast = LLSectionSamples[-1]\n",
        "            try:\n",
        "                if len(LSectionSamplesLast) < samplesize:\n",
        "                    LLSectionSamples[-2].extend(LSectionSamplesLast)\n",
        "                    LLSectionSamples.pop()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            '''\n",
        "            print(LLSectionSamples)\n",
        "            for lw in LLSectionSamples:\n",
        "                print(len(lw))\n",
        "            print(' ')\n",
        "            '''\n",
        "            count_samples = 0\n",
        "            for lw in LLSectionSamples:\n",
        "                count_samples += 1\n",
        "                # print(len(lw))\n",
        "                # find the size of dictionary for this sample of 100 words\n",
        "                setlw = set(lw)\n",
        "                lendict = str(len(setlw))\n",
        "\n",
        "                lensample = str(len(lw))\n",
        "\n",
        "                # create string from list of 100 words\n",
        "                sw = ' '.join(lw)\n",
        "                # count key_n here!!!\n",
        "                LCatRnd = []\n",
        "                LCatMI = []\n",
        "                LCatRndWords = []\n",
        "                LCatMIWords = []\n",
        "\n",
        "                for category, regexpression in self.LTMapLexRND:\n",
        "                    LCatXr = re.findall(regexpression, sw)\n",
        "                    LCatRnd.append(str(len(LCatXr)))\n",
        "                    SCatXr = ' '.join(LCatXr)\n",
        "                    LCatRndWords.append(SCatXr)\n",
        "                for category, regexpression in self.LTMapLexMI:\n",
        "                    LCatXm = re.findall(regexpression, sw)\n",
        "                    LCatMI.append(str(len(LCatXm)))\n",
        "                    SCatXm = ' '.join(LCatXm)\n",
        "                    LCatMIWords.append(SCatXm)\n",
        "\n",
        "                SCatRnd = '\\t'.join(LCatRnd)\n",
        "                SCatMI = '\\t'.join(LCatMI)\n",
        "                SCatRndWords = '\\t'.join(LCatRndWords)\n",
        "                SCatMIWords = '\\t'.join(LCatMIWords)\n",
        "\n",
        "\n",
        "                LCatConnCLen = []\n",
        "                LCatConnCWords = []\n",
        "                for category, regexpression in self.LTMapConnGroups:\n",
        "                    LCatX = re.findall(regexpression, sw)\n",
        "                    LCatConnCLen.append(str(len(LCatX)))\n",
        "                    SCatX = '~'.join(LCatX)\n",
        "                    LCatConnCWords.append(SCatX)\n",
        "                SCatCConn = '\\t'.join(LCatConnCLen) # there are 3 categories of connectors, print 3 fields for lengths\n",
        "                SCatCConnWords = '\\t'.join(LCatConnCWords) # there are 3 categories of words, print 3 fields for examples\n",
        "\n",
        "                LSemClConnCLen = []\n",
        "                LSemClConnCWords = []\n",
        "                for category, regexpression in self.LTMapConnTypes:\n",
        "                    LCatX = re.findall(regexpression, sw)\n",
        "                    LSemClConnCLen.append(str(len(LCatX)))\n",
        "                    SCatX = '~'.join(LCatX)\n",
        "                    LSemClConnCWords.append(SCatX)\n",
        "                SSemClConnCLen = '\\t'.join(LSemClConnCLen) # there are 16 categories of connectors, print 3 fields for lengths\n",
        "                SSemClConnCWords = '\\t'.join(LSemClConnCWords) # there are 16 categories of words, print 3 fields for examples\n",
        "\n",
        "\n",
        "                FTsvOutput.write(str(docnumber) + '\\t' + str(section_number) + '\\t' + str(count_samples) + '\\t' + str(lensample) + '\\t' + str(lendict)  + '\\t' + SCatCConn + '\\t' + SCatCConnWords  + '\\t' + SSemClConnCLen + '\\t' + SSemClConnCWords + '\\t' + str(SCatRnd)  + '\\t' + str(SCatMI) + '\\t' + SCatNorm + '\\t' + SSectNameNorm + '\\t' + str(SCatRndWords)  + '\\t' + str(SCatMIWords) + '\\t' + sw + '\\n')\n",
        "                FTsvOutputNum.write(str(docnumber) + '\\t' + str(section_number) + '\\t' + str(count_samples) + '\\t' + str(lensample) + '\\t' + str(lendict)  + '\\t' + SCatCConn + '\\t' + SCatCConnWords  + '\\t' + SSemClConnCLen + '\\t' + SSemClConnCWords  + '\\t' + str(SCatRnd)  + '\\t' + str(SCatMI) + '\\t' + SCatNorm + '\\t' + SSectNameNorm + '\\t' + str(SCatRndWords)  + '\\t' + str(SCatMIWords) + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "            # FTsvOutput.write(str(docnumber) + '\\t' + str(section_number) + '\\t' + SCatNorm + '\\t' + SSectNameNorm + '\\t' + SSectionText + '\\n')\n",
        "            # print(TSection)\n",
        "        # print(f'norm={norm}\\trule={rule}\\tnonorm={nomatch}')\n",
        "        return norm, rule, nomatch\n",
        "\n",
        "\n",
        "    def divide_chunks(self, l, n):\n",
        "        # looping till length l\n",
        "        for i in range(0, len(l), n):\n",
        "            yield l[i:i + n]\n",
        "\n",
        "    def procSection(self, SSectionText):\n",
        "        LSectionSamples = []\n",
        "        return LSectionSamples\n",
        "\n",
        "\n",
        "    '''\n",
        "    def runMain(self, data, FTsvOutput):\n",
        "        soup = BeautifulSoup(data)\n",
        "        i = 0\n",
        "        for doc in soup.find_all('doc'):\n",
        "            # print(doc)\n",
        "            sections = soup.find_all('section', doc)\n",
        "            # children = soup.findChildren()\n",
        "            k = 0\n",
        "            for section in sections:\n",
        "                k += 1\n",
        "                # print(str(k))\n",
        "                # print(section)\n",
        "            print('\\t' + str(k))\n",
        "\n",
        "            i += 1\n",
        "            print('.')\n",
        "\n",
        "        return i\n",
        "        '''\n",
        "\n",
        "# end: class\n",
        "\n",
        "'''if __name__ == '__main__':\n",
        "    OXml2Stat = clXml2Stat('/content/cord19_1k.lem', output_file = '/content/cord19_1k_stat.txt')\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "q99nzuyFaGi1",
        "outputId": "7f4a34c3-cf95-4413-faeb-75bb8d29ff6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"if __name__ == '__main__':\\n    OXml2Stat = clXml2Stat('/content/cord19_1k.lem', output_file = '/content/cord19_1k_stat.txt')\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OXml2Stat = clXml2Stat('/content/cord19-100k.lems', output_file = '/content/cord19_100k_stat.txt')"
      ],
      "metadata": {
        "id": "hTMIz9i1emSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecbc255a-95b6-4e7f-a82d-e386c92ecc94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file read into memory\n",
            "el:Introduction\n",
            "\tintroduction\n",
            "el:Statements\n",
            "\tauthor|contributions|conflict|interest|declaration|competing|funding|availability|statement|ethics|credit|authorship|contribution|publisher's|conflicts|publisher’s|note|ethical|approval|interests|disclosures|sources|financial|support|sponsorship|considerations|disclosure|acknowledgments|contributors|consent|sharing|guarantor|declarations|additional|provenance|peer|source|authors'|participate|acknowledgements|disclaimer|informed\n",
            "el:Methods\n",
            "\tmethod|statistical analysis|materials|data analysis|data collection|study design|participants|participants|study design|statistical analyses|statistical analyses|study population|study design and participants|analysis|methodology|measures|search strategy|data extraction|design|procedure|study population|procedure|procedures|setting|data sources|statistics|statistics|data analysis|eligibility criteria|study design and participants|measures|statistical methods|study selection|patients|inclusion and exclusion criteria|method|material and methods|study design and setting|sample size|data|patients|study setting|statistical analysis|cell culture|recruitment|data source|sample|design|outcome measures|study design and population|intervention|statistical methods|study participants|search strategy|inclusion criteria|animals|analysis|data extraction|quality assessment|outcomes|covariates|definitions|variables|procedures|methodology|materials\n",
            "el:Conclusion\n",
            "\tconclusion|concluding|summary|recommendations|future\n",
            "el:Discussion\n",
            "\tdiscussion|limitation|limitations|strengths|findings|implications|discussions\n",
            "el:Presentation\n",
            "\tan event is serious|treatment|outcome and follow-up|follow-up|application\n",
            "el:Background\n",
            "\tbackground|literature|progress .* research|research .* progress|review|related|overview\n",
            "el:Results\n",
            "\tresult\n",
            "el:Remove\n",
            "\treference\n",
            "el:KEY_N\n",
            "\t agent| antigen| variability| nucleic| acid| purify| method| immunogenicity| solution| infusion| vaccination| intervention| molecular| biology| genetic| bacterial| culture| supernatant| enzyme| genome| database| epidemiological| secrete| vitro| assay| synthetic| dose| fusion| subtype| stem| hospitalisation| lipid| mers| systemic| treatment| statistical| palliative| ratio| mortality| diagnosis| comorbidities| simulate| standardized| resolution| pneumonia| syndrome| sars| symptom| pulmonary| latent| datasets| classification| criterion| opacity| pattern| lobe| pleural| effusion| confidence| fibrosis| linear| clinically| anesthesia| ventilation| avoidance| oxygen| emission| validate| algorithm| evolution| experimental| reproductive| behavioral| dataset| threshold| computational| trigger| predictor| physician| throat| storm| derivative| suppress| longitudinal| error| coefficient| covid| categorize| neurological| saturation| cerebral| renal| stimulus| mri| venous| thrombotic| hypoxia| inpatient| data| sampling| monitoring| telehealth| duplicate| outpatient| symptomatic| determination| clinician| biobank| cytokine| endothelial| tissue| nasopharyngeal| prescribe| steroid| remdesivir| stethoscope| troponin| placebo| nucleotide| species| deletion| comparative| transmembrane| peptide| interferon| transcription| rdrp| furin| nucleocapsid| transcriptional| nucleus| arterial| epithelial| invasive| substrate| dioxide| bioactive| prevalence| secretion| heterogeneity| sequential| dilution| incubate| protease| cascade| clot| tomography| chloroquine| predispose| fibrinogen| allele| corona| adjusted| vivo| helicase| hydrogen| nitrogen| absorption| radiology| autoimmune| ultrasound| cerebrovascular| urine| neurologic| radiation| oncology| plaque| muscle| conditional| forecasting| neural| sterile| aggregation| biomarkers| antimicrobial| murine| oxide| emergent| bead| addressability| sstis| aftereffect| reimagined| speeding| medicalisation| pharmacoenosis| pfps| mdas| aibds| wholistic| ieq| partiality| pandemicity| deprioritized| coerciveness| dsms| mitigated| heor| medicalization| grvs| kdts| pwuds| surveilled| aecopds| chronification| diabetologists| preeminence| visioli| transplantable| palliate| nonnegligible| pervasively| cyclicality| hypovolaemic| compositionally| computerization| mrfs| restrictively| scfs| mplp| chemoreceptive| llin| rvis| dimant| lymphodepletion| uptrend| reinsertion| seclude| crippling| vops| consummation| vesus| ptcp| wfs| nstis| paucibacillary| ssti| cfsgs| undetectability| spts| exopd| hmgcs| toxicities| motoneuron| srms| aminoacylation| vegfs| pdos| smgt| spirally| eracr| prokinetics| antacid| peptidomics| mobilan| mϕ| vams| kawacovid| contributive| underdetection| nosocomially| reexposure| txndc| predisposing| toxicologically| cish| delafloxacin| sublevels| dhapq| exportable| transudation| ejm| dyscrasia| misexpression| hyperlactatemia| mhciilow| xyp| iupd| tsas| chemosensing| tepid| accurateness| lvnc| noncaseating| nops| micturition| eruptive| parabiont| catc| progressed| nonbiologic| chromotripsis| thrombopenia| dermatome| intraglomerular| medullar| desaturate| hyporeactive| pathogeneses| oxog| noncardiac| unvaried| alri| azacitidine| hyphenated| hbecs| hypermutated| hypomorphs| oprelvekin| microcolonies| fibrinolytics| mesotherapy| codiv| dysgonomonas| rndd| poikilosis| apposite| distill| bups| estimands| autopoietic| pubquiz| channelize| subtopics| pscpm| ehi| nitags| hierarchized| proxemic| interpretivist| sbme| prelude| technicality| wwa| kinds| individuate| yardstick| geomorphosites| emrn| topicality| renvoi| idiosyncrasy| forewarn| phenomenologist| gdphrwkd| largescale| jhdi| dohad| crwms| chims| countercyclical| faultline| hygienically| rationing| fbt| huhep| bcpap| esbs\n",
            "el:M_ARG\n",
            "\t study| consistency| say| normally| focus| apparent| speak| reason| motivation| result| cause| demonstrate| analysis| publish| predict| consist| assume| formulate| academic| describe| consideration| uncertain| author| validation| limitation| finding| reflect| inherent| assessment| relevant| revise| theory| confirm| assess| recommend| opinion| advocate| evidence| believe| summarize| conclude| definition| interesting| prediction| sense| nearly| relevance| think| certainly| reasonable| actually| emphasize| analogue| hypothesis| reject| motivate| publication| manuscript| tell| paradigm| conclusion| scholar| communicate| presentation| explanation| contrary| generalize| analyse| motif| consequently| secondly| suggestion| argue| narrative| assumption| peer| disagree| manifest| emphasis| rationale| methodological| means| misattribute| expectable| overlooked| nonrelated| unpredicted| undisputable| seldomly| unsound| arbitrariness| unheard| observationally| parenthetically| ignorable| confirmative| limitedly| overemphasis| contemplated| moot| unproblematic| hypothetic| rebuttal| oddity| enquiring| serendipity| doubtless| incongruous| restate| mindsponge| reemphasize| supposition| relativize\n",
            "el:EVAL\n",
            "\t expertise| risk| disruption| well| outbreak| ebola| notable| achieve| epidemic| preparedness| helpful| threat| crisis| harmful| usefulness| effectiveness| adverse| aware| able| facilitate| ability| easy| unable| preserve| stable| enhance| prevent| emergency| failure| respond| unprecedented| difficulty| crucial| cancer| negative| tumor| developed| pain| beneficial| morbidity| feasible| struggle| mitigate| severe| exposure| distress| severity| contamination| compromise| fit| warning| alert| stroke| decline| tuberculosis| professional| collapse| sick| justice| impair| anxiety| appropriate| unclear| ill| pathology| hemorrhage| experienced| suspected| canada| strength| extreme| reliable| systematically| hazard| referral| depression| deficit| inability| optimize| abuse| poverty| achievement| contaminated| precise| malignant| stability| powerful| ease| fail| accessibility| excessive| inequality| pilot| stabilize| imbalance| liberty| incomplete| discomfort| arthritis| pesticide| strategic| contagious| weakness| intelligent| stressful| saving| suicidal| abnormal| unrealized| uncompromising| calamitous| underfeed| deleteriously| devasting| frustratingly| cataclysmic| expediently| flimsy| disruptions| uncontained| undersupply| deteriorating| remediating| hypervigilant| misbehave| hobble| broadness| insanity| accomplice| promptness| pathetic| overutilization| inadvisable| precociously| ultrarapid| dysfunctioning| painstakingly| admirably| burdens| venerable| gracefully| favoritism| infallible| pressingly| fallible| sparseness| prepotency| incapacitation| undermet| slander| overconfident\n",
            "el:KEY_N\n",
            "\t inactivate| purify| subcutaneous| nasal| immunogenicity| biology| microorganism| recombinant| mutant| codon| supernatant| coli| sequenced| biotechnology| secrete| aureus| synthetic| seed| fda| polymer| epitope| conformational| unstable| monoclonal| subtype| adjuvant| stimulation| hospitalisation| soluble| cholesterol| therapeutics| chemotherapy| numerical| univariate| colorectal| cytotoxic| simulate| multivariable| biomedical| polymerase| abdominal| radiologist| opacity| lobe| diameter| pleural| effusion| covariates| monocyte| dyspnea| radiological| anesthesia| avoidance| intravenous| oxygenation| blockade| extension| hybrid| normalize| axis| classifier| bone| throat| myalgia| derivative| behavioural| compatible| ferritin| ldh| prophylactic| cerebral| creatinine| stimulus| limb| artery| venous| prophylaxis| thrombotic| hypoxemia| transplantation| pubmed| telehealth| duplicate| medicaid| mellitus| orthopaedic| postoperative| surgeon| biobank| genotype| proinflammatory| magnetic| permeability| olfactory| infarction| steroid| spontaneous| remdesivir| epidemiology| stigma| mouth| stethoscope| inject| fluctuation| dominant| resistant| ambient| biomarker| troponin| transplant| azithromycin| evolutionary| species| deletion| depletion| sequencing| transmembrane| interferon| phylogenetic| rdrp| subunits| lineage| furin| nucleocapsid| detectable| transcriptional| translational| cutoff| residual| disseminate| alpha| beta| subunit| nucleus| repair| intracellular| arterial| phosphorylation| diabetic| adhesion| extracellular| invasive| fungus| dioxide| organic| bioactive| microbial| postpartum| serological| secretion| sequential| amplification| reagent| sputum| plasmid| dilution| vero| incubate| elute| cascade| clot| thrombin| tomography| lymphopenia| lupus| anticoagulant| chloroquine| predispose| neuron| microvascular| fibrinogen| likert| polymorphism| allele| neutralization| electron| microscopy| serine| catalytic| prostate| intestinal| epithelium| quantification| patients| prescription| eosinophil| binary| adjusted| composite| helicase| preclinical| unwind| biochemical| hydrogen| nitrogen| rnas| suppression| pharmacological| absorption| interference| endoscopic| radiology| respirator| prevalent| ultrasound| cerebrovascular| urine| transient| neurologic| radiation| oncology| coagulopathy| alveolar| anticoagulation| immunoassay| plaque| muscle| conditional| volatility| exogenous| specification| paediatric| segmentation| neural| biopsy| array| novo| saliva| discrimination| developmental| sterile| filtration| aggregation| hormone| pathophysiology| oestrogen| progesterone| fungal| propensity| fetal| relaxation| mucosal| ion| sodium| oxidative| murine| disinfection| oxide| formula| capillary| optical| emergent| gradient| etiology| stressors| endogenous| precursor| metabolite| bead| sstis| paraviral| addressability| mitigator| exportable| unmonitored| counterweight| restrictively| lymphodepletion| cud| transplantable| huhep| pervasively| rndds| osteosarcopenia| pwuds| afce| aftereffect| semiology| uptrend| progressed| nocturia| ligature| compositionally| overcompensate| schistosome| spirally| alvo| formulaic| sdvs| deprioritized| deglutition| inchoate| phototoxic| phenoconversion| defensiveness| diagnosable| hypercatabolic| referable| decongest| ssti| srms| pandemicity| dsms| pqcs| cyclicality| capabilization| phenotoxicity| insonation| vesus| tlco| allostatine| transudation| nstis| hpaecs| nephrolithiasis| morf| microthromboses| rars| ltri| hyperresponsive| antacid| clots| prokinetics| mϕ| idcm| trqc| dyscrasia| hypovolaemic| mesotherapy| dhapq| autolysis| neuropraxia| chemotypes| congenitally| txndc| hepatoblastoma| microembolic| nrh| ivdd| peptidomics| predisposing| expd| intraglomerular| micturition| epiairway| unconditioned| isoprostane| dnmts| permissivity| extdna| uncharacterised| pretreat| placentitis| noncaseating| transgenics| selenos| anticoagulative| wwa| ghio| ssrnas| svneo| phototoxicity| obrb| edvs| caia| bronchodilation| cosmesis| edeine| inappetence| unresected| lymphovascular| immunoreaction| glycomes| tsas| wfs| pytri| hdts| endomitosis| zscan| thrombopenia| fibrinogenesis| tauopathy| pachynema| flatland| mitotically| aanat| vics| metabolizers| abluminal| chemoresistant| lel| promiscuously| extrarenal| entrypoint| estimands| rndd| uncaptured| heor| microallocation| haphazard| contributive| undercoverage| renvoi| imy| theoriaphobia| subtopics| scammer| monad| lvvos| gdphrwkd| disjuncture| pots| instantiations| inclusionary| clinimetric| nitags| riddor| denotation| pscpm| ocg| ehi| rpis| susars| feps| bups| lvms| socioculturally| phenomenologist| equiptt| determinative| torraco| reliabilism\n",
            "el:M_ARG\n",
            "\t consistency| normally| oppose| speak| publicly| formulate| categorical| statistics| inherent| conceptual| revise| ethics| confirmation| figs| infer| abstract| ignore| known| roughly| certainly| underestimate| analogue| exploratory| reject| correction| surprisingly| anonymous| tell| probable| cf| paradigm| scholar| consult| exploration| speculate| expectancy| obviously| analog| unrelated| motif| conversely| remarkable| reviewer| narrative| intellectual| disagree| argument| synthesize| structured| remarkably| considerably| thought| essentially| briefly| rationale| really| plausible| unless| solely| intrinsic| witness| methodological| implicate| reflection| discourse| justify| means| indubitable| unsound| measurably| ascertained| ignorable| enunciation| emphasized| overlooked| undisputable| foretell| epiphenomenon| warranty| disputed| nonrelated| susceptive| limitedly| trivialize| resounding| hearsay| adamantly| contemplated| misattribute| exceptions| dissonant| naively| acquiesce| unquestioned| enlightening| valuing| iconography| doubtless| disputable| indepth| untruthful| deemphasize| interpellation| incommensurable| warnings| misbelief| interpellate\n",
            "el:EVAL\n",
            "\t kill| excess| suspicion| misinformation| harmful| usefulness| virulence| preserve| malaria| danger| enhancement| careful| developed| toxic| malignancy| struggle| appropriately| heavily| edema| superior| consolidation| viable| obstacle| flexible| alert| precision| war| thank| tuberculosis| collapse| sick| justice| impair| flexibility| traumatic| poorly| delirium| carefully| hemorrhage| ischemic| impaired| defect| experienced| obstructive| safely| collaborative| systematically| necrosis| invasion| thrombosis| wound| deficit| disturbance| caution| inability| abuse| poverty| achievement| disrupt| properly| contaminated| homeless| efficiently| abundant| deleterious| lie| disadvantage| detrimental| parasite| malignant| powerful| dengue| accessibility| complaint| aggravate| allergic| exacerbation| attractive| stabilize| competence| viability| imbalance| liberty| diarrhea| incomplete| anomaly| illegal| untreated| discomfort| racism| complicate| competency| nervous| thrombus| arthritis| innovative| hinder| pesticide| reward| strategic| instability| psychotic| depressive| obese| weakness| rescue| anticancer| intelligent| autonomy| expensive| comfort| moral| saving| addiction| overdose| suicidal| freedom| hepatic| unrealized| remediable| relapses| undesirably| expediently| frustratingly| accomplice| weariness| inordinate| misdiagnosing| unwillingly| deathly| cataclysmic| hobble| lameness| echolalia| comorbidites| ineffectively| misprice| painstakingly| underprepared| deride| nihilistic| composure| outlandish| intratumor| dysimmunity| machiavellians| backpain| fickle| venerable| unnerving| excitingly| uninjured| magister| pneumonias| aberrancy| broadness| sacredness| infallible| resolute| obsessed| undemocratic| routinized| slander| murderer| calamitous| inadmissible| particularism| overblown| overconfident| burdens\n",
            "el:cTime\n",
            "\t Then(?= )| Afterwards(?= )| Later(?= )| Just than(?= )| At the same time(?= )| Before that(?= )| Hitherto(?= )| Previously(?= )| At once(?= )| Thereupon(?= )| Straightaway(?= )| Soon(?= )| After a while(?= )| Meanwhile(?= )| During(?= )| In the meantime(?= )| All that time(?= )| At this moment(?= )| Once(?= )| After(?= )| When(?= )| While(?= )| Until(?= )| Previously(?= )| Before(?= )| Up to that point(?= )| At this point(?= )| Here(?= )| Now(?= )| And now(?= )| An hour later(?= )| That morning(?= )| Last time(?= )| Last week(?= )| Last year(?= )| This week(?= )| Sometimes(?= )| Beforehand(?= )| Ever since(?= )| Earlier(?= )| Presently(?= )| At present(?= )| Simultaneously(?= )| Subsequently(?= )| Suddenly(?= )| Throughout(?= )| Today(?= )| Every day(?= )| Yesterday(?= )| This year(?= )| Never(?= )| So far(?= )| Since then(?= )| This time(?= )| At the time(?= )| By the time(?= )| At the moment(?= )| Last night(?= )| Currently(?= )| Recently(?= )| Initially(?= )| Early(?= )| Earlier this year(?= )| Ever(?= )| Already(?= )| Recent(?= )| As long as(?= )| As soon as(?= )| Always(?= )| For years(?= )| Whenever(?= )| Every time(?= )| Nowadays(?= )| By the end(?= )| Originally(?= )| In the first time(?= )| These days(?= )| Often(?= )| At first(?= )| Eventually(?= )| Shortly(?= )| Shortly after(?= )| Right now(?= )| At a time(?= )| In recent years(?= )| For now(?= )| Whilst(?= )| The next day(?= )| Over the years(?= )| Over the past(?= )| More recently(?= )| In August(?= )| In January(?= )| In February(?= )| In March(?= )| In April(?= )| In May(?= )| In June(?= )| In July(?= )| In September(?= )| In October(?= )| In November(?= )| In December(?= )\n",
            "el:cReason\n",
            "\t In account of this(?= )| For (?:|this|that|a) reason(?= )| In connection with(?= )| In this connection(?= )| Therefore(?= )| Because of that(?= )| Because(?= )| Thereby(?= )| Thus(?= )| Hence(?= )| Since(?= )| That's why(?= )| That is why(?= )| This is why(?= )| Thanks to(?= )| The reason(?= )| Because of(?= )| This is because(?= )| Due to(?= )\n",
            "el:cResult\n",
            "\t In consequence(?= )| As a consequence(?= )| As a result(?= )| Consequently(?= )| By such means(?= )| The result(?= )| So when(?= )| So(?= )| So why(?= )\n",
            "el:cCondition\n",
            "\t If [^\\.]+ then(?= )| If not(?= )| Even if(?= )| What if(?= )| In that case(?= )| In the case(?= )| In the case of(?= )| In that event(?= )| Then(?= )| Under the circumstances(?= )| Otherwise(?= )| Whether(?= )| And if(?= )| If so(?= )| Given the circumstances(?= )| Given that(?= )| Given the(?= )\n",
            "el:cSequence\n",
            "\t The next(?= )| Firstly(?= )| Secondly(?= )| Thirdly(?= )| First(?= )| The first(?= )| For a start(?= )| Second(?= )| The second(?= )| Third(?= )| The third(?= )| Forth(?= )| Last(?= )| My next point is(?= )| Another(?= )| On another occasion(?= )| First of all(?= )| The following(?= )| The latter(?= )| At last(?= )| Briefly(?= )| To sum up(?= )| In conclusion(?= )| In short(?= )| To resume(?= )| Lastly(?= )| Last of all(?= )| In the end(?= )| Finally(?= )| At the end of the day(?= )| Last but not least(?= )| At the end(?= )| Overall(?= )| It's all we know(?= )| It is all we know(?= )| Ultimately(?= )\n",
            "el:cAddition\n",
            "\t Also(?= )| Moreover(?= )| In addition(?= )| Besides(?= )| Furthermore(?= )| Apart from(?= )| Except for(?= )| As well(?= )| At the same time(?= )| On top of that(?= )| Above all(?= )| Again(?= )| After all(?= )| And then(?= )| And when(?= )| And while(?= )| And even(?= )| And why(?= )| Not only(?= )| Not just(?= )| Further(?= )| A further(?= )| By the way(?= )| Especially(?= )| In addition to(?= )| Along with(?= )| What's more(?= )| What is more(?= )| To add(?= )| Just to add(?= )| Even more(?= )\n",
            "el:cPurpose\n",
            "\t For this purpose(?= )| For that purpose(?= )| For the purpose(?= )| For purpose(?= )| With this in mind(?= )| With this in view(?= )| With this in(?= )| In order(?= )| So that(?= )\n",
            "el:cConcession\n",
            "\t Nevertheless(?= )| Nonetheless(?= )| Despite(?= )| Yet(?= )| And yet(?= )| Still ,(?= )| Thought(?= )| However(?= )| Even so(?= )| All the same(?= )| Although(?= )| In spite of(?= )| At least(?= )| Despite the fact(?= )| Even though(?= )| No matter(?= )| No matter how(?= )| Never mind(?= )| In any case(?= )\n",
            "el:cComparison\n",
            "\t Likewise(?= )| In the same way(?= )| Similarly(?= )| In a different way(?= )| As if(?= )| As it were(?= )| As though(?= )| Compared with(?= )| Similar(?= )| Unlike(?= )| Differing from(?= )| Dissimilar(?= )| By the same token(?= )| Equally(?= )| Not that(?= )| So if(?= )\n",
            "el:cContrast\n",
            "\t On the other(?= )| On the one(?= )| Conversely(?= )| In contrast(?= )| By contrast(?= )| Whereas(?= )| But(?= )| On the contrary(?= )| Yes, but(?= )| But despite(?= )| Opposite(?= )| Opposing(?= )| Contrary to(?= )| Alternatively(?= )| How about(?= )| Nor(?= )| Instead(?= )| Then again,(?= )| Rather(?= )| Rather than(?= )| But then(?= )| But since(?= )| But even(?= )| But just(?= )| But while(?= )| Instead of(?= )| But now(?= )| But not(?= )| But after(?= )| But according to(?= )| But why(?= )| On the other side(?= )| On the other hand(?= )| Except(?= )| But when it comes(?= )| Against(?= )| Why not(?= )| Not so(?= )| That is not(?= )\n",
            "el:cClarification\n",
            "\t That is(?= )| In other words(?= )| To put it in another way(?= )| For instance(?= )| For example(?= )| To illustrate(?= )| To be precise(?= )| More especially(?= )| To give just an example of(?= )| Specifically(?= )| In particular(?= )| One example(?= )| As in the case of(?= )| Proof is found in(?= )| This means(?= )| That means(?= )| It means(?= )| It's about(?= )| It is about(?= )| This is about(?= )| That is about(?= )| The idea is(?= )\n",
            "el:cReference\n",
            "\t As has been noted(?= )| Whichever(?= )| Whoever(?= )| Whomever(?= )| Wherever(?= )| Whatever(?= )| According to(?= )| As I was saying(?= )| To get back to that point(?= )| As I have said(?= )| As I have noted(?= )| As for(?= )| They say(?= )| When it comes to(?= )| When it comes down to(?= )| When it comes right down to(?= )\n",
            "el:cViewpoint\n",
            "\t Luckily(?= )| Unfortunately(?= )| Absurdly(?= )| Alas(?= )| Astonishingly(?= )| Characteristically(?= )| Coincidentally(?= )| Conveniently(?= )| Curiously(?= )| Foolishly(?= )| Fortunately(?= )| Happily(?= )| Incredibly(?= )| Interestingly(?= )| Ironically(?= )| Mercifully(?= )| Miraculously(?= )| Mistakenly(?= )| Mysteriously(?= )| Naturally(?= )| Oddly(?= )| Paradoxically(?= )| Predictably(?= )| Remarkably(?= )| Regrettably(?= )| Sadly(?= )| Significantly(?= )| Strangely(?= )| Surprisingly(?= )| Unsurprisingly(?= )| To my surprise(?= )| Hopefully(?= )| Typically(?= )| Unbelievably(?= )| Understandably(?= )| Unexpectedly(?= )| Unhappily(?= )| Unnecessarily(?= )| Unwisely(?= )| Wisely(?= )| Somehow(?= )| To my delight(?= )| To my distress(?= )| I wish(?= )| For me(?= )| No wonder(?= )| Thankfully(?= )\n",
            "el:cPossibility\n",
            "\t Actually(?= )| Definitely(?= )| Presumably(?= )| Perhaps(?= )| And perhaps(?= )| But perhaps(?= )| Or perhaps(?= )| Possibly(?= )| Seemingly(?= )| It seems(?= )| Probably(?= )| Or maybe(?= )| Maybe(?= )| I'm sure(?= )| surely(?= )| And of course(?= )| of course(?= )| certainly(?= )| Undoubtedly(?= )| No doubt(?= )| Doubtless(?= )| I doubt(?= )| Apparently(?= )| It appears(?= )| Obviously(?= )| Clearly(?= )| Visibly(?= )| It is clear(?= )| Conceivably(?= )| Manifestly(?= )| Plainly(?= )| Potentially(?= )| Evidently(?= )| In fact(?= )| In practice(?= )| In reality(?= )| In theory(?= )| Officially(?= )| Unofficially(?= )| Allegedly(?= )| Apparently(?= )| Nominally(?= )| Ostensibly(?= )| Supposedly(?= )| theoretically(?= )| Unmistakably(?= )| It appeared(?= )| It appears(?= )| Obviously(?= )| It may be(?= )| I suppose(?= )| Generally(?= )| Normally(?= )| It seemed(?= )| Basically(?= )| Strictly(?= )\n",
            "el:cAttitude\n",
            "\t Frankly(?= )| Honestly(?= )| To be honest(?= )| In all my honesty(?= )| To tell the truth(?= )| To tell you the truth(?= )| The truth is(?= )| In truth(?= )| You see(?= )| In fairness(?= )| In retrospect(?= )| On reflection(?= )| Personally(?= )| To my mind(?= )| Believe me(?= )| Admittedly(?= )| I think(?= )| And I think(?= )| But I think(?= )| I do think(?= )| So I think(?= )| I don't think(?= )| But I don't think(?= )| Do you think(?= )| Do you think that(?= )| You think(?= )| You might think(?= )| What do you think(?= )| We also think that(?= )| We think(?= )| To my mind(?= )| In my view(?= )| In my opinion(?= )| I believe(?= )| I agree(?= )| Just(?= )| I hope(?= )| We hope(?= )| I don't know(?= )| I feel(?= )| I believe that(?= )| I believe(?= )| You know(?= )| I think that(?= )| I find(?= )| I guess(?= )| I wonder(?= )| We believe(?= )| I found(?= )| I'm not sure(?= )| I am sure(?= )| I suspect(?= )| I hate(?= )\n",
            "el:cEmphasis\n",
            "\t The only question(?= )| The only thing(?= )| The answer is(?= )| The question is(?= )| It is important(?= )| The fact is(?= )| The good news(?= )| The most important(?= )\n",
            "el:ca\n",
            "\t In account of this(?= )| For (?:|this|that|a) reason(?= )| In connection with(?= )| In this connection(?= )| Therefore(?= )| Because of that(?= )| Because(?= )| Thereby(?= )| Thus(?= )| Hence(?= )| Since(?= )| That's why(?= )| That is why(?= )| This is why(?= )| Thanks to(?= )| The reason(?= )| Because of(?= )| This is because(?= )| Due to(?= )| In consequence(?= )| As a consequence(?= )| As a result(?= )| Consequently(?= )| By such means(?= )| The result(?= )| So when(?= )| So(?= )| So why(?= )| If [^\\.]+ then(?= )| If not(?= )| Even if(?= )| What if(?= )| In that case(?= )| In the case(?= )| In the case of(?= )| In that event(?= )| Then(?= )| Under the circumstances(?= )| Otherwise(?= )| Whether(?= )| And if(?= )| If so(?= )| Given the circumstances(?= )| Given that(?= )| Given the(?= )| For this purpose(?= )| For that purpose(?= )| For the purpose(?= )| For purpose(?= )| With this in mind(?= )| With this in view(?= )| With this in(?= )| In order(?= )| So that(?= )| Nevertheless(?= )| Nonetheless(?= )| Despite(?= )| Yet(?= )| And yet(?= )| Still ,(?= )| Thought(?= )| However(?= )| Even so(?= )| All the same(?= )| Although(?= )| In spite of(?= )| At least(?= )| Despite the fact(?= )| Even though(?= )| No matter(?= )| No matter how(?= )| Never mind(?= )| In any case(?= )| Likewise(?= )| In the same way(?= )| Similarly(?= )| In a different way(?= )| As if(?= )| As it were(?= )| As though(?= )| Compared with(?= )| Similar(?= )| Unlike(?= )| Differing from(?= )| Dissimilar(?= )| By the same token(?= )| Equally(?= )| Not that(?= )| So if(?= )| On the other(?= )| On the one(?= )| Conversely(?= )| In contrast(?= )| By contrast(?= )| Whereas(?= )| But(?= )| On the contrary(?= )| Yes, but(?= )| But despite(?= )| Opposite(?= )| Opposing(?= )| Contrary to(?= )| Alternatively(?= )| How about(?= )| Nor(?= )| Instead(?= )| Then again,(?= )| Rather(?= )| Rather than(?= )| But then(?= )| But since(?= )| But even(?= )| But just(?= )| But while(?= )| Instead of(?= )| But now(?= )| But not(?= )| But after(?= )| But according to(?= )| But why(?= )| On the other side(?= )| On the other hand(?= )| Except(?= )| But when it comes(?= )| Against(?= )| Why not(?= )| Not so(?= )| That is not(?= )| That is(?= )| In other words(?= )| To put it in another way(?= )| For instance(?= )| For example(?= )| To illustrate(?= )| To be precise(?= )| More especially(?= )| To give just an example of(?= )| Specifically(?= )| In particular(?= )| One example(?= )| As in the case of(?= )| Proof is found in(?= )| This means(?= )| That means(?= )| It means(?= )| It's about(?= )| It is about(?= )| This is about(?= )| That is about(?= )| The idea is(?= )| As has been noted(?= )| Whichever(?= )| Whoever(?= )| Whomever(?= )| Wherever(?= )| Whatever(?= )| According to(?= )| As I was saying(?= )| To get back to that point(?= )| As I have said(?= )| As I have noted(?= )| As for(?= )| They say(?= )| When it comes to(?= )| When it comes down to(?= )| When it comes right down to(?= )\n",
            "el:co\n",
            "\t Then(?= )| Afterwards(?= )| Later(?= )| Just than(?= )| At the same time(?= )| Before that(?= )| Hitherto(?= )| Previously(?= )| At once(?= )| Thereupon(?= )| Straightaway(?= )| Soon(?= )| After a while(?= )| Meanwhile(?= )| During(?= )| In the meantime(?= )| All that time(?= )| At this moment(?= )| Once(?= )| After(?= )| When(?= )| While(?= )| Until(?= )| Previously(?= )| Before(?= )| Up to that point(?= )| At this point(?= )| Here(?= )| Now(?= )| And now(?= )| An hour later(?= )| That morning(?= )| Last time(?= )| Last week(?= )| Last year(?= )| This week(?= )| Sometimes(?= )| Beforehand(?= )| Ever since(?= )| Earlier(?= )| Presently(?= )| At present(?= )| Simultaneously(?= )| Subsequently(?= )| Suddenly(?= )| Throughout(?= )| Today(?= )| Every day(?= )| Yesterday(?= )| This year(?= )| Never(?= )| So far(?= )| Since then(?= )| This time(?= )| At the time(?= )| By the time(?= )| At the moment(?= )| Last night(?= )| Currently(?= )| Recently(?= )| Initially(?= )| Early(?= )| Earlier this year(?= )| Ever(?= )| Already(?= )| Recent(?= )| As long as(?= )| As soon as(?= )| Always(?= )| For years(?= )| Whenever(?= )| Every time(?= )| Nowadays(?= )| By the end(?= )| Originally(?= )| In the first time(?= )| These days(?= )| Often(?= )| At first(?= )| Eventually(?= )| Shortly(?= )| Shortly after(?= )| Right now(?= )| At a time(?= )| In recent years(?= )| For now(?= )| Whilst(?= )| The next day(?= )| Over the years(?= )| Over the past(?= )| More recently(?= )| In August(?= )| In January(?= )| In February(?= )| In March(?= )| In April(?= )| In May(?= )| In June(?= )| In July(?= )| In September(?= )| In October(?= )| In November(?= )| In December(?= )| The next(?= )| Firstly(?= )| Secondly(?= )| Thirdly(?= )| First(?= )| The first(?= )| For a start(?= )| Second(?= )| The second(?= )| Third(?= )| The third(?= )| Forth(?= )| Last(?= )| My next point is(?= )| Another(?= )| On another occasion(?= )| First of all(?= )| The following(?= )| The latter(?= )| At last(?= )| Briefly(?= )| To sum up(?= )| In conclusion(?= )| In short(?= )| To resume(?= )| Lastly(?= )| Last of all(?= )| In the end(?= )| Finally(?= )| At the end of the day(?= )| Last but not least(?= )| At the end(?= )| Overall(?= )| It's all we know(?= )| It is all we know(?= )| Ultimately(?= )| Also(?= )| Moreover(?= )| In addition(?= )| Besides(?= )| Furthermore(?= )| Apart from(?= )| Except for(?= )| As well(?= )| At the same time(?= )| On top of that(?= )| Above all(?= )| Again(?= )| After all(?= )| And then(?= )| And when(?= )| And while(?= )| And even(?= )| And why(?= )| Not only(?= )| Not just(?= )| Further(?= )| A further(?= )| By the way(?= )| Especially(?= )| In addition to(?= )| Along with(?= )| What's more(?= )| What is more(?= )| To add(?= )| Just to add(?= )| Even more(?= )\n",
            "el:cs\n",
            "\t Luckily(?= )| Unfortunately(?= )| Absurdly(?= )| Alas(?= )| Astonishingly(?= )| Characteristically(?= )| Coincidentally(?= )| Conveniently(?= )| Curiously(?= )| Foolishly(?= )| Fortunately(?= )| Happily(?= )| Incredibly(?= )| Interestingly(?= )| Ironically(?= )| Mercifully(?= )| Miraculously(?= )| Mistakenly(?= )| Mysteriously(?= )| Naturally(?= )| Oddly(?= )| Paradoxically(?= )| Predictably(?= )| Remarkably(?= )| Regrettably(?= )| Sadly(?= )| Significantly(?= )| Strangely(?= )| Surprisingly(?= )| Unsurprisingly(?= )| To my surprise(?= )| Hopefully(?= )| Typically(?= )| Unbelievably(?= )| Understandably(?= )| Unexpectedly(?= )| Unhappily(?= )| Unnecessarily(?= )| Unwisely(?= )| Wisely(?= )| Somehow(?= )| To my delight(?= )| To my distress(?= )| I wish(?= )| For me(?= )| No wonder(?= )| Thankfully(?= )| Actually(?= )| Definitely(?= )| Presumably(?= )| Perhaps(?= )| And perhaps(?= )| But perhaps(?= )| Or perhaps(?= )| Possibly(?= )| Seemingly(?= )| It seems(?= )| Probably(?= )| Or maybe(?= )| Maybe(?= )| I'm sure(?= )| surely(?= )| And of course(?= )| of course(?= )| certainly(?= )| Undoubtedly(?= )| No doubt(?= )| Doubtless(?= )| I doubt(?= )| Apparently(?= )| It appears(?= )| Obviously(?= )| Clearly(?= )| Visibly(?= )| It is clear(?= )| Conceivably(?= )| Manifestly(?= )| Plainly(?= )| Potentially(?= )| Evidently(?= )| In fact(?= )| In practice(?= )| In reality(?= )| In theory(?= )| Officially(?= )| Unofficially(?= )| Allegedly(?= )| Apparently(?= )| Nominally(?= )| Ostensibly(?= )| Supposedly(?= )| theoretically(?= )| Unmistakably(?= )| It appeared(?= )| It appears(?= )| Obviously(?= )| It may be(?= )| I suppose(?= )| Generally(?= )| Normally(?= )| It seemed(?= )| Basically(?= )| Strictly(?= )| Frankly(?= )| Honestly(?= )| To be honest(?= )| In all my honesty(?= )| To tell the truth(?= )| To tell you the truth(?= )| The truth is(?= )| In truth(?= )| You see(?= )| In fairness(?= )| In retrospect(?= )| On reflection(?= )| Personally(?= )| To my mind(?= )| Believe me(?= )| Admittedly(?= )| I think(?= )| And I think(?= )| But I think(?= )| I do think(?= )| So I think(?= )| I don't think(?= )| But I don't think(?= )| Do you think(?= )| Do you think that(?= )| You think(?= )| You might think(?= )| What do you think(?= )| We also think that(?= )| We think(?= )| To my mind(?= )| In my view(?= )| In my opinion(?= )| I believe(?= )| I agree(?= )| Just(?= )| I hope(?= )| We hope(?= )| I don't know(?= )| I feel(?= )| I believe that(?= )| I believe(?= )| You know(?= )| I think that(?= )| I find(?= )| I guess(?= )| I wonder(?= )| We believe(?= )| I found(?= )| I'm not sure(?= )| I am sure(?= )| I suspect(?= )| I hate(?= )| The only question(?= )| The only thing(?= )| The answer is(?= )| The question is(?= )| It is important(?= )| The fact is(?= )| The good news(?= )| The most important(?= )\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "\tgnorm=10730\tgrule=17764\tgnonorm=5299\n",
            "3252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvzf cord19_100k_stat.txt.tgz cord19_100k_stat.txt\n",
        "!tar cvzf cord19_100k_stat.txt_n.txt.tgz cord19_100k_stat.txt_n.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdBuYvJFhwlv",
        "outputId": "27061ebf-d34e-4ff4-8f09-b1e36d74286f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cord19_100k_stat.txt\n",
            "cord19_100k_stat.txt_n.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wc cord19_100k_stat.txt\n",
        "!wc cord19_100k_stat.txt_n.txt"
      ],
      "metadata": {
        "id": "8EyQYF4HfrGa",
        "outputId": "ac6de3e2-157e-4b31-9fbd-dd8e0d4ae512",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   118261  18297708 100704406 cord19_100k_stat.txt\n",
            "  118261  5579570 26373161 cord19_100k_stat.txt_n.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=10 cord19_100k_stat.txt_n.txt"
      ],
      "metadata": {
        "id": "DoFXuVL3kF8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5819f036-72ce-4cde-cf2c-c33def771f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc_n\tsec_n\tsmp_n\tsmp_l\tdct_l\tnC_a\tnC_o\tnC_s\twdC_a\twdC_o\twdC_s\tccTime\tccReason\tccResult\tccCondition\tccSequence\tccAddition\tccPurpose\tccConcession\tccComparison\tccContrast\tccClarification\tccReference\tccViewpoint\tccPossibility\tccAttitude\tccEmphasis\twcTime\twcReason\twcResult\twcCondition\twcSequence\twcAddition\twcPurpose\twcConcession\twcComparison\twcContrast\twcClarification\twcReference\twcViewpoint\twcPossibility\twcAttitude\twcEmphasis\tnKN_R\tnMA_R\tnEV_R\tnKN_MI\tnMA_MI\tnEV_MI\tSCatNorm\tSSectNameNorm\twKN_RndWords\twMA_RndWords\twEV_RndWords\twKN_MIWords\twMA_MIWords\twEV_MIWords\n",
            "2\t1\t1\t100\t72\t1\t0\t0\t thus\t\t\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\t thus\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t1\t2\t3\t1\t0\t0\tMAP\tIntroduction\t stem\t result  evidence\t emergency  extreme  strength\t epidemiology\t\t\n",
            "2\t1\t2\t100\t67\t1\t2\t1\t except\t during~ along with\t predictably\t1\t0\t0\t0\t0\t1\t0\t0\t0\t1\t0\t0\t1\t0\t0\t0\t during\t\t\t\t\t along with\t\t\t\t except\t\t\t predictably\t\t\t\t0\t1\t2\t0\t0\t0\tMAP\tIntroduction\t\t predict\t ability  preparedness\t\t\t\n",
            "2\t1\t3\t100\t74\t1\t1\t0\t thus\t while\t\t1\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t while\t thus\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t0\t0\t6\t0\t0\t1\tMAP\tIntroduction\t\t\t extreme  anxiety  distress  saving  crisis  ability\t\t\t saving\n",
            "2\t1\t4\t100\t62\t1\t2\t1\t thus\t while~ also\t clearly\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t while\t thus\t\t\t\t also\t\t\t\t\t\t\t\t clearly\t\t\t1\t0\t2\t0\t0\t0\tMAP\tIntroduction\t mortality\t\t distress  morbidity\t\t\t\n",
            "2\t1\t5\t100\t67\t1\t2\t0\t due to\t also~ in March\t\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t in March\t due to\t\t\t\t also\t\t\t\t\t\t\t\t\t\t\t0\t0\t5\t1\t0\t1\tMAP\tIntroduction\t\t\t emergency  inability  emergency  emergency  emergency\t chemotherapy\t\t inability\n",
            "2\t1\t6\t151\t89\t4\t1\t0\t but~ however~ thus~ rather\t while\t\t1\t1\t0\t0\t0\t0\t0\t1\t0\t2\t0\t0\t0\t0\t0\t0\t while\t thus\t\t\t\t\t\t however\t\t but~ rather\t\t\t\t\t\t\t4\t6\t3\t0\t0\t0\tMAP\tIntroduction\t symptom  epidemiological  intervention  mortality\t author  cause  cause  demonstrate  argue  predict\t crisis  risk  morbidity\t\t\t\n",
            "2\t2\t1\t90\t58\t1\t2\t0\t due to\t currently~ also\t\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t currently\t due to\t\t\t\t also\t\t\t\t\t\t\t\t\t\t\t1\t0\t4\t0\t0\t0\tRULE\tBackground\t mortality\t\t epidemic  epidemic  risk  outbreak\t\t\t\n",
            "2\t3\t1\t190\t112\t2\t5\t0\t whether~ rather\t in addition~ while~ further~ already~ further\t\t2\t0\t0\t1\t0\t3\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t while~ already\t\t\t whether\t\t in addition~ further~ further\t\t\t\t rather\t\t\t\t\t\t\t1\t4\t9\t0\t0\t1\tNONORM\thealth systems preparedness to handle the covid epidemic ::: a proposed framework\t intervention\t consideration  consideration  assessment  focus\t preparedness  epidemic  epidemic  preparedness  achieve  preparedness  ability  achieve  preparedness\t\t\t caution\n",
            "2\t4\t1\t100\t63\t2\t0\t0\t thus~ as for\t\t\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t\t thus\t\t\t\t\t\t\t\t\t\t as for\t\t\t\t\t3\t3\t5\t1\t1\t2\tNONORM\tprotecting the right to essential health services for non-covid- medical conditions ::: a proposed framework\t reproductive  treatment  reproductive\t emphasis  means  consideration\t ability  emergency  tuberculosis  cancer  tuberculosis\t chemotherapy\t means\t tuberculosis  tuberculosis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=74 cord19_100k_stat.txt >cord19_1ks_stat.txt"
      ],
      "metadata": {
        "id": "76mGwVletwnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OXml2Stat = clXml2Stat('/content/cord19.lems', output_file = '/content/cord19_stat.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0HQS920m06u",
        "outputId": "f4720340-3873-4bf9-8a68-d8409e831d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file read into memory\n",
            "el:Introduction\n",
            "\tintroduction\n",
            "el:Statements\n",
            "\tauthor|contributions|conflict|interest|declaration|competing|funding|availability|statement|ethics|credit|authorship|contribution|publisher's|conflicts|publisher’s|note|ethical|approval|interests|disclosures|sources|financial|support|sponsorship|considerations|disclosure|acknowledgments|contributors|consent|sharing|guarantor|declarations|additional|provenance|peer|source|authors'|participate|acknowledgements|disclaimer|informed\n",
            "el:Methods\n",
            "\tmethod|statistical analysis|materials|data analysis|data collection|study design|participants|participants|study design|statistical analyses|statistical analyses|study population|study design and participants|analysis|methodology|measures|search strategy|data extraction|design|procedure|study population|procedure|procedures|setting|data sources|statistics|statistics|data analysis|eligibility criteria|study design and participants|measures|statistical methods|study selection|patients|inclusion and exclusion criteria|method|material and methods|study design and setting|sample size|data|patients|study setting|statistical analysis|cell culture|recruitment|data source|sample|design|outcome measures|study design and population|intervention|statistical methods|study participants|search strategy|inclusion criteria|animals|analysis|data extraction|quality assessment|outcomes|covariates|definitions|variables|procedures|methodology|materials\n",
            "el:Conclusion\n",
            "\tconclusion|concluding|summary|recommendations|future\n",
            "el:Discussion\n",
            "\tdiscussion|limitation|limitations|strengths|findings|implications|discussions\n",
            "el:Presentation\n",
            "\tan event is serious|treatment|outcome and follow-up|follow-up|application\n",
            "el:Background\n",
            "\tbackground|literature|progress .* research|research .* progress|review|related|overview\n",
            "el:Results\n",
            "\tresult\n",
            "el:Remove\n",
            "\treference\n",
            "el:KEY_N\n",
            "\t agent| antigen| variability| nucleic| acid| purify| method| immunogenicity| solution| infusion| vaccination| intervention| molecular| biology| genetic| bacterial| culture| supernatant| enzyme| genome| database| epidemiological| secrete| vitro| assay| synthetic| dose| fusion| subtype| stem| hospitalisation| lipid| mers| systemic| treatment| statistical| palliative| ratio| mortality| diagnosis| comorbidities| simulate| standardized| resolution| pneumonia| syndrome| sars| symptom| pulmonary| latent| datasets| classification| criterion| opacity| pattern| lobe| pleural| effusion| confidence| fibrosis| linear| clinically| anesthesia| ventilation| avoidance| oxygen| emission| validate| algorithm| evolution| experimental| reproductive| behavioral| dataset| threshold| computational| trigger| predictor| physician| throat| storm| derivative| suppress| longitudinal| error| coefficient| covid| categorize| neurological| saturation| cerebral| renal| stimulus| mri| venous| thrombotic| hypoxia| inpatient| data| sampling| monitoring| telehealth| duplicate| outpatient| symptomatic| determination| clinician| biobank| cytokine| endothelial| tissue| nasopharyngeal| prescribe| steroid| remdesivir| stethoscope| troponin| placebo| nucleotide| species| deletion| comparative| transmembrane| peptide| interferon| transcription| rdrp| furin| nucleocapsid| transcriptional| nucleus| arterial| epithelial| invasive| substrate| dioxide| bioactive| prevalence| secretion| heterogeneity| sequential| dilution| incubate| protease| cascade| clot| tomography| chloroquine| predispose| fibrinogen| allele| corona| adjusted| vivo| helicase| hydrogen| nitrogen| absorption| radiology| autoimmune| ultrasound| cerebrovascular| urine| neurologic| radiation| oncology| plaque| muscle| conditional| forecasting| neural| sterile| aggregation| biomarkers| antimicrobial| murine| oxide| emergent| bead| addressability| sstis| aftereffect| reimagined| speeding| medicalisation| pharmacoenosis| pfps| mdas| aibds| wholistic| ieq| partiality| pandemicity| deprioritized| coerciveness| dsms| mitigated| heor| medicalization| grvs| kdts| pwuds| surveilled| aecopds| chronification| diabetologists| preeminence| visioli| transplantable| palliate| nonnegligible| pervasively| cyclicality| hypovolaemic| compositionally| computerization| mrfs| restrictively| scfs| mplp| chemoreceptive| llin| rvis| dimant| lymphodepletion| uptrend| reinsertion| seclude| crippling| vops| consummation| vesus| ptcp| wfs| nstis| paucibacillary| ssti| cfsgs| undetectability| spts| exopd| hmgcs| toxicities| motoneuron| srms| aminoacylation| vegfs| pdos| smgt| spirally| eracr| prokinetics| antacid| peptidomics| mobilan| mϕ| vams| kawacovid| contributive| underdetection| nosocomially| reexposure| txndc| predisposing| toxicologically| cish| delafloxacin| sublevels| dhapq| exportable| transudation| ejm| dyscrasia| misexpression| hyperlactatemia| mhciilow| xyp| iupd| tsas| chemosensing| tepid| accurateness| lvnc| noncaseating| nops| micturition| eruptive| parabiont| catc| progressed| nonbiologic| chromotripsis| thrombopenia| dermatome| intraglomerular| medullar| desaturate| hyporeactive| pathogeneses| oxog| noncardiac| unvaried| alri| azacitidine| hyphenated| hbecs| hypermutated| hypomorphs| oprelvekin| microcolonies| fibrinolytics| mesotherapy| codiv| dysgonomonas| rndd| poikilosis| apposite| distill| bups| estimands| autopoietic| pubquiz| channelize| subtopics| pscpm| ehi| nitags| hierarchized| proxemic| interpretivist| sbme| prelude| technicality| wwa| kinds| individuate| yardstick| geomorphosites| emrn| topicality| renvoi| idiosyncrasy| forewarn| phenomenologist| gdphrwkd| largescale| jhdi| dohad| crwms| chims| countercyclical| faultline| hygienically| rationing| fbt| huhep| bcpap| esbs\n",
            "el:M_ARG\n",
            "\t study| consistency| say| normally| focus| apparent| speak| reason| motivation| result| cause| demonstrate| analysis| publish| predict| consist| assume| formulate| academic| describe| consideration| uncertain| author| validation| limitation| finding| reflect| inherent| assessment| relevant| revise| theory| confirm| assess| recommend| opinion| advocate| evidence| believe| summarize| conclude| definition| interesting| prediction| sense| nearly| relevance| think| certainly| reasonable| actually| emphasize| analogue| hypothesis| reject| motivate| publication| manuscript| tell| paradigm| conclusion| scholar| communicate| presentation| explanation| contrary| generalize| analyse| motif| consequently| secondly| suggestion| argue| narrative| assumption| peer| disagree| manifest| emphasis| rationale| methodological| means| misattribute| expectable| overlooked| nonrelated| unpredicted| undisputable| seldomly| unsound| arbitrariness| unheard| observationally| parenthetically| ignorable| confirmative| limitedly| overemphasis| contemplated| moot| unproblematic| hypothetic| rebuttal| oddity| enquiring| serendipity| doubtless| incongruous| restate| mindsponge| reemphasize| supposition| relativize\n",
            "el:EVAL\n",
            "\t expertise| risk| disruption| well| outbreak| ebola| notable| achieve| epidemic| preparedness| helpful| threat| crisis| harmful| usefulness| effectiveness| adverse| aware| able| facilitate| ability| easy| unable| preserve| stable| enhance| prevent| emergency| failure| respond| unprecedented| difficulty| crucial| cancer| negative| tumor| developed| pain| beneficial| morbidity| feasible| struggle| mitigate| severe| exposure| distress| severity| contamination| compromise| fit| warning| alert| stroke| decline| tuberculosis| professional| collapse| sick| justice| impair| anxiety| appropriate| unclear| ill| pathology| hemorrhage| experienced| suspected| canada| strength| extreme| reliable| systematically| hazard| referral| depression| deficit| inability| optimize| abuse| poverty| achievement| contaminated| precise| malignant| stability| powerful| ease| fail| accessibility| excessive| inequality| pilot| stabilize| imbalance| liberty| incomplete| discomfort| arthritis| pesticide| strategic| contagious| weakness| intelligent| stressful| saving| suicidal| abnormal| unrealized| uncompromising| calamitous| underfeed| deleteriously| devasting| frustratingly| cataclysmic| expediently| flimsy| disruptions| uncontained| undersupply| deteriorating| remediating| hypervigilant| misbehave| hobble| broadness| insanity| accomplice| promptness| pathetic| overutilization| inadvisable| precociously| ultrarapid| dysfunctioning| painstakingly| admirably| burdens| venerable| gracefully| favoritism| infallible| pressingly| fallible| sparseness| prepotency| incapacitation| undermet| slander| overconfident\n",
            "el:KEY_N\n",
            "\t inactivate| purify| subcutaneous| nasal| immunogenicity| biology| microorganism| recombinant| mutant| codon| supernatant| coli| sequenced| biotechnology| secrete| aureus| synthetic| seed| fda| polymer| epitope| conformational| unstable| monoclonal| subtype| adjuvant| stimulation| hospitalisation| soluble| cholesterol| therapeutics| chemotherapy| numerical| univariate| colorectal| cytotoxic| simulate| multivariable| biomedical| polymerase| abdominal| radiologist| opacity| lobe| diameter| pleural| effusion| covariates| monocyte| dyspnea| radiological| anesthesia| avoidance| intravenous| oxygenation| blockade| extension| hybrid| normalize| axis| classifier| bone| throat| myalgia| derivative| behavioural| compatible| ferritin| ldh| prophylactic| cerebral| creatinine| stimulus| limb| artery| venous| prophylaxis| thrombotic| hypoxemia| transplantation| pubmed| telehealth| duplicate| medicaid| mellitus| orthopaedic| postoperative| surgeon| biobank| genotype| proinflammatory| magnetic| permeability| olfactory| infarction| steroid| spontaneous| remdesivir| epidemiology| stigma| mouth| stethoscope| inject| fluctuation| dominant| resistant| ambient| biomarker| troponin| transplant| azithromycin| evolutionary| species| deletion| depletion| sequencing| transmembrane| interferon| phylogenetic| rdrp| subunits| lineage| furin| nucleocapsid| detectable| transcriptional| translational| cutoff| residual| disseminate| alpha| beta| subunit| nucleus| repair| intracellular| arterial| phosphorylation| diabetic| adhesion| extracellular| invasive| fungus| dioxide| organic| bioactive| microbial| postpartum| serological| secretion| sequential| amplification| reagent| sputum| plasmid| dilution| vero| incubate| elute| cascade| clot| thrombin| tomography| lymphopenia| lupus| anticoagulant| chloroquine| predispose| neuron| microvascular| fibrinogen| likert| polymorphism| allele| neutralization| electron| microscopy| serine| catalytic| prostate| intestinal| epithelium| quantification| patients| prescription| eosinophil| binary| adjusted| composite| helicase| preclinical| unwind| biochemical| hydrogen| nitrogen| rnas| suppression| pharmacological| absorption| interference| endoscopic| radiology| respirator| prevalent| ultrasound| cerebrovascular| urine| transient| neurologic| radiation| oncology| coagulopathy| alveolar| anticoagulation| immunoassay| plaque| muscle| conditional| volatility| exogenous| specification| paediatric| segmentation| neural| biopsy| array| novo| saliva| discrimination| developmental| sterile| filtration| aggregation| hormone| pathophysiology| oestrogen| progesterone| fungal| propensity| fetal| relaxation| mucosal| ion| sodium| oxidative| murine| disinfection| oxide| formula| capillary| optical| emergent| gradient| etiology| stressors| endogenous| precursor| metabolite| bead| sstis| paraviral| addressability| mitigator| exportable| unmonitored| counterweight| restrictively| lymphodepletion| cud| transplantable| huhep| pervasively| rndds| osteosarcopenia| pwuds| afce| aftereffect| semiology| uptrend| progressed| nocturia| ligature| compositionally| overcompensate| schistosome| spirally| alvo| formulaic| sdvs| deprioritized| deglutition| inchoate| phototoxic| phenoconversion| defensiveness| diagnosable| hypercatabolic| referable| decongest| ssti| srms| pandemicity| dsms| pqcs| cyclicality| capabilization| phenotoxicity| insonation| vesus| tlco| allostatine| transudation| nstis| hpaecs| nephrolithiasis| morf| microthromboses| rars| ltri| hyperresponsive| antacid| clots| prokinetics| mϕ| idcm| trqc| dyscrasia| hypovolaemic| mesotherapy| dhapq| autolysis| neuropraxia| chemotypes| congenitally| txndc| hepatoblastoma| microembolic| nrh| ivdd| peptidomics| predisposing| expd| intraglomerular| micturition| epiairway| unconditioned| isoprostane| dnmts| permissivity| extdna| uncharacterised| pretreat| placentitis| noncaseating| transgenics| selenos| anticoagulative| wwa| ghio| ssrnas| svneo| phototoxicity| obrb| edvs| caia| bronchodilation| cosmesis| edeine| inappetence| unresected| lymphovascular| immunoreaction| glycomes| tsas| wfs| pytri| hdts| endomitosis| zscan| thrombopenia| fibrinogenesis| tauopathy| pachynema| flatland| mitotically| aanat| vics| metabolizers| abluminal| chemoresistant| lel| promiscuously| extrarenal| entrypoint| estimands| rndd| uncaptured| heor| microallocation| haphazard| contributive| undercoverage| renvoi| imy| theoriaphobia| subtopics| scammer| monad| lvvos| gdphrwkd| disjuncture| pots| instantiations| inclusionary| clinimetric| nitags| riddor| denotation| pscpm| ocg| ehi| rpis| susars| feps| bups| lvms| socioculturally| phenomenologist| equiptt| determinative| torraco| reliabilism\n",
            "el:M_ARG\n",
            "\t consistency| normally| oppose| speak| publicly| formulate| categorical| statistics| inherent| conceptual| revise| ethics| confirmation| figs| infer| abstract| ignore| known| roughly| certainly| underestimate| analogue| exploratory| reject| correction| surprisingly| anonymous| tell| probable| cf| paradigm| scholar| consult| exploration| speculate| expectancy| obviously| analog| unrelated| motif| conversely| remarkable| reviewer| narrative| intellectual| disagree| argument| synthesize| structured| remarkably| considerably| thought| essentially| briefly| rationale| really| plausible| unless| solely| intrinsic| witness| methodological| implicate| reflection| discourse| justify| means| indubitable| unsound| measurably| ascertained| ignorable| enunciation| emphasized| overlooked| undisputable| foretell| epiphenomenon| warranty| disputed| nonrelated| susceptive| limitedly| trivialize| resounding| hearsay| adamantly| contemplated| misattribute| exceptions| dissonant| naively| acquiesce| unquestioned| enlightening| valuing| iconography| doubtless| disputable| indepth| untruthful| deemphasize| interpellation| incommensurable| warnings| misbelief| interpellate\n",
            "el:EVAL\n",
            "\t kill| excess| suspicion| misinformation| harmful| usefulness| virulence| preserve| malaria| danger| enhancement| careful| developed| toxic| malignancy| struggle| appropriately| heavily| edema| superior| consolidation| viable| obstacle| flexible| alert| precision| war| thank| tuberculosis| collapse| sick| justice| impair| flexibility| traumatic| poorly| delirium| carefully| hemorrhage| ischemic| impaired| defect| experienced| obstructive| safely| collaborative| systematically| necrosis| invasion| thrombosis| wound| deficit| disturbance| caution| inability| abuse| poverty| achievement| disrupt| properly| contaminated| homeless| efficiently| abundant| deleterious| lie| disadvantage| detrimental| parasite| malignant| powerful| dengue| accessibility| complaint| aggravate| allergic| exacerbation| attractive| stabilize| competence| viability| imbalance| liberty| diarrhea| incomplete| anomaly| illegal| untreated| discomfort| racism| complicate| competency| nervous| thrombus| arthritis| innovative| hinder| pesticide| reward| strategic| instability| psychotic| depressive| obese| weakness| rescue| anticancer| intelligent| autonomy| expensive| comfort| moral| saving| addiction| overdose| suicidal| freedom| hepatic| unrealized| remediable| relapses| undesirably| expediently| frustratingly| accomplice| weariness| inordinate| misdiagnosing| unwillingly| deathly| cataclysmic| hobble| lameness| echolalia| comorbidites| ineffectively| misprice| painstakingly| underprepared| deride| nihilistic| composure| outlandish| intratumor| dysimmunity| machiavellians| backpain| fickle| venerable| unnerving| excitingly| uninjured| magister| pneumonias| aberrancy| broadness| sacredness| infallible| resolute| obsessed| undemocratic| routinized| slander| murderer| calamitous| inadmissible| particularism| overblown| overconfident| burdens\n",
            "el:cTime\n",
            "\t Then(?= )| Afterwards(?= )| Later(?= )| Just than(?= )| At the same time(?= )| Before that(?= )| Hitherto(?= )| Previously(?= )| At once(?= )| Thereupon(?= )| Straightaway(?= )| Soon(?= )| After a while(?= )| Meanwhile(?= )| During(?= )| In the meantime(?= )| All that time(?= )| At this moment(?= )| Once(?= )| After(?= )| When(?= )| While(?= )| Until(?= )| Previously(?= )| Before(?= )| Up to that point(?= )| At this point(?= )| Here(?= )| Now(?= )| And now(?= )| An hour later(?= )| That morning(?= )| Last time(?= )| Last week(?= )| Last year(?= )| This week(?= )| Sometimes(?= )| Beforehand(?= )| Ever since(?= )| Earlier(?= )| Presently(?= )| At present(?= )| Simultaneously(?= )| Subsequently(?= )| Suddenly(?= )| Throughout(?= )| Today(?= )| Every day(?= )| Yesterday(?= )| This year(?= )| Never(?= )| So far(?= )| Since then(?= )| This time(?= )| At the time(?= )| By the time(?= )| At the moment(?= )| Last night(?= )| Currently(?= )| Recently(?= )| Initially(?= )| Early(?= )| Earlier this year(?= )| Ever(?= )| Already(?= )| Recent(?= )| As long as(?= )| As soon as(?= )| Always(?= )| For years(?= )| Whenever(?= )| Every time(?= )| Nowadays(?= )| By the end(?= )| Originally(?= )| In the first time(?= )| These days(?= )| Often(?= )| At first(?= )| Eventually(?= )| Shortly(?= )| Shortly after(?= )| Right now(?= )| At a time(?= )| In recent years(?= )| For now(?= )| Whilst(?= )| The next day(?= )| Over the years(?= )| Over the past(?= )| More recently(?= )| In August(?= )| In January(?= )| In February(?= )| In March(?= )| In April(?= )| In May(?= )| In June(?= )| In July(?= )| In September(?= )| In October(?= )| In November(?= )| In December(?= )\n",
            "el:cReason\n",
            "\t In account of this(?= )| For (?:|this|that|a) reason(?= )| In connection with(?= )| In this connection(?= )| Therefore(?= )| Because of that(?= )| Because(?= )| Thereby(?= )| Thus(?= )| Hence(?= )| Since(?= )| That's why(?= )| That is why(?= )| This is why(?= )| Thanks to(?= )| The reason(?= )| Because of(?= )| This is because(?= )| Due to(?= )\n",
            "el:cResult\n",
            "\t In consequence(?= )| As a consequence(?= )| As a result(?= )| Consequently(?= )| By such means(?= )| The result(?= )| So when(?= )| So(?= )| So why(?= )\n",
            "el:cCondition\n",
            "\t If [^\\.]+ then(?= )| If not(?= )| Even if(?= )| What if(?= )| In that case(?= )| In the case(?= )| In the case of(?= )| In that event(?= )| Then(?= )| Under the circumstances(?= )| Otherwise(?= )| Whether(?= )| And if(?= )| If so(?= )| Given the circumstances(?= )| Given that(?= )| Given the(?= )\n",
            "el:cSequence\n",
            "\t The next(?= )| Firstly(?= )| Secondly(?= )| Thirdly(?= )| First(?= )| The first(?= )| For a start(?= )| Second(?= )| The second(?= )| Third(?= )| The third(?= )| Forth(?= )| Last(?= )| My next point is(?= )| Another(?= )| On another occasion(?= )| First of all(?= )| The following(?= )| The latter(?= )| At last(?= )| Briefly(?= )| To sum up(?= )| In conclusion(?= )| In short(?= )| To resume(?= )| Lastly(?= )| Last of all(?= )| In the end(?= )| Finally(?= )| At the end of the day(?= )| Last but not least(?= )| At the end(?= )| Overall(?= )| It's all we know(?= )| It is all we know(?= )| Ultimately(?= )\n",
            "el:cAddition\n",
            "\t Also(?= )| Moreover(?= )| In addition(?= )| Besides(?= )| Furthermore(?= )| Apart from(?= )| Except for(?= )| As well(?= )| At the same time(?= )| On top of that(?= )| Above all(?= )| Again(?= )| After all(?= )| And then(?= )| And when(?= )| And while(?= )| And even(?= )| And why(?= )| Not only(?= )| Not just(?= )| Further(?= )| A further(?= )| By the way(?= )| Especially(?= )| In addition to(?= )| Along with(?= )| What's more(?= )| What is more(?= )| To add(?= )| Just to add(?= )| Even more(?= )\n",
            "el:cPurpose\n",
            "\t For this purpose(?= )| For that purpose(?= )| For the purpose(?= )| For purpose(?= )| With this in mind(?= )| With this in view(?= )| With this in(?= )| In order(?= )| So that(?= )\n",
            "el:cConcession\n",
            "\t Nevertheless(?= )| Nonetheless(?= )| Despite(?= )| Yet(?= )| And yet(?= )| Still ,(?= )| Thought(?= )| However(?= )| Even so(?= )| All the same(?= )| Although(?= )| In spite of(?= )| At least(?= )| Despite the fact(?= )| Even though(?= )| No matter(?= )| No matter how(?= )| Never mind(?= )| In any case(?= )\n",
            "el:cComparison\n",
            "\t Likewise(?= )| In the same way(?= )| Similarly(?= )| In a different way(?= )| As if(?= )| As it were(?= )| As though(?= )| Compared with(?= )| Similar(?= )| Unlike(?= )| Differing from(?= )| Dissimilar(?= )| By the same token(?= )| Equally(?= )| Not that(?= )| So if(?= )\n",
            "el:cContrast\n",
            "\t On the other(?= )| On the one(?= )| Conversely(?= )| In contrast(?= )| By contrast(?= )| Whereas(?= )| But(?= )| On the contrary(?= )| Yes, but(?= )| But despite(?= )| Opposite(?= )| Opposing(?= )| Contrary to(?= )| Alternatively(?= )| How about(?= )| Nor(?= )| Instead(?= )| Then again,(?= )| Rather(?= )| Rather than(?= )| But then(?= )| But since(?= )| But even(?= )| But just(?= )| But while(?= )| Instead of(?= )| But now(?= )| But not(?= )| But after(?= )| But according to(?= )| But why(?= )| On the other side(?= )| On the other hand(?= )| Except(?= )| But when it comes(?= )| Against(?= )| Why not(?= )| Not so(?= )| That is not(?= )\n",
            "el:cClarification\n",
            "\t That is(?= )| In other words(?= )| To put it in another way(?= )| For instance(?= )| For example(?= )| To illustrate(?= )| To be precise(?= )| More especially(?= )| To give just an example of(?= )| Specifically(?= )| In particular(?= )| One example(?= )| As in the case of(?= )| Proof is found in(?= )| This means(?= )| That means(?= )| It means(?= )| It's about(?= )| It is about(?= )| This is about(?= )| That is about(?= )| The idea is(?= )\n",
            "el:cReference\n",
            "\t As has been noted(?= )| Whichever(?= )| Whoever(?= )| Whomever(?= )| Wherever(?= )| Whatever(?= )| According to(?= )| As I was saying(?= )| To get back to that point(?= )| As I have said(?= )| As I have noted(?= )| As for(?= )| They say(?= )| When it comes to(?= )| When it comes down to(?= )| When it comes right down to(?= )\n",
            "el:cViewpoint\n",
            "\t Luckily(?= )| Unfortunately(?= )| Absurdly(?= )| Alas(?= )| Astonishingly(?= )| Characteristically(?= )| Coincidentally(?= )| Conveniently(?= )| Curiously(?= )| Foolishly(?= )| Fortunately(?= )| Happily(?= )| Incredibly(?= )| Interestingly(?= )| Ironically(?= )| Mercifully(?= )| Miraculously(?= )| Mistakenly(?= )| Mysteriously(?= )| Naturally(?= )| Oddly(?= )| Paradoxically(?= )| Predictably(?= )| Remarkably(?= )| Regrettably(?= )| Sadly(?= )| Significantly(?= )| Strangely(?= )| Surprisingly(?= )| Unsurprisingly(?= )| To my surprise(?= )| Hopefully(?= )| Typically(?= )| Unbelievably(?= )| Understandably(?= )| Unexpectedly(?= )| Unhappily(?= )| Unnecessarily(?= )| Unwisely(?= )| Wisely(?= )| Somehow(?= )| To my delight(?= )| To my distress(?= )| I wish(?= )| For me(?= )| No wonder(?= )| Thankfully(?= )\n",
            "el:cPossibility\n",
            "\t Actually(?= )| Definitely(?= )| Presumably(?= )| Perhaps(?= )| And perhaps(?= )| But perhaps(?= )| Or perhaps(?= )| Possibly(?= )| Seemingly(?= )| It seems(?= )| Probably(?= )| Or maybe(?= )| Maybe(?= )| I'm sure(?= )| surely(?= )| And of course(?= )| of course(?= )| certainly(?= )| Undoubtedly(?= )| No doubt(?= )| Doubtless(?= )| I doubt(?= )| Apparently(?= )| It appears(?= )| Obviously(?= )| Clearly(?= )| Visibly(?= )| It is clear(?= )| Conceivably(?= )| Manifestly(?= )| Plainly(?= )| Potentially(?= )| Evidently(?= )| In fact(?= )| In practice(?= )| In reality(?= )| In theory(?= )| Officially(?= )| Unofficially(?= )| Allegedly(?= )| Apparently(?= )| Nominally(?= )| Ostensibly(?= )| Supposedly(?= )| theoretically(?= )| Unmistakably(?= )| It appeared(?= )| It appears(?= )| Obviously(?= )| It may be(?= )| I suppose(?= )| Generally(?= )| Normally(?= )| It seemed(?= )| Basically(?= )| Strictly(?= )\n",
            "el:cAttitude\n",
            "\t Frankly(?= )| Honestly(?= )| To be honest(?= )| In all my honesty(?= )| To tell the truth(?= )| To tell you the truth(?= )| The truth is(?= )| In truth(?= )| You see(?= )| In fairness(?= )| In retrospect(?= )| On reflection(?= )| Personally(?= )| To my mind(?= )| Believe me(?= )| Admittedly(?= )| I think(?= )| And I think(?= )| But I think(?= )| I do think(?= )| So I think(?= )| I don't think(?= )| But I don't think(?= )| Do you think(?= )| Do you think that(?= )| You think(?= )| You might think(?= )| What do you think(?= )| We also think that(?= )| We think(?= )| To my mind(?= )| In my view(?= )| In my opinion(?= )| I believe(?= )| I agree(?= )| Just(?= )| I hope(?= )| We hope(?= )| I don't know(?= )| I feel(?= )| I believe that(?= )| I believe(?= )| You know(?= )| I think that(?= )| I find(?= )| I guess(?= )| I wonder(?= )| We believe(?= )| I found(?= )| I'm not sure(?= )| I am sure(?= )| I suspect(?= )| I hate(?= )\n",
            "el:cEmphasis\n",
            "\t The only question(?= )| The only thing(?= )| The answer is(?= )| The question is(?= )| It is important(?= )| The fact is(?= )| The good news(?= )| The most important(?= )\n",
            "el:ca\n",
            "\t In account of this(?= )| For (?:|this|that|a) reason(?= )| In connection with(?= )| In this connection(?= )| Therefore(?= )| Because of that(?= )| Because(?= )| Thereby(?= )| Thus(?= )| Hence(?= )| Since(?= )| That's why(?= )| That is why(?= )| This is why(?= )| Thanks to(?= )| The reason(?= )| Because of(?= )| This is because(?= )| Due to(?= )| In consequence(?= )| As a consequence(?= )| As a result(?= )| Consequently(?= )| By such means(?= )| The result(?= )| So when(?= )| So(?= )| So why(?= )| If [^\\.]+ then(?= )| If not(?= )| Even if(?= )| What if(?= )| In that case(?= )| In the case(?= )| In the case of(?= )| In that event(?= )| Then(?= )| Under the circumstances(?= )| Otherwise(?= )| Whether(?= )| And if(?= )| If so(?= )| Given the circumstances(?= )| Given that(?= )| Given the(?= )| For this purpose(?= )| For that purpose(?= )| For the purpose(?= )| For purpose(?= )| With this in mind(?= )| With this in view(?= )| With this in(?= )| In order(?= )| So that(?= )| Nevertheless(?= )| Nonetheless(?= )| Despite(?= )| Yet(?= )| And yet(?= )| Still ,(?= )| Thought(?= )| However(?= )| Even so(?= )| All the same(?= )| Although(?= )| In spite of(?= )| At least(?= )| Despite the fact(?= )| Even though(?= )| No matter(?= )| No matter how(?= )| Never mind(?= )| In any case(?= )| Likewise(?= )| In the same way(?= )| Similarly(?= )| In a different way(?= )| As if(?= )| As it were(?= )| As though(?= )| Compared with(?= )| Similar(?= )| Unlike(?= )| Differing from(?= )| Dissimilar(?= )| By the same token(?= )| Equally(?= )| Not that(?= )| So if(?= )| On the other(?= )| On the one(?= )| Conversely(?= )| In contrast(?= )| By contrast(?= )| Whereas(?= )| But(?= )| On the contrary(?= )| Yes, but(?= )| But despite(?= )| Opposite(?= )| Opposing(?= )| Contrary to(?= )| Alternatively(?= )| How about(?= )| Nor(?= )| Instead(?= )| Then again,(?= )| Rather(?= )| Rather than(?= )| But then(?= )| But since(?= )| But even(?= )| But just(?= )| But while(?= )| Instead of(?= )| But now(?= )| But not(?= )| But after(?= )| But according to(?= )| But why(?= )| On the other side(?= )| On the other hand(?= )| Except(?= )| But when it comes(?= )| Against(?= )| Why not(?= )| Not so(?= )| That is not(?= )| That is(?= )| In other words(?= )| To put it in another way(?= )| For instance(?= )| For example(?= )| To illustrate(?= )| To be precise(?= )| More especially(?= )| To give just an example of(?= )| Specifically(?= )| In particular(?= )| One example(?= )| As in the case of(?= )| Proof is found in(?= )| This means(?= )| That means(?= )| It means(?= )| It's about(?= )| It is about(?= )| This is about(?= )| That is about(?= )| The idea is(?= )| As has been noted(?= )| Whichever(?= )| Whoever(?= )| Whomever(?= )| Wherever(?= )| Whatever(?= )| According to(?= )| As I was saying(?= )| To get back to that point(?= )| As I have said(?= )| As I have noted(?= )| As for(?= )| They say(?= )| When it comes to(?= )| When it comes down to(?= )| When it comes right down to(?= )\n",
            "el:co\n",
            "\t Then(?= )| Afterwards(?= )| Later(?= )| Just than(?= )| At the same time(?= )| Before that(?= )| Hitherto(?= )| Previously(?= )| At once(?= )| Thereupon(?= )| Straightaway(?= )| Soon(?= )| After a while(?= )| Meanwhile(?= )| During(?= )| In the meantime(?= )| All that time(?= )| At this moment(?= )| Once(?= )| After(?= )| When(?= )| While(?= )| Until(?= )| Previously(?= )| Before(?= )| Up to that point(?= )| At this point(?= )| Here(?= )| Now(?= )| And now(?= )| An hour later(?= )| That morning(?= )| Last time(?= )| Last week(?= )| Last year(?= )| This week(?= )| Sometimes(?= )| Beforehand(?= )| Ever since(?= )| Earlier(?= )| Presently(?= )| At present(?= )| Simultaneously(?= )| Subsequently(?= )| Suddenly(?= )| Throughout(?= )| Today(?= )| Every day(?= )| Yesterday(?= )| This year(?= )| Never(?= )| So far(?= )| Since then(?= )| This time(?= )| At the time(?= )| By the time(?= )| At the moment(?= )| Last night(?= )| Currently(?= )| Recently(?= )| Initially(?= )| Early(?= )| Earlier this year(?= )| Ever(?= )| Already(?= )| Recent(?= )| As long as(?= )| As soon as(?= )| Always(?= )| For years(?= )| Whenever(?= )| Every time(?= )| Nowadays(?= )| By the end(?= )| Originally(?= )| In the first time(?= )| These days(?= )| Often(?= )| At first(?= )| Eventually(?= )| Shortly(?= )| Shortly after(?= )| Right now(?= )| At a time(?= )| In recent years(?= )| For now(?= )| Whilst(?= )| The next day(?= )| Over the years(?= )| Over the past(?= )| More recently(?= )| In August(?= )| In January(?= )| In February(?= )| In March(?= )| In April(?= )| In May(?= )| In June(?= )| In July(?= )| In September(?= )| In October(?= )| In November(?= )| In December(?= )| The next(?= )| Firstly(?= )| Secondly(?= )| Thirdly(?= )| First(?= )| The first(?= )| For a start(?= )| Second(?= )| The second(?= )| Third(?= )| The third(?= )| Forth(?= )| Last(?= )| My next point is(?= )| Another(?= )| On another occasion(?= )| First of all(?= )| The following(?= )| The latter(?= )| At last(?= )| Briefly(?= )| To sum up(?= )| In conclusion(?= )| In short(?= )| To resume(?= )| Lastly(?= )| Last of all(?= )| In the end(?= )| Finally(?= )| At the end of the day(?= )| Last but not least(?= )| At the end(?= )| Overall(?= )| It's all we know(?= )| It is all we know(?= )| Ultimately(?= )| Also(?= )| Moreover(?= )| In addition(?= )| Besides(?= )| Furthermore(?= )| Apart from(?= )| Except for(?= )| As well(?= )| At the same time(?= )| On top of that(?= )| Above all(?= )| Again(?= )| After all(?= )| And then(?= )| And when(?= )| And while(?= )| And even(?= )| And why(?= )| Not only(?= )| Not just(?= )| Further(?= )| A further(?= )| By the way(?= )| Especially(?= )| In addition to(?= )| Along with(?= )| What's more(?= )| What is more(?= )| To add(?= )| Just to add(?= )| Even more(?= )\n",
            "el:cs\n",
            "\t Luckily(?= )| Unfortunately(?= )| Absurdly(?= )| Alas(?= )| Astonishingly(?= )| Characteristically(?= )| Coincidentally(?= )| Conveniently(?= )| Curiously(?= )| Foolishly(?= )| Fortunately(?= )| Happily(?= )| Incredibly(?= )| Interestingly(?= )| Ironically(?= )| Mercifully(?= )| Miraculously(?= )| Mistakenly(?= )| Mysteriously(?= )| Naturally(?= )| Oddly(?= )| Paradoxically(?= )| Predictably(?= )| Remarkably(?= )| Regrettably(?= )| Sadly(?= )| Significantly(?= )| Strangely(?= )| Surprisingly(?= )| Unsurprisingly(?= )| To my surprise(?= )| Hopefully(?= )| Typically(?= )| Unbelievably(?= )| Understandably(?= )| Unexpectedly(?= )| Unhappily(?= )| Unnecessarily(?= )| Unwisely(?= )| Wisely(?= )| Somehow(?= )| To my delight(?= )| To my distress(?= )| I wish(?= )| For me(?= )| No wonder(?= )| Thankfully(?= )| Actually(?= )| Definitely(?= )| Presumably(?= )| Perhaps(?= )| And perhaps(?= )| But perhaps(?= )| Or perhaps(?= )| Possibly(?= )| Seemingly(?= )| It seems(?= )| Probably(?= )| Or maybe(?= )| Maybe(?= )| I'm sure(?= )| surely(?= )| And of course(?= )| of course(?= )| certainly(?= )| Undoubtedly(?= )| No doubt(?= )| Doubtless(?= )| I doubt(?= )| Apparently(?= )| It appears(?= )| Obviously(?= )| Clearly(?= )| Visibly(?= )| It is clear(?= )| Conceivably(?= )| Manifestly(?= )| Plainly(?= )| Potentially(?= )| Evidently(?= )| In fact(?= )| In practice(?= )| In reality(?= )| In theory(?= )| Officially(?= )| Unofficially(?= )| Allegedly(?= )| Apparently(?= )| Nominally(?= )| Ostensibly(?= )| Supposedly(?= )| theoretically(?= )| Unmistakably(?= )| It appeared(?= )| It appears(?= )| Obviously(?= )| It may be(?= )| I suppose(?= )| Generally(?= )| Normally(?= )| It seemed(?= )| Basically(?= )| Strictly(?= )| Frankly(?= )| Honestly(?= )| To be honest(?= )| In all my honesty(?= )| To tell the truth(?= )| To tell you the truth(?= )| The truth is(?= )| In truth(?= )| You see(?= )| In fairness(?= )| In retrospect(?= )| On reflection(?= )| Personally(?= )| To my mind(?= )| Believe me(?= )| Admittedly(?= )| I think(?= )| And I think(?= )| But I think(?= )| I do think(?= )| So I think(?= )| I don't think(?= )| But I don't think(?= )| Do you think(?= )| Do you think that(?= )| You think(?= )| You might think(?= )| What do you think(?= )| We also think that(?= )| We think(?= )| To my mind(?= )| In my view(?= )| In my opinion(?= )| I believe(?= )| I agree(?= )| Just(?= )| I hope(?= )| We hope(?= )| I don't know(?= )| I feel(?= )| I believe that(?= )| I believe(?= )| You know(?= )| I think that(?= )| I find(?= )| I guess(?= )| I wonder(?= )| We believe(?= )| I found(?= )| I'm not sure(?= )| I am sure(?= )| I suspect(?= )| I hate(?= )| The only question(?= )| The only thing(?= )| The answer is(?= )| The question is(?= )| It is important(?= )| The fact is(?= )| The good news(?= )| The most important(?= )\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "\tgnorm=145005\tgrule=243608\tgnonorm=71985\n",
            "31293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvzf cord19_stat.txt.tgz cord19_stat.txt\n",
        "!tar cvzf cord19_stat.txt_n.txt.tgz cord19_stat.txt_n.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URv0I-zEnJVN",
        "outputId": "935bd7a9-d5fe-4053-cdb9-f52a517abc70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cord19_stat.txt\n",
            "cord19_stat.txt_n.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wc cord19_stat.txt\n",
        "!wc cord19_stat.txt_n.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Avljb5X4kqx",
        "outputId": "2ebedd42-1f6e-4223-bfd3-dd48e7c16378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   1615869  249856173 1376924920 cord19_stat.txt\n",
            "  1615869  76206254 363633674 cord19_stat.txt_n.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OXml2Stat = clXml2Stat('/content/cord19.lems', output_file = '/content/cord19_stat.txt')"
      ],
      "metadata": {
        "id": "XC1EZdLNq2TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=30 cord19_stat.txt"
      ],
      "metadata": {
        "id": "rZxCec4N0FB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Route via Json: test approach...\n",
        "## working with 100MW sample (40k texts)\n",
        "### selecting 100-word long samples, writing to JSon dictionary, mapping names, recording"
      ],
      "metadata": {
        "id": "TPVG10xsWQ0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output directory\n",
        "!mkdir document_parses/pmc_json_sample02/"
      ],
      "metadata": {
        "id": "6NGEIHrYrysv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modifying script to select 100-word long samples\n",
        "'''\n",
        "algorithm:\n",
        "    1. form sections as strings\n",
        "    2. process sections (map names), create a record\n",
        "    3. write samples in a python dictionary\n",
        "\n",
        "    ? do we need xml output to a file ?\n",
        "\n",
        "    architecture:\n",
        "      - create list of sections from list of paragraphs\n",
        "      - process each section, splitting it into samples of a pre-defined size\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "O4CjgIzOWP_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c8165460-a7b5-4b9e-bc35-57aa9476866c",
        "id": "5JlmZ1U1Wx4v"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Python script to open each file, read json input and copy to one text file for subsequent processing\n",
        "import os, re, sys\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "class clJsonDir2txtSamples(object):\n",
        "    '''\n",
        "    @author Bogdan Babych, IÜD, Heidelberg University\n",
        "    @email bogdan [dot] babych [at] iued [dot] uni-heidelberg [dot] de\n",
        "    a script for processing covid-19 corpus:\n",
        "    @url https://www.semanticscholar.org/cord19 @url https://www.semanticscholar.org/cord19/download\n",
        "        recursively reads files from a directory, and glues them together into a single corpus file\n",
        "\n",
        "    @todo:\n",
        "        working with sections - collect titles of all sections; frequent sections; select argumentative sections (e.g., discussion, analysis...)\n",
        "        - to compare descriptive and argumentative parts of the corpus\n",
        "\n",
        "        experimenting with different annotations (pos, parsing... ); MT quality evaluation...\n",
        "    '''\n",
        "    def __init__(self, SDirName, output_file = 'corpus_out.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = True, include_authors = True, tag='doc', id=1000000, split_by_docs = 0, copy_docs = 0, sample_size = 0, outjsondir = '02'): # initialising by openning the directories\n",
        "        self.SOutput_file = output_file\n",
        "        self.SOutput_file_stat = 'stat_' + output_file\n",
        "        self.STextFilter = textfilter\n",
        "        self.RFilter = re.compile(textfilter, re.IGNORECASE | re.MULTILINE)\n",
        "        self.BInclTitle = include_title # implemented\n",
        "        self.BInclSectionNames = include_sectionNames # implemented\n",
        "        self.BInclRefs = include_refs # not implemented yet\n",
        "        self.BInclAuth = include_authors # not implemented yet\n",
        "        self.STag = tag\n",
        "        self.ID = id\n",
        "        self.ISplitByDocs = int(split_by_docs)\n",
        "        self.ICopyDocs = int(copy_docs)\n",
        "        self.ISampleSize = int(sample_size)\n",
        "        self.SDirJsonOutput = outjsondir\n",
        "\n",
        "        # global dictionary of section names (to check and make rules...)\n",
        "        self.DSectNames = defaultdict(int)\n",
        "        # print(self.ISplitByDocs)\n",
        "        self.openDir(SDirName)\n",
        "        self.printDictionary(self.DSectNames, 'corpus-section-names.txt')\n",
        "        return\n",
        "\n",
        "\n",
        "    def openDir(self, path): # implementation of recursively openning directories from a given rule directory and reading each file recursively into a string\n",
        "        i = 0\n",
        "        path_sample = path + '_sample'\n",
        "        path_sample_out = path + '_sample' + self.SDirJsonOutput\n",
        "        FOutStat = open(self.SOutput_file_stat, 'w')\n",
        "        if self.ISplitByDocs:\n",
        "            SPartFile = \"part1000000\" + self.SOutput_file\n",
        "            FOut = open(SPartFile, 'w')\n",
        "        else:\n",
        "            FOut = open(self.SOutput_file, 'w')\n",
        "\n",
        "        for root,d_names,f_names in os.walk(path):\n",
        "            for f in f_names:\n",
        "                i+=1\n",
        "                if i%10000==0: print(str(i) + '. Processing: ' + f)\n",
        "                fullpath = os.path.join(root, f)\n",
        "                ## output of Json files\n",
        "                fullpathout = os.path.join(path_sample_out, f) \n",
        "                # print(fullpath)\n",
        "                print('full path output: ' + fullpathout)\n",
        "                try:\n",
        "                    ## implement : processing output file; statistics output file...\n",
        "                    FIn = open(fullpath,'r')\n",
        "                    SIn = FIn.read()\n",
        "                    # apply text filter, if not None\n",
        "                    if self.STextFilter and (re.search(self.RFilter, SIn) == None): continue\n",
        "                    DData, SText2Write = self.procFile(SIn,f,i)\n",
        "                    if SText2Write: FOut.write(SText2Write) # if the string is not empty then write to file\n",
        "                    FIn.close()\n",
        "\n",
        "                    ## writing json files with new dictionary structure\n",
        "                    ## FJsonOut = open(fullpathout, 'w')\n",
        "                    with open(fullpathout, 'w', encoding='utf-8') as jf:\n",
        "                        json.dump(DData, jf, ensure_ascii=False, indent=4)\n",
        "\n",
        "                except:\n",
        "                    print(f'file {f} cannot be read or processed')\n",
        "                finally:\n",
        "                  ## closing json output file\n",
        "                    jf.close() ## close json file\n",
        "                    # splitting output into chunks of \"split_by_docs\" size\n",
        "                    if self.ISplitByDocs and (i % self.ISplitByDocs == 0): # if self.ISplitByDocs == 0 then everything goes into one file; if this > 0 then\n",
        "                        SPartFile = \"part\" + str(1000000 + i) + self.SOutput_file # generate new file name\n",
        "                        FOut.flush()\n",
        "                        FOut.close()\n",
        "                        FOut = open(SPartFile, 'w')\n",
        "                    if self.ICopyDocs and (i >= self.ICopyDocs) and (i < (self.ICopyDocs + self.ISplitByDocs)):\n",
        "                        try:\n",
        "                            SOutputDirN = root + '_sample'\n",
        "                            SOutputFN = os.path.join(SOutputDirN, f)\n",
        "                            os.system(f'cp {fullpath} {SOutputFN}')\n",
        "                        except:\n",
        "                            print('.')\n",
        "\n",
        "        FOut.flush()\n",
        "        FOut.close()\n",
        "\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def procFile(self, SIn,SFNameIn,i): # sending each json string for extraction of text and attaching an correct tags to each output string output string\n",
        "        STagOpen = '<' + self.STag + ' id=\"' + self.STag + str(self.ID + i)  + '\">\\n'\n",
        "        STagClose = '\\n</' + self.STag + '>\\n\\n'\n",
        "        DData, SText4Corpus = self.getJson(SIn, SFNameIn)\n",
        "        STagsAndText = STagOpen + SText4Corpus + STagClose\n",
        "        if SText4Corpus:\n",
        "            return DData, STagsAndText\n",
        "        else:\n",
        "            print('\\tNo data read from: ' + SFNameIn)\n",
        "            return DData, None\n",
        "\n",
        "\n",
        "    def getJson(self, SIn, SFNameIn): # for each file-level string read from a file: managing internal structure of the covid-19 json file\n",
        "        LOut = [] # collecting a list of strings\n",
        "\n",
        "        LBodyTextSections = [] ## list of sections: Json body text\n",
        "        ### DJsonOut = {} # this will be copied from the input json structure and enriched\n",
        "\n",
        "        try:\n",
        "            DDoc = json.loads(SIn)\n",
        "        except:\n",
        "            print('\\t\\t' + SFNameIn + ' => error reading json2dictionary')\n",
        "            return {}, None\n",
        "\n",
        "        # metadata:\n",
        "        try:\n",
        "            DMetaData = DDoc['metadata']\n",
        "            if DMetaData:\n",
        "                DMetaDataOut, SMetaData = self.getJson_Metadata(DMetaData)\n",
        "                if SMetaData: LOut.append(SMetaData)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no metadata')\n",
        "            DMetaData = None\n",
        "        # body text\n",
        "        try:\n",
        "            LBodyText = DDoc['body_text']\n",
        "            if LBodyText:\n",
        "                ## implementing sampling here ~ returning list of sections first (with a paired list of section names); then sampling for sample size...\n",
        "\n",
        "                LBodyTextSections, LBodyTextSectNames, SBodyText = self.getJson_BodyText(LBodyText) ## modified function, returns 3 arguments\n",
        "                LOut.append(SBodyText)\n",
        "        except:\n",
        "            print('\\t\\t\\t' + SFNameIn + ' ====> no body_text')\n",
        "            LBodyText = None\n",
        "        # further: to implement references\n",
        "\n",
        "        SText = '\\n\\n'.join(LOut)\n",
        "        ### DJsonOut[\"body_text\"] = LBodyTextSections\n",
        "        # returning an enriched data structure\n",
        "        return DDoc, SText\n",
        "\n",
        "\n",
        "    def getJson_Metadata(self, DIn): # converts interesting parts of metadata into a string\n",
        "        SMetadata = ''\n",
        "        LMetadata = []\n",
        "        DMetaDataOut = {}\n",
        "        try: STitle = DIn[\"title\"]\n",
        "        except: STitle = None\n",
        "        if STitle and self.BInclTitle:\n",
        "            LMetadata.append(STitle)\n",
        "\n",
        "        # to implement reading of authors' names\n",
        "        '''\n",
        "        try:\n",
        "            SPaperID = DIn[\"authors\"]\n",
        "            DMetaDataOut[\"authors\"] = \"SAuthors\"\n",
        "        except:\n",
        "            print('au!')\n",
        "        '''\n",
        "\n",
        "        if LMetadata: SMetadata = '\\n\\n'.join(LMetadata)\n",
        "        return DMetaDataOut, SMetadata\n",
        "\n",
        "    ## updated function ~\n",
        "    def getJson_BodyText(self, LIn): # converts interesting parts of the body texts into a string\n",
        "        ## modified ~ for json output\n",
        "        LBodyTextSections = [] ## output - list of sections\n",
        "        LBodyTextSectionPars = [] ## element of the LBodyTextSections[] list, one section-long        \n",
        "        LBodyTextSectNames = [] ##\n",
        "        LBodyTextSectNamesM = [] ##\n",
        "\n",
        "\n",
        "        SBodyText = '' # output (old)\n",
        "        LBodyText = []\n",
        "        SSectionName0 = '' # current section name set to empty for a new text\n",
        "        # todo: later, in post-processing stage for the whole corpus, maybe after lemmatization...\n",
        "        # ISampleNumber = 0 # samples of 100 words in text; they do not cross section boundaries \n",
        "\n",
        "        for DParagraph in LIn:\n",
        "            # sections added 2022-09-28\n",
        "            # updating and adding the section name\n",
        "            try:\n",
        "                # DParagraphs[\"section\"] ## distinction between different sections....\n",
        "                SSectionName = DParagraph[\"section\"]\n",
        "                # normalizing new section name (1)\n",
        "                SSectionName = SSectionName.replace(\"\\n\", \" \")\n",
        "\n",
        "                if self.BInclSectionNames and SSectionName: # if we opted to include section names and section name is not empty\n",
        "                    if SSectionName != SSectionName0: # if we found a new section name\n",
        "                        # processing section name\n",
        "                        LBodyTextSectNames.append(SSectionName0) ## record the previous section name\n",
        "                        ## to implement here: call a function for mapping the section names\n",
        "                        LBodyTextSectNamesM.append(SSectionName0)\n",
        "\n",
        "                        SSectionName0 = SSectionName # change the current section name\n",
        "                        ## recording previous section if not empty; then adding the name of the section to a list\n",
        "\n",
        "                        SBodyTextSectionPars = '\\n'.join(LBodyTextSectionPars)\n",
        "                        LBodyTextSections.append(SBodyTextSectionPars)\n",
        "                        LBodyTextSectionPars = []\n",
        "                            \n",
        "\n",
        "                        # normalizing section name (2)\n",
        "                        SSectionNameNorm = SSectionName.lower()\n",
        "                        SSectionNameNorm = re.sub('[0-9\\.]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = re.sub('[ ]+', ' ', SSectionNameNorm)\n",
        "                        SSectionNameNorm = SSectionNameNorm.strip()\n",
        "                        \n",
        "                        self.DSectNames[SSectionNameNorm] += 1\n",
        "                        SSect4text = f'<section sName=\"{SSectionNameNorm}\">\\n{SSectionName}\\n</section>'\n",
        "\n",
        "                        LBodyText.append(SSect4text)\n",
        "            except:\n",
        "                print('S!',)\n",
        "                continue\n",
        "            # first original section (we extract text after extracting section name)\n",
        "            try:\n",
        "                ## DParagraphs[section] ## -- later on >> distinction between different sections....\n",
        "                SParagraph = DParagraph[\"text\"]\n",
        "                LBodyText.append(SParagraph)\n",
        "            except:\n",
        "                print('!',)\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "        SBodyText = '\\n\\n'.join(LBodyText)\n",
        "        return LBodyTextSections, LBodyTextSectNamesM, SBodyText\n",
        "\n",
        "    def printDictionary(self, DFreq, SFOutDict):\n",
        "        FOutDict = open(SFOutDict, 'w')\n",
        "        for key, val in sorted(DFreq.items(), key=lambda x: x[1], reverse=True):\n",
        "            FOutDict.write(f'{key}\\t{str(val)}\\n')\n",
        "        FOutDict.flush()\n",
        "        FOutDict.close()\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# arguments:\n",
        "'''\n",
        "        sys.argv[1], # obligatory: input directory name;\n",
        "            other arguments optional:\n",
        "            output_file = 'covid19corpus.txt',\n",
        "            textfilter = None, # if this is string, only texts containing it are collected, e.g., covid\n",
        "            include_title = True, # include or exclude title\n",
        "            include_refs = False, # not implemented yet: include or exclude references\n",
        "            split_by_docs=0 # split by groups of n documents; if 0 then write to one file\n",
        "\n",
        "'''\n",
        "\n",
        "'''if __name__ == '__main__':\n",
        "    OJsonDir2txt = clJsonDir2txtSamples(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=240000, sample_size = 100, outjsondir = 'document_parses/pmc_json_sample02')\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"if __name__ == '__main__':\\n    OJsonDir2txt = clJsonDir2txt(sys.argv[1], output_file = 'covid19corpus.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=240000)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OJsonDir2txtSamples = clJsonDir2txtSamples(\"document_parses/pmc_json_sample\", output_file = 'cord19.txt', textfilter=None, include_title = True, include_sectionNames = True, include_refs = False, split_by_docs=0, copy_docs=0, sample_size = 100, outjsondir = 'document_parses/pmc_json_sample02')\n"
      ],
      "metadata": {
        "id": "zDH-FGHuh92R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}